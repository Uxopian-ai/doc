{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to uxopian-ai","text":"<p>uxopian-ai is a complete, standalone framework designed to accelerate and simplify the integration of powerful AI features into any enterprise application.</p> <p>Built on a solid foundation of Java 21 LTS and Spring 3.5, it goes far beyond a simple library by providing a full suite of tools \u2014 from backend services to frontend components \u2014 to create sophisticated, reliable, and scalable AI solutions.</p>"},{"location":"#the-uxopian-ai-advantage-more-than-just-a-library","title":"The uxopian-ai Advantage: More Than Just a Library","text":"<p>While uxopian-ai uses the excellent Langchain4j library as its core for LLM interactions, it builds a complete enterprise-ready ecosystem around it. Here's the added value:</p> <ul> <li>Standalone Service, Not Just Code: A pre-packaged, deployable service that saves you months of development and infrastructure setup.</li> <li>Ready-to-Use UI Components: Instantly integrate AI with web-components (IIFE compiled, scoped CSS), plus plug-and-play integration scripts.</li> <li>Advanced Orchestration Engine: The unique Goal system enables dynamic prompt selection based on context \u2014 no need to build this from scratch.</li> <li>Complete Conversation Management: Persistent conversations with cost tracking, response regeneration, and user feedback support.</li> <li>Data-Driven Insights: A comprehensive admin panel to monitor ROI, token usage, and adoption trends.</li> </ul>"},{"location":"#key-features-at-a-glance","title":"Key Features at a Glance","text":""},{"location":"#effortless-scalable-integration","title":"Effortless &amp; Scalable Integration","text":"<ul> <li>Standalone Service: Deployable via Docker or as a Java 21 application.</li> <li>Multi-Tenant Architecture: Designed for internal deployments with clear logical separation and distinct tenant management.</li> <li>Web-Component UI: Lightweight, embeddable components for any web app.</li> <li>Rich REST API: Fully documented (Swagger) for seamless integration.</li> </ul>"},{"location":"#powerful-admin-analytics","title":"Powerful Admin &amp; Analytics","text":"<ul> <li>Granular Token Monitoring: Visualize input and output token consumption globally, by specific users, or per conversation.</li> <li>ROI &amp; Efficiency Tracking: Specific metrics allow you to view the number of times a prompt is used and estimate the total time saved.</li> <li>Usage Trends: Analyze activity over time (requests per week), monitor LLM model distribution, and track the adoption of advanced features like multi-modal capabilities.</li> </ul>"},{"location":"#intelligent-orchestration","title":"Intelligent Orchestration","text":"<ul> <li>Goal System: Define context-aware workflows using filters and priorities.   Example: A \"comparison\" goal automatically picks a legal prompt for contracts, and a generic one for others.</li> <li>Templating Engine: Dynamic data injection, custom Java services, and conditional logic with Thymeleaf.</li> <li>Template Helpers: Add your own Java functions to enrich prompts.</li> </ul>"},{"location":"#robust-llm-interaction","title":"Robust LLM Interaction","text":"<ul> <li>Broad Support: Compatible with many LLM providers out-of-the-box.</li> <li>Custom Connectors: Add private or fine-tuned models easily.</li> <li>Advanced Features: Native support for function calling, multi-modal requests (text + image), and streaming/non-streaming responses.</li> <li>MCP Server Client: Acts as a client for Model Context Protocol (MCP) servers.</li> </ul>"},{"location":"#complete-conversation-management","title":"Complete Conversation Management","text":"<ul> <li>Persistent History: Conversations and messages are stored with full context.</li> <li>Feedback Loop: Gather specific user feedback (Good/Bad/Neutral) on responses to improve prompt quality.</li> <li>Rich UX: Regenerate, copy, and manage conversation content easily.</li> </ul>"},{"location":"#reading-paths","title":"Reading Paths","text":"<p>Choose the path that matches your role:</p>"},{"location":"#new-to-uxopian-ai","title":"New to uxopian-ai?","text":"<ol> <li>Quick Start \u2014 Your first AI exchange in 5 minutes.</li> <li>Core Concepts \u2014 Understand Prompts, Goals, and Conversations.</li> <li>Architecture Overview \u2014 See how the components fit together.</li> </ol>"},{"location":"#operator-devops","title":"Operator / DevOps?","text":"<ol> <li>Deploy with Docker \u2014 Set up the full stack.</li> <li>Configuration Files \u2014 YAML reference for all config files.</li> <li>Environment Variables \u2014 Quick reference for Docker deployments.</li> <li>Backup and Recovery \u2014 Protect your data.</li> </ol>"},{"location":"#integrator","title":"Integrator?","text":"<ol> <li>Architecture Overview \u2014 Understand the BFF pattern.</li> <li>Embedding in a Web Page \u2014 Add AI to any web app.</li> <li>Integrating with ARender \u2014 Add AI buttons in ARender.</li> <li>Integrating with FlowerDocs \u2014 Add AI features in FlowerDocs.</li> </ol>"},{"location":"#java-developer","title":"Java Developer?","text":"<ol> <li>Core Concepts \u2014 Understand the domain model.</li> <li>The Templating Engine \u2014 Master dynamic prompt authoring.</li> <li>Creating Custom Helpers \u2014 Inject your own data into prompts.</li> <li>Creating Custom Tools \u2014 Give the LLM the ability to take actions.</li> </ol>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#can-uxopian-ai-plug-into-other-ecms-viewers-and-llm-providers-can-i-buy-uxopian-ai-alone","title":"Can Uxopian AI plug into other ECMs, viewers, and LLM providers? Can I buy Uxopian AI alone?","text":"<p>Yes. Uxopian AI can be deployed independently from FlowerDocs and ARender, and connected to third-party ECM platforms (e.g., OpenText) and third-party viewers (e.g., OpenText Intelligent Viewing). It can also be configured to use LLM providers other than OpenAI (from the list of providers already supported by Uxopian AI).</p> <p>The main condition is integration: a system integrator or partner must implement the connector(s) for the target ECM/viewer/application. Once implemented, these connectors are reusable across projects.</p>"},{"location":"faq/#how-it-works-prompt-templating-prompt-helpers","title":"How it works (prompt templating + \u201cprompt helpers\u201d)","text":"<p>Uxopian AI sends requests to LLMs through a prompt templating system. Prompts can include expressions that are evaluated at runtime to fetch context from the surrounding application or content system.</p> <p>Documentation: Creating Custom Helpers</p> <p>Example (as used in standard demos), where Uxopian AI summarizes the document currently opened by the user:</p> <pre><code>Summarize the following document. It has to stay in less than 60 words, but have the key information to grasp the bulk of the conversation. You can use markdown, only to put in bold the critical pieces.\n\nDocument content:\n[[${documentService.extractTextualContent(documentId)}]]\n</code></pre> <p>When the prompt templating engine encounters the expression inside <code>[[ ... ]]</code>, it calls a backend helper to resolve it. In the example above, <code>documentService.extractTextualContent(documentId)</code> retrieves the full text of the document (using the document ID already available because the user opened it in the UI). The final prompt sent to the LLM is the original template plus the resolved document content.</p> <p>You can verify what is actually sent to the LLM by reviewing the history of LLM exchanges in the Users section of Uxopian AI.</p>"},{"location":"faq/#plugging-into-other-ecmsviewers","title":"Plugging into other ECMs/viewers","text":"<p>The <code>documentService.extractTextualContent(...)</code> behavior in the example is provided through an integration component called a prompt helper. The helper used in the demo is specific to ARender, but the mechanism is designed to be pluggable.</p> <p>That means you can implement a new prompt helper for another system. For example:</p> <ul> <li>A Documentum helper that retrieves full text given a Documentum document ID</li> <li>An OpenText helper that retrieves full text and metadata from OpenText</li> <li>A \u201ccase context\u201d helper for Salesforce that injects the current case details (customer, policy, claim status, next actions, etc.)</li> </ul> <p>How to create custom prompt helpers: Creating Custom Helpers</p> <p>Uxopian AI also includes existing helpers (for example, a helper for FlowerDocs to access documents by FlowerDocs ID), which can be used as a reference.</p>"},{"location":"faq/#integrating-the-assistant-ui-into-other-applications","title":"Integrating the assistant UI into other applications","text":"<p>On the UI side, the assistant is packaged as a web component, which is a highly portable format for embedding UI elements in web applications. This makes it straightforward to integrate Uxopian AI into any web app that provides an extension mechanism (plugin areas, custom widgets, embedded panels, etc.).</p> <p>Documentation: Embedding in a Web Page</p>"},{"location":"faq/#summary","title":"Summary","text":"<ul> <li>Uxopian AI can be purchased and deployed on its own.</li> <li>Integration with third-party ECM/viewers/apps is achieved by implementing reusable connectors (prompt helpers and UI embedding).</li> <li>Switching LLM providers is supported (within the list of providers supported by Uxopian AI).</li> <li>Most customers rely on a system integrator/partner to implement the connectors once, then reuse them across deployments.</li> </ul>"},{"location":"faq/#can-i-use-xopia-to-trigger-ai-agents-instead-of-a-flat-llm-based-conversation","title":"Can I use Xopia to trigger AI agents, instead of a \"flat\" LLM-based conversation.","text":"<p>The term \u201cagent\u201d is still understood in different ways. Uxopian AI will allow later in 2026 to declare new agents. when it comes to integrating existing ones, let's separate two use cases:</p> <p>1) Conversational agents (chat-based agent services). If your \u201cagent\u201d exposes a messaging / chat API (and runs its own reasoning loop), you can use the Uxopian Assistant UI to converse with it by implementing a provider / model class that connects to that agent, the same way a provider/model would connect to an LLM endpoint. From the UI standpoint, it remains a normal assistant conversation; under the hood, the \u201cmodel\u201d is your agent service. See the How to Guides section for this.</p> <p>2) Action agents (agents that execute tasks). If your \u201cagent\u201d is something you call to perform an action (redaction, metadata updates, lookups, update customer in CRM,  etc.) rather than a chat endpoint, then the right concept is a Tool / Function Calling integration. Uxopian AI can invoke declared tools behind the scenes when the user asks for actions (e.g., \u201cRedact these documents and remove addresses\u201d). See: Creating Custom Tools We also plan to add a direct MCP bridge later this year.</p>"},{"location":"faq/#can-i-connect-customer-developed-llm-endpoints-or-proprietary-ai-services-in-xopia","title":"Can I connect customer-developed LLM endpoints (or proprietary AI services) in Xopia?","text":"<p>Yes. Customer-developed LLM endpoints (or proprietary AI services) can be integrated by implementing a provider / model connector so that Xopia routes calls to that endpoint. This can be used either as a classical LLM provider, or to connect to a conversational-agent service (chat/messaging API) presented as a \u201cmodel\u201d to the Assistant UI.</p>"},{"location":"faq/#do-we-have-an-internal-paper-about-agents-in-uxopian-ai","title":"Do we have an internal paper about \u201cagents\u201d in Uxopian AI?","text":"<p>Not at the moment as a single dedicated paper. The most authoritative references today are the extension documentation (custom tools, and provider/model extensions). We will integrate the concept of agent later in 2026 in Uxopian AI framework.</p>"},{"location":"admin/dashboard/","title":"Admin Dashboard","text":"<p>The Uxopian-ai Admin Panel acts as the central command center for your AI infrastructure. It provides a visual interface to manage resources, monitor usage statistics, and oversee user activity.</p> <p>Access</p> <p>The Admin Panel is restricted to users with the admin role. URL: <code>https://&lt;your-uxopian-endpoint&gt;/admin</code></p>"},{"location":"admin/dashboard/#key-features","title":"Key Features","text":"<p>The dashboard offers quick access to the primary management modules:</p> <ul> <li>Prompts: Manage, test, and analyze AI prompt configurations.</li> <li>LLM Providers: Configure and test LLM provider connections.</li> <li>Statistics: View global health metrics, usage trends, and ROI data.</li> <li>Users: Monitor user activity and view detailed interaction history.</li> </ul>"},{"location":"admin/dashboard/#resources","title":"Resources","text":"<p>The dashboard also provides direct access to developer tools:</p> <ul> <li>Official Documentation: Link to these guides.</li> <li>Swagger UI: Access the interactive API documentation for exploring and testing endpoints directly.</li> </ul>"},{"location":"admin/llm_providers/","title":"LLM Provider Management","text":"<p>The LLM Provider Management page allows administrators to configure, test, and manage LLM provider connections at runtime \u2014 without restarting the service.</p> <p>Each provider configuration defines connection parameters (API key, endpoint, timeouts) and a list of available models with their specific overrides.</p>"},{"location":"admin/llm_providers/#1-provider-list","title":"1. Provider List","text":"<p>The main view displays all configured LLM providers for the current tenant.</p> <ul> <li>Search &amp; Filter: Quickly find providers by name or type.</li> <li>Actions: Create, edit, or delete provider configurations.</li> <li>Status Indicators: See at a glance which providers have valid configurations.</li> <li>API Reference: <code>GET /api/v1/admin/llm/provider-conf</code></li> </ul> <p>Each row shows the provider type (e.g., <code>openai</code>, <code>anthropic</code>), the default model alias, and the number of configured models.</p>"},{"location":"admin/llm_providers/#2-provider-editor","title":"2. Provider Editor","text":"<p>The editor provides granular control over a provider's configuration. It is divided into three sections.</p>"},{"location":"admin/llm_providers/#provider-identity","title":"Provider Identity","text":"Field Description Provider The provider type (must match a registered Spring bean: <code>openai</code>, <code>anthropic</code>, <code>azure</code>, <code>gemini</code>, <code>mistral</code>, <code>ollama</code>, etc.) Default Model Conf Name The alias of the model to use by default when no model is specified in the request."},{"location":"admin/llm_providers/#global-configuration","title":"Global Configuration","text":"<p>These settings apply to all models under this provider unless overridden at the model level.</p> Field Type Description <code>apiSecret</code> String API key or secret for authentication. Encrypted at rest (AES-GCM). <code>endpointUrl</code> String Base URL for the provider's API. <code>temperature</code> Double Sampling temperature (0.0 \u2013 2.0). <code>topP</code> Double Nucleus sampling threshold. <code>topK</code> Integer Top-K sampling parameter. <code>seed</code> Integer Deterministic seed for reproducibility. <code>maxTokens</code> Integer Maximum tokens in the response. <code>presencePenalty</code> Double Penalizes repeated topics. <code>frequencyPenalty</code> Double Penalizes repeated tokens. <code>maxRetries</code> Integer Number of retry attempts on failure. <code>timeout</code> Duration Request timeout (e.g., <code>60s</code>, <code>PT2M</code>). <code>multiModalSupported</code> Boolean Whether the provider supports image inputs. <code>functionCallSupported</code> Boolean Whether the provider supports tool/function calling. <code>extras</code> Map Provider-specific key-value pairs (e.g., <code>deploymentName</code> for Azure)."},{"location":"admin/llm_providers/#model-configurations","title":"Model Configurations","text":"<p>Each model entry represents a specific model alias available through this provider. Models inherit all global configuration values and can override any of them.</p> Field Description Model Conf Name (<code>llmModelConfName</code>) The alias used in prompts and API calls (e.g., <code>my-gpt5</code>). Model Name (<code>modelName</code>) The actual model identifier sent to the provider's API (e.g., <code>gpt-5.1</code>). (all global fields) Any field from the global configuration can be overridden per model. <p>Configuration Inheritance</p> <p>When processing a request, the service merges global and model-specific settings. Model-level values take precedence. For example, if the global <code>temperature</code> is <code>0.7</code> but a specific model sets <code>temperature: 0.2</code>, the model-level value (<code>0.2</code>) is used. Fields not overridden at the model level fall back to the global value.</p>"},{"location":"admin/llm_providers/#3-provider-detail","title":"3. Provider Detail","text":"<p>Selecting a provider from the list opens the detail view, which combines:</p> <ul> <li>Editor Tab \u2014 The full provider editor (see above).</li> <li>Tester Tab \u2014 The connection tester (see below).</li> </ul>"},{"location":"admin/llm_providers/#4-connection-tester","title":"4. Connection Tester","text":"<p>The Connection Tester lets you verify that a provider configuration is working correctly before using it in production prompts.</p>"},{"location":"admin/llm_providers/#how-it-works","title":"How It Works","text":"<ol> <li>Select a model from the provider's configured model list.</li> <li>Click Test Connection.</li> <li>The system sends a minimal test request to the provider's API.</li> <li>Results are displayed with:<ul> <li>Status badge: Success or failure indicator.</li> <li>Response details: Model response, latency, and token usage.</li> <li>Error details: If the test fails, the full error message is shown for debugging.</li> </ul> </li> </ol> <p>Test After Changes</p> <p>After modifying API keys, endpoints, or model names, always use the Connection Tester before saving. This prevents misconfigured providers from affecting live prompts.</p>"},{"location":"admin/llm_providers/#5-per-tenant-configuration","title":"5. Per-Tenant Configuration","text":"<p>LLM provider configurations support multi-tenancy. Configurations can be defined at the global level and then customized per tenant using a merge strategy:</p> Strategy Behavior <code>MERGE</code> Tenant-specific providers are merged with global ones. Matching providers are updated; non-matching ones are added. <code>OVERWRITE</code> Tenant configuration completely replaces the global configuration. <code>CREATE_IF_MISSING</code> Tenant-specific providers are only added if no global configuration exists for that provider. <p>This allows a central admin to define a base set of providers (e.g., OpenAI with a shared API key) while individual tenants can add their own providers or override API keys.</p>"},{"location":"admin/llm_providers/#api-reference","title":"API Reference","text":"<p>All operations require the <code>admin</code> role (<code>X-User-Roles: admin</code>).</p> Method Endpoint Description <code>GET</code> <code>/api/v1/admin/llm/providers</code> List all registered provider types (bean names). <code>GET</code> <code>/api/v1/admin/llm/provider-conf</code> List all provider configurations for the tenant. <code>GET</code> <code>/api/v1/admin/llm/provider-conf/{id}</code> Get a specific provider configuration by ID. <code>POST</code> <code>/api/v1/admin/llm/provider-conf</code> Create a new provider configuration. <code>PUT</code> <code>/api/v1/admin/llm/provider-conf/{id}</code> Update an existing provider configuration. <code>DELETE</code> <code>/api/v1/admin/llm/provider-conf/{id}</code> Delete a provider configuration."},{"location":"admin/llm_providers/#example-creating-a-provider-configuration","title":"Example: Creating a Provider Configuration","text":"<pre><code>curl -X POST \"http://localhost:8080/api/v1/admin/llm/provider-conf\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Roles: admin\" \\\n  -d '{\n    \"provider\": \"openai\",\n    \"defaultLlmModelConfName\": \"gpt5\",\n    \"globalConf\": {\n      \"apiSecret\": \"sk-your-api-key-here\",\n      \"temperature\": 0.7,\n      \"maxRetries\": 3,\n      \"timeout\": \"60s\"\n    },\n    \"llModelConfs\": [\n      {\n        \"llmModelConfName\": \"gpt5\",\n        \"modelName\": \"gpt-5.1\",\n        \"multiModalSupported\": true,\n        \"functionCallSupported\": true\n      },\n      {\n        \"llmModelConfName\": \"gpt5-mini\",\n        \"modelName\": \"gpt-5-mini\",\n        \"temperature\": 0.3,\n        \"multiModalSupported\": true,\n        \"functionCallSupported\": true\n      }\n    ]\n  }'\n</code></pre>"},{"location":"admin/llm_providers/#related-resources","title":"Related Resources","text":"<ul> <li>Configuration Files \u2014 Dynamic Provider Configuration \u2014 YAML bootstrapping reference.</li> <li>Core Concepts \u2014 Parameter Precedence \u2014 How model parameters are resolved.</li> <li>Adding a New LLM Provider \u2014 Creating custom provider connectors.</li> </ul>"},{"location":"admin/prompts/","title":"Prompts Management","text":"<p>Prompts are the building blocks of your AI interactions. This section allows administrators to manage the lifecycle of these prompts, from creation to performance analysis.</p>"},{"location":"admin/prompts/#1-prompt-list-search","title":"1. Prompt List &amp; Search","text":"<p>The main view displays all available prompts for the current tenant.</p> <ul> <li>Search: Find specific prompts by their ID or content.</li> <li>Actions: Create new prompts, edit existing ones, or view their specific statistics.</li> <li>API Reference: <code>GET /api/v1/admin/prompts</code></li> </ul>"},{"location":"admin/prompts/#2-prompt-editor-crud","title":"2. Prompt Editor (CRUD)","text":"<p>The editor provides granular control over prompt behavior.</p>"},{"location":"admin/prompts/#content-configuration","title":"Content &amp; Configuration","text":"<ul> <li>Content Editor: Edit the Thymeleaf template used to generate the prompt dynamically. See The Templating Engine for syntax details.</li> <li>Model Configuration: Set the default LLM provider (e.g., <code>openai</code>) and model (e.g., <code>gpt-5.1</code>). See Choosing the Right Model for guidance.</li> <li>Capabilities: Toggle flags based on the prompt's needs:<ul> <li>Reasoning: Enable or disable reasoning capabilities.</li> <li>Multi-modal: Flag if the prompt requires image inputs.</li> <li>Function Calling: Flag if the prompt triggers external tools.</li> </ul> </li> </ul>"},{"location":"admin/prompts/#roi-settings","title":"ROI Settings","text":"<ul> <li>Time Saved: Define an estimated \"Time Saved per usage\" (in seconds). This value is used to calculate the Return on Investment (ROI) metrics across the platform.</li> </ul>"},{"location":"admin/prompts/#3-prompt-statistics","title":"3. Prompt Statistics","text":"<p>Each prompt has a dedicated statistics view with performance metrics:</p> <ul> <li>Usage Count: The total number of times this prompt has been triggered.</li> <li>Feedback: A breakdown of user ratings (Good, Bad, Neutral) to assess output quality.</li> <li>Cost &amp; ROI: Token consumption (<code>totalCost</code>, <code>costAverage</code>) and the specific time saved by this prompt.</li> </ul>"},{"location":"admin/prompts/#4-prompt-tester","title":"4. Prompt Tester","text":"<p>The Prompt Tester allows you to execute a prompt directly from the admin interface \u2014 without needing to set up a conversation or use external tools like cURL.</p>"},{"location":"admin/prompts/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Variable Detection: When you open the tester for a prompt, it automatically parses the Thymeleaf template and detects all variables (e.g., <code>${documentId}</code>, <code>${language}</code>).</p> </li> <li> <p>Variable Configuration: For each detected variable, an input field is displayed. You can provide:</p> <ul> <li>Text values \u2014 For string variables like document IDs, language codes, or user queries.</li> <li>Image values \u2014 For multi-modal prompts that expect Base64-encoded images.</li> </ul> </li> <li> <p>Execute: Click \"Test\" to send the prompt with the configured variables to the LLM. The system uses the prompt's <code>defaultLlmProvider</code> and <code>defaultLlmModel</code> settings.</p> </li> <li> <p>Results: The tester displays:</p> <ul> <li>The LLM response.</li> <li>Token usage (input/output).</li> <li>Response latency.</li> </ul> </li> <li> <p>cURL Generation: The tester generates the equivalent cURL command for the test configuration, making it easy to reproduce the request from a terminal or integrate into scripts.</p> </li> </ol> <p>Iterate Quickly</p> <p>Use the Prompt Tester to refine your prompt content and model selection before deploying to production. Adjust the template, change variables, and re-test \u2014 all without leaving the admin panel.</p>"},{"location":"admin/statistics/","title":"Global Statistics","text":"<p>The Statistics page provides a high-level overview of your system's health, usage trends, and Return on Investment (ROI).</p>"},{"location":"admin/statistics/#time-interval-selector","title":"Time Interval Selector","text":"<p>All time-based views support a configurable time interval. Use the interval selector to adjust the granularity of the data:</p> Interval Description <code>HOUR</code> Hourly breakdown (useful for monitoring recent activity). <code>DAY</code> Daily aggregation (default). <code>WEEK</code> Weekly summary. <code>MONTH</code> Monthly overview. <code>YEAR</code> Yearly totals. <p>The selected interval applies to the Usage Trends view and is passed as the <code>interval</code> query parameter when calling the statistics API.</p>"},{"location":"admin/statistics/#global-metrics","title":"Global Metrics","text":"<p>Real-time counters provide an instant view of the organization's total consumption:</p> <ul> <li>Total Requests: The aggregate number of interactions processed.</li> <li>Total Conversations: The count of unique conversation threads.</li> <li>Total Tokens: The sum of all Input and Output tokens consumed.</li> <li>Total Time Saved: The cumulative estimated hours saved, calculated based on the ROI settings of utilized prompts.</li> </ul> <p>API: <code>GET /api/v1/admin/stats/global</code></p>"},{"location":"admin/statistics/#usage-trends","title":"Usage Trends","text":"<p>Time-series charts displaying activity over the selected time interval:</p> <ul> <li>Request Volume: Number of requests over time.</li> <li>Token Consumption: Input and output tokens over time.</li> </ul> <p>API: <code>GET /api/v1/admin/stats/timeseries?interval=DAY</code></p>"},{"location":"admin/statistics/#llm-distribution","title":"LLM Distribution","text":"<p>A breakdown of which AI models and providers are being utilized the most. Helps identify model concentration and optimize costs.</p> <p>API: <code>GET /api/v1/admin/stats/llm-distribution</code></p>"},{"location":"admin/statistics/#top-prompts-by-time-saved","title":"Top Prompts by Time Saved","text":"<p>A ranking of the most valuable prompts in terms of estimated productivity gains. Each prompt's <code>timeSaved</code> value (set during prompt creation) is multiplied by its usage count to calculate the total ROI.</p> <p>API: <code>GET /api/v1/admin/stats/top-prompts-time-saved</code></p>"},{"location":"admin/statistics/#feature-adoption","title":"Feature Adoption","text":"<p>Visualizes the usage rate of advanced capabilities:</p> <ul> <li>Multi-Modal: Percentage of requests that include image inputs.</li> <li>Function Calling: Percentage of requests that trigger external tools.</li> </ul> <p>API: <code>GET /api/v1/admin/stats/feature-adoption</code></p>"},{"location":"admin/users/","title":"User Management","text":"<p>The User Management section allows administrators to monitor activity at the user level to understand adoption patterns and usage intensity.</p>"},{"location":"admin/users/#user-list","title":"User List","text":"<p>The main view displays all users who have interacted with the system, with the following metrics per user:</p> Metric Description Conversation Count The number of sessions created by the user. Token Usage The total Input and Output tokens consumed. Request Count The total number of individual interactions."},{"location":"admin/users/#user-details","title":"User Details","text":"<p>Selecting a user opens a detailed view of their activity.</p> <ul> <li>Summary Stats: Recaps the user's total consumption and activity metrics.</li> <li>Conversation History: Browse the full list of conversations created by the user.</li> <li>Request History: Inspect individual requests and their LLM responses. This is useful for support troubleshooting and auditing purposes.</li> </ul>"},{"location":"extending/advanced_helpers/","title":"Advanced Helpers: Map-Reduce Paradigm","text":"<p>This guide explains how to implement Advanced Helpers in Uxopian-ai using a Map-Reduce paradigm to summarize or process large documents exceeding standard LLM context limits.</p>"},{"location":"extending/advanced_helpers/#overview","title":"Overview","text":"<p>The Map-Reduce approach allows splitting heavy tasks into manageable chunks processed in parallel (Map) and then combining and reducing the results recursively (Reduce).</p> <p>Workflow:</p> <ol> <li>Client Request: Triggers the main user prompt.</li> <li>Main Prompt: Invokes the Advanced Helper.</li> <li>Helper (Map Phase): Splits document into chunks and processes them in parallel using an internal LLM prompt.</li> <li>Helper (Reduce Phase): Aggregates results and recursively condenses until final summary fits a single context window.</li> <li>Output: Returns final summarized or processed content to the Main Prompt.</li> </ol>"},{"location":"extending/advanced_helpers/#step-1-define-internal-map-prompt","title":"Step 1: Define Internal Map Prompt","text":"<p>Register an internal prompt that the Helper will call for each chunk of data.</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/admin/prompts\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Roles: admin\" \\\n  -d '{\n    \"id\": \"map_chunk_prompt\",\n    \"role\": \"system\",\n    \"content\": \"Role: Expert Analyst.\\nGoal: Extract key facts efficiently.\\nInstructions:\\n1. Extract critical points as bullet list.\\n2. Merge duplicates.\\n3. Output only the content.\\n\\nInput Data:\\n[[${content}]]\",\n    \"defaultLlmProvider\": \"openai\",\n    \"defaultLlmModel\": \"gpt-5.1\",\n    \"timeSaved\": 5\n  }'\n</code></pre> <p>Recommendation: Use a fast, cheap model for the map phase (e.g., <code>gpt-5-mini</code> or <code>gpt-4.1-nano</code>). These intermediate steps process many chunks in parallel \u2014 using a flagship model here would multiply costs with little quality benefit. Reserve the powerful model for the final reduce/synthesis step (Step 3). See Choosing the Right Model for the full model tier guide.</p>"},{"location":"extending/advanced_helpers/#step-2-implement-the-advanced-helper-java","title":"Step 2: Implement the Advanced Helper (Java)","text":"<p>The Helper orchestrates splitting, parallel processing, and recursive reduction.</p> <pre><code>@Service\n@HelperService(name = \"advancedMapReduceHelper\")\npublic class AdvancedMapReduceHelper {\n\n    private static final String INTERNAL_PROMPT_ID = \"map_chunk_prompt\";\n    private final ExecutorService executorService = Executors.newVirtualThreadPerTaskExecutor();\n    private final SecureRequestService requestService;\n\n    public String processDocument(String docId) {\n        List&lt;String&gt; rawContents = fetchDocumentContent(docId);\n        ContextSnapshot contextSnapshot = AiContext.captureSnapshot();\n        try (AiResourceContext ignored = contextSnapshot.restore()) {\n            return recursiveProcess(rawContents, contextSnapshot);\n        }\n    }\n\n    private String recursiveProcess(List&lt;String&gt; segments, ContextSnapshot contextSnapshot) {\n        if (fitsInOneContext(segments)) {\n            return callLlm(String.join(\"\\n\", segments), contextSnapshot);\n        }\n\n        List&lt;String&gt; chunks = createOptimizedChunks(segments);\n        List&lt;CompletableFuture&lt;String&gt;&gt; futures = chunks.stream()\n            .map(chunk -&gt; CompletableFuture.supplyAsync(() -&gt; {\n                try (AiResourceContext ignored = contextSnapshot.restore()) {\n                    return callLlm(chunk, contextSnapshot);\n                }\n            }, executorService))\n            .toList();\n\n        List&lt;String&gt; results = futures.stream().map(CompletableFuture::join).toList();\n        return recursiveProcess(results, contextSnapshot);\n    }\n\n    private String callLlm(String text, ContextSnapshot contextSnapshot) {\n        LlmConfig conf = new LlmConfig();\n        conf.setDisableReasoning(true);\n        return requestService.sendRequestsWithoutHistory(buildRequest(text), conf, currentUser).getAnswer();\n    }\n}\n</code></pre>"},{"location":"extending/advanced_helpers/#step-3-create-main-user-prompt","title":"Step 3: Create Main User Prompt","text":"<p>The user-facing prompt triggers the Advanced Helper and specifies output constraints.</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/admin/prompts\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Roles: admin\" \\\n  -d '{\n    \"id\": \"process_long_document\",\n    \"role\": \"user\",\n    \"content\": \"Please summarize the document in 500 words:\\n\\n[[${advancedMapReduceHelper.processDocument(documentId)}]]\",\n    \"defaultLlmProvider\": \"openai\",\n    \"defaultLlmModel\": \"gpt-5.1\",\n    \"timeSaved\": 60\n  }'\n</code></pre> <p>Helper Call: <code>[[${advancedMapReduceHelper.processDocument(documentId)}]]</code></p>"},{"location":"extending/advanced_helpers/#step-4-usage-via-api","title":"Step 4: Usage via API","text":"<p>Invoke the prompt and provide the document ID.</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/requests?conversation=uuid\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Id: user-id\" \\\n  -d '{\n    \"inputs\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"prompt\",\n            \"value\": \"process_long_document\",\n            \"payload\": {\"documentId\": \"doc-12345\"}\n          }\n        ]\n      }\n    ]\n  }'\n</code></pre> <p>Result</p> <ol> <li>System loads <code>process_long_document</code> template.</li> <li>Advanced Helper executes Map-Reduce over document chunks.</li> <li>Intermediate results are recursively merged.</li> <li>Final summary is returned as a concise 500-word narrative.</li> </ol>"},{"location":"extending/custom_auth_provider/","title":"Adding a Custom Gateway Authentication Provider","text":"<p>The Uxopian Gateway (BFF) is a standalone service, deployed independently from the AI service, that acts as the security entry point for the platform. It is responsible for authenticating incoming requests and establishing a security context (Tenant ID, User ID, Roles) before forwarding traffic to the core AI service.</p> <p>To integrate with your organization's specific authentication mechanism (e.g., OAuth2 introspection, LDAP, Custom Headers, or JWT validation), you must implement a custom Auth Provider.</p>"},{"location":"extending/custom_auth_provider/#overview","title":"Overview","text":"<p>The Gateway acts as a proxy. Your custom provider intercepts the <code>ServerHttpRequest</code>, validates the credentials, and returns an <code>AuthenticatedUser</code> object. The Gateway then injects these details as secure headers (<code>X-User-Tenant</code>, <code>X-User-Id</code>) when calling the backend.</p>"},{"location":"extending/custom_auth_provider/#prerequisites","title":"Prerequisites","text":"<ul> <li>Java 21+: The Gateway service requires Java 21 or higher.</li> <li>Maven Settings: Your <code>.m2/settings.xml</code> must be configured to access Uxopian-ai artifacts.</li> </ul>"},{"location":"extending/custom_auth_provider/#step-1-project-configuration","title":"Step 1: Project Configuration","text":"<p>Create a new Maven module. Unlike LLM connectors, this module relies on the <code>gateway</code> parent.</p> <p><code>pom.xml</code> Setup:</p> <p>You must include the <code>maven-shade-plugin</code> configured to exclude Spring Boot starters to avoid conflicts, as this JAR will be loaded dynamically by the main application.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"[http://maven.apache.org/POM/4.0.0](http://maven.apache.org/POM/4.0.0)\"\n         xmlns:xsi=\"[http://www.w3.org/2001/XMLSchema-instance](http://www.w3.org/2001/XMLSchema-instance)\"\n         xsi:schemaLocation=\"[http://maven.apache.org/POM/4.0.0](http://maven.apache.org/POM/4.0.0) [http://maven.apache.org/xsd/maven-4.0.0.xsd](http://maven.apache.org/xsd/maven-4.0.0.xsd)\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;artifactId&gt;my-custom-auth-provider&lt;/artifactId&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.uxopian.ai.gateway&lt;/groupId&gt;\n            &lt;artifactId&gt;model&lt;/artifactId&gt;\n            &lt;version&gt;${project.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.projectreactor&lt;/groupId&gt;\n            &lt;artifactId&gt;reactor-test&lt;/artifactId&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n            &lt;version&gt;${reactor.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n\n    &lt;build&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n                &lt;version&gt;3.5.2&lt;/version&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;shade&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;filters&gt;\n                                &lt;filter&gt;\n                                    &lt;artifact&gt;*:*&lt;/artifact&gt;\n                                    &lt;excludes&gt;\n                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;\n                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;\n                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;\n                                    &lt;/excludes&gt;\n                                &lt;/filter&gt;\n                            &lt;/filters&gt;\n                            &lt;artifactSet&gt;\n                                &lt;excludes&gt;\n                                    &lt;exclude&gt;org.springframework.boot:*&lt;/exclude&gt;\n                                    &lt;exclude&gt;org.springframework:*&lt;/exclude&gt;\n                                &lt;/excludes&gt;\n                            &lt;/artifactSet&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n&lt;/project&gt;\n</code></pre>"},{"location":"extending/custom_auth_provider/#step-2-implement-the-provider","title":"Step 2: Implement the Provider","text":"<p>You must create a class that implements <code>com.uxopian.ai.bff.gateway.model.provider.AuthProvider</code>.</p> <p>The Gateway uses Spring WebFlux (Reactive Stack), so your implementation must return a <code>Mono&lt;AuthenticatedUser&gt;</code>.</p> <p>Crucial: You must annotate this class with <code>@Service(\"YourProviderName\")</code>. This name maps directly to the configuration in <code>application.yml</code>.</p> <p>Example: <code>DevProvider.java</code> This example demonstrates extracting identity from raw headers (useful for development or behind trusted proxies).</p> <pre><code>package com.uxopian.ai.bff.gateway.development.provider;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.http.server.reactive.ServerHttpRequest;\nimport org.springframework.stereotype.Service;\nimport org.springframework.util.StringUtils;\nimport com.uxopian.ai.bff.gateway.model.provider.AuthProvider;\nimport com.uxopian.ai.bff.gateway.model.user.AuthenticatedUser;\nimport reactor.core.publisher.Mono;\n\n@Service(\"DevProvider\") // &lt;--- This Name is CRITICAL\npublic class DevProvider implements AuthProvider {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(DevProvider.class);\n    private static final String HEADER_USER_ID = \"X-User-Id\";\n    private static final String HEADER_USER_ROLES = \"X-User-Roles\";\n    private static final String HEADER_TENANT_ID = \"X-User-Tenant\";\n\n    @Override\n    public Mono&lt;AuthenticatedUser&gt; authenticate(ServerHttpRequest request) {\n\n        // 1. Extract credentials or headers from the request\n        String userId = request.getHeaders().getFirst(HEADER_USER_ID);\n        String rolesRaw = request.getHeaders().getFirst(HEADER_USER_ROLES);\n        String tenantId = request.getHeaders().getFirst(HEADER_TENANT_ID);\n\n        // 2. Validate (Simplified for example)\n        if (!StringUtils.hasText(userId)) {\n            LOGGER.warn(\"DevProvider: Header '{}' is missing.\", HEADER_USER_ID);\n            // Return empty user or throw exception depending on security requirements\n            return Mono.just(new AuthenticatedUser());\n        }\n\n        LOGGER.info(\"Authenticating user '{}'\", userId);\n\n        // 3. Construct the AuthenticatedUser object\n        AuthenticatedUser user = new AuthenticatedUser();\n        user.setId(userId);\n        user.setToken(\"dev-dummy-token\"); // Set the token to be passed downstream\n\n        if (StringUtils.hasText(tenantId)) {\n            user.setTenantId(tenantId);\n        }\n\n        if (StringUtils.hasText(rolesRaw)) {\n            String[] roles = rolesRaw.split(\",\");\n            for (String role : roles) {\n                user.addRole(role.trim());\n            }\n        }\n\n        // 4. Return as a Reactive Mono\n        return Mono.just(user);\n    }\n}\n</code></pre>"},{"location":"extending/custom_auth_provider/#step-3-gateway-configuration","title":"Step 3: Gateway Configuration","text":"<p>Once your code is ready, you must configure the Gateway to use this specific provider for the AI service route.</p> <p>In your <code>gateway-service</code> configuration (e.g., <code>application.yml</code>), locate the <code>routes</code> section. Update the <code>provider</code> field to match your <code>@Service</code> annotation.</p> <pre><code>app:\n  gateway:\n    provider-header: X-Provider-ID\n  routes:\n    - id: uxopian-ai\n      uri: http://uxopian-ai-standalone:8080 # The internal address of the Core Service\n      prefix: /uxopian-ai/\n      path: /uxopian-ai/**\n      rewritePath: /uxopian-ai/?(?&lt;segment&gt;.*), /uxopian-ai/${segment}\n\n      # MAPPING HAPPENS HERE:\n      provider: DevProvider # Must match @Service(\"DevProvider\")\n\n      # Public/Private Path Security Rules\n      security:\n        - path: /.well-known/**\n          public: true\n        - path: /swagger-ui/**\n          public: true\n        - path: /api/v1/admin/**\n          # roles: [ \"ADMIN\" ] # Uncomment to enforce role checks\n          public: true\n</code></pre>"},{"location":"extending/custom_auth_provider/#step-4-deployment","title":"Step 4: Deployment","text":"<p>The Gateway loads providers from a plugin directory defined by <code>auth.provider.path</code> (default: <code>provider/</code>).</p>"},{"location":"extending/custom_auth_provider/#docker-deployment","title":"Docker Deployment","text":"<p>You must use the Gateway Service base image (different from the core AI image).</p> <p>Dockerfile Example:</p> <pre><code># Start from the Gateway Service image\nFROM artifactory.arondor.cloud:5001/uxopian-ai/gateway-service:2026.0.0-ft2-full\n\n# Copy your custom Auth Provider Fat JAR into the provider directory\nCOPY ./target/my-custom-auth-provider-1.0-SNAPSHOT.jar /app/provider/\n</code></pre>"},{"location":"extending/custom_auth_provider/#verification","title":"Verification","text":"<ol> <li>Restart the Gateway container.</li> <li>Monitor Logs: Ensure the service starts without errors regarding missing beans.</li> <li>Test: Send a request to the Gateway URL (e.g., <code>https://gateway/uxopian-ai/api/v1/users/details</code>) with the headers required by your logic.</li> <li>Confirm: Check that the Core Service receives the request with the injected <code>X-User-Tenant</code> and <code>X-User-Id</code> headers.</li> </ol>"},{"location":"extending/custom_helpers/","title":"How to Create Custom Prompt Helpers","text":"<p>Helpers are custom Java services that allow you to inject dynamic data or perform server-side operations directly within your LLM prompts.</p>"},{"location":"extending/custom_helpers/#overview","title":"Overview","text":"<p>Uxopian-ai uses the Thymeleaf engine to interpret prompts. By creating a custom Helper, you expose Java objects and methods that can be called using the standard Thymeleaf syntax: <code>[[${helperName.methodName()}]]</code>.</p> <p>Common Use Cases:</p> <ul> <li>Data Enrichment: Fetching real-time data (Weather, Stock prices, Time).</li> <li>Content Retrieval: Calling external systems (e.g., FlowerDocs) to retrieve document text or images.</li> <li>Data Transformation: Formatting dates, hashing strings, or parsing complex inputs before sending them to the LLM.</li> </ul>"},{"location":"extending/custom_helpers/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure your development environment is configured to access the Uxopian artifacts.</p> <ol> <li>Maven Settings: You must configure your <code>.m2/settings.xml</code> to include the credentials and repository definitions required to download the <code>com.uxopian.ai</code> libraries.</li> <li>Java Development Kit (JDK): Ensure you are using a compatible JDK version (Java 21+ recommended).</li> </ol>"},{"location":"extending/custom_helpers/#step-1-project-configuration","title":"Step 1: Project Configuration","text":"<p>Create a new Maven project or module. You need to import the <code>shared</code> parent project and the <code>annotation</code> dependency to mark your services correctly.</p> <p>Add the following to your <code>pom.xml</code> (adjust the <code>&lt;version&gt;</code> to match your current Uxopian-ai deployment):</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"[http://maven.apache.org/POM/4.0.0](http://maven.apache.org/POM/4.0.0)\"\n         xmlns:xsi=\"[http://www.w3.org/2001/XMLSchema-instance](http://www.w3.org/2001/XMLSchema-instance)\"\n         xsi:schemaLocation=\"[http://maven.apache.org/POM/4.0.0](http://maven.apache.org/POM/4.0.0) [http://maven.apache.org/xsd/maven-4.0.0.xsd](http://maven.apache.org/xsd/maven-4.0.0.xsd)\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;artifactId&gt;my-custom-helper&lt;/artifactId&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.uxopian.ai&lt;/groupId&gt;\n            &lt;artifactId&gt;annotation&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre>"},{"location":"extending/custom_helpers/#step-2-implement-the-service","title":"Step 2: Implement the Service","text":"<p>To create a helper, you must create a Java class annotated with <code>@Component</code> (standard Spring) and <code>@HelperService</code>.</p> <p>The <code>name</code> attribute defined in <code>@HelperService</code> is the variable name you will use in your prompt.</p> <pre><code>package com.mycompany.uxopian.helpers;\n\nimport org.springframework.stereotype.Component;\nimport com.uxopian.ai.model.annotation.helper.HelperService;\n\n@Component\n@HelperService(name = \"myTools\") // This service will be accessible as 'myTools' in prompts\npublic class MyCustomHelper {\n\n    /**\n     * Example method to get current status.\n     * Usage in prompt: [[${myTools.getStatus()}]]\n     */\n    public String getStatus() {\n        return \"System is operational\";\n    }\n\n    /**\n     * Example method with parameters.\n     * Usage in prompt: [[${myTools.greet(userName)}]]\n     */\n    public String greet(String name) {\n        return \"Hello, \" + name + \"!\";\n    }\n}\n</code></pre>"},{"location":"extending/custom_helpers/#step-3-building-the-artifact-fat-jar","title":"Step 3: Building the Artifact (Fat JAR)","text":"<p>Because your helper might rely on third-party libraries that are not present in the core Uxopian-ai classpath, you must package your helper as a Fat JAR (or Uber JAR).</p> <p>This ensures all your specific dependencies are included inside the JAR file. You can achieve this using the <code>maven-shade-plugin</code>.</p> <p>Dependency Conflicts</p> <p>Ensure you do not bundle core Spring or Uxopian dependencies that conflict with the running server. Scope strictly necessary dependencies.</p>"},{"location":"extending/custom_helpers/#step-4-deployment","title":"Step 4: Deployment","text":"<p>Uxopian-ai loads helpers from a specific directory on the filesystem, typically <code>helper-services/</code>.</p>"},{"location":"extending/custom_helpers/#docker-deployment-recommended","title":"Docker Deployment (Recommended)","text":"<p>When running Uxopian-ai in a containerized environment, the best practice is to build a custom Docker image that extends the base image and adds your JAR.</p> <p>Dockerfile Example:</p> <pre><code># Start from the specific version of Uxopian-ai\nFROM artifactory.arondor.cloud:5001/uxopian-ai:v2026.0.0-ft1-rc2-full\n\n# Copy your custom helper Fat JAR into the loader directory\nCOPY ./target/my-custom-helper-1.0-SNAPSHOT.jar /app/helper-services/\n</code></pre>"},{"location":"extending/custom_helpers/#step-5-usage-in-prompts-requests","title":"Step 5: Usage in Prompts &amp; Requests","text":"<p>Once deployed, your helper is ready to be used in Prompts.</p>"},{"location":"extending/custom_helpers/#1-the-prompt-template","title":"1. The Prompt Template","text":"<p>In your Thymeleaf prompt, call the method using the name defined in the annotation.</p> <p>Example Prompt:</p> <pre><code>The system status is: [[${myTools.getStatus()}]]\nPlease write a polite message for: [[${myTools.greet(customerName)}]]\n</code></pre>"},{"location":"extending/custom_helpers/#2-passing-parameters-the-payload","title":"2. Passing Parameters (The Payload)","text":"<p>If your helper method requires parameters (like <code>greet(String name)</code> above), the values for these parameters must be provided in the request payload.</p> <p>Crucial Rule: The variable names in the payload must match the variable names used in the method call within the prompt.</p> <p>API Request Example: If your prompt contains <code>[[${myTools.greet(customerName)}]]</code>, your JSON payload to the request endpoint must include <code>customerName</code>:</p> <pre><code>{\n  \"conversation\": \"...\",\n  \"inputs\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"...\",\n      \"payload\": {\n        \"customerName\": \"Alice\"\n      }\n    }\n  ]\n}\n</code></pre> <p>Troubleshooting</p> <p>If your helper returns null or throws an error, verify that: 1. The payload key matches the argument name used in the Thymeleaf expression exactly. 2. The variable type in the payload matches the method signature (e.g., Integer vs String).</p>"},{"location":"extending/custom_tools/","title":"How to Create Custom Prompt Tools","text":""},{"location":"extending/custom_tools/#tools","title":"Tools","text":"<p>Tools are custom Java services that empower the LLM to perform actions or retrieve information autonomously. Unlike Helpers (which inject data into the prompt before generation), Tools are Function Calling capabilities that the AI can decide to execute during the conversation.</p>"},{"location":"extending/custom_tools/#overview","title":"Overview","text":"<p>Uxopian-ai uses LangChain4j to manage Tools. By creating a custom Tool, you provide the LLM with a set of methods (functions) described in natural language. The LLM analyzes the user's request and determines if and when to call your Java methods.</p>"},{"location":"extending/custom_tools/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Action Execution: Sending emails, creating Jira tickets, updating a database.</li> <li>Dynamic Queries: Searching an SQL database based on natural language criteria.</li> <li>Complex Calculations: Performing mathematical operations that LLMs struggle with.</li> </ul>"},{"location":"extending/custom_tools/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure your development environment is configured properly.</p> <ul> <li>Maven Settings: Configure your <code>.m2/settings.xml</code> to access the <code>com.uxopian.ai</code> libraries.</li> <li>Java Development Kit (JDK): Ensure you are using a compatible JDK version (Java 21+ recommended).</li> </ul>"},{"location":"extending/custom_tools/#step-1-project-configuration","title":"Step 1: Project Configuration","text":"<p>Create a new Maven project. You need to import the Uxopian annotation dependency (to mark the service) and the langchain4j-core dependency (to define the tool methods).</p> <p>Add the following to your <code>pom.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;artifactId&gt;my-custom-tool&lt;/artifactId&gt;\n\n    &lt;properties&gt;\n        &lt;langchain4j.version&gt;1.11.0&lt;/langchain4j.version&gt;\n    &lt;/properties&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.uxopian.ai&lt;/groupId&gt;\n            &lt;artifactId&gt;annotation&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n\n        &lt;dependency&gt;\n            &lt;groupId&gt;dev.langchain4j&lt;/groupId&gt;\n            &lt;artifactId&gt;langchain4j-core&lt;/artifactId&gt;\n            &lt;version&gt;${langchain4j.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre>"},{"location":"extending/custom_tools/#step-2-implement-the-service","title":"Step 2: Implement the Service","text":"<p>The Uxopian-ai Tool Loader scans for classes annotated with <code>@ToolService</code>. These classes must also be standard Spring beans (annotated with <code>@Component</code> or <code>@Service</code>).</p>"},{"location":"extending/custom_tools/#1-the-tool-class","title":"1. The Tool Class","text":"<p>You must use the <code>@Tool</code> annotation from LangChain4j on the methods you want to expose to the AI. The text inside <code>@Tool(\"...\")</code> is crucial: it is the description the LLM reads to understand what the tool does.</p> <pre><code>package com.mycompany.uxopian.tools;\n\nimport org.springframework.stereotype.Service;\nimport com.uxopian.ai.model.annotation.tool.ToolService;\nimport dev.langchain4j.agent.tool.Tool;\n\n@Service(\"bookingTool\") // Standard Spring annotation\n@ToolService // Marks this as a Tool for Uxopian-ai scanning\npublic class BookingTool {\n\n    private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BookingTool.class);\n\n    /**\n     * The description inside @Tool is what the LLM sees.\n     * Be descriptive about what the method does and what the parameters represent.\n     */\n    @Tool(\"Checks the availability of a meeting room for a specific date\")\n    public boolean checkAvailability(String roomName, String date) {\n        // Logic to check database or external API\n        logger.info(\"Checking availability for {} on {}\", roomName, date);\n        return true;\n    }\n\n    @Tool(\"Books a meeting room if available\")\n    public String bookRoom(String roomName, String date, String organizer) {\n        return \"Room \" + roomName + \" successfully booked for \" + organizer + \" on \" + date;\n    }\n}\n</code></pre>"},{"location":"extending/custom_tools/#2-internal-dependencies-optional","title":"2. Internal Dependencies (Optional)","text":"<p>The <code>ToolServiceLoader</code> also scans and registers internal beans found in your JAR. If your Tool relies on a repository or a helper class, simply annotate them with <code>@Component</code> or <code>@Service</code> (without <code>@ToolService</code>), and they will be injected automatically.</p> <pre><code>@Component\npublic class InternalDatabaseConnector {\n    // This bean is not exposed to the LLM, but can be Autowired into BookingTool\n}\n</code></pre>"},{"location":"extending/custom_tools/#step-3-building-the-artifact-fat-jar","title":"Step 3: Building the Artifact (Fat JAR)","text":"<p>Just like Helpers, Tools must be packaged as a Fat JAR (Uber JAR) to include all specific dependencies (e.g., database drivers, specific HTTP clients) that are not part of the core platform.</p> <p>When building your Fat JAR, you must exclude <code>langchain4j-core</code> (and other platform-provided libraries) to avoid classpath conflicts at runtime.</p>"},{"location":"extending/custom_tools/#maven-shade-plugin-example","title":"Maven Shade Plugin Example","text":"<pre><code>&lt;build&gt;\n    &lt;plugins&gt;\n        &lt;plugin&gt;\n            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;\n            &lt;version&gt;3.5.0&lt;/version&gt;\n            &lt;executions&gt;\n                &lt;execution&gt;\n                    &lt;phase&gt;package&lt;/phase&gt;\n                    &lt;goals&gt;\n                        &lt;goal&gt;shade&lt;/goal&gt;\n                    &lt;/goals&gt;\n                    &lt;configuration&gt;\n                        &lt;artifactSet&gt;\n                            &lt;excludes&gt;\n                                &lt;exclude&gt;dev.langchain4j:langchain4j-core&lt;/exclude&gt;\n                            &lt;/excludes&gt;\n                        &lt;/artifactSet&gt;\n                    &lt;/configuration&gt;\n                &lt;/execution&gt;\n            &lt;/executions&gt;\n        &lt;/plugin&gt;\n    &lt;/plugins&gt;\n&lt;/build&gt;\n</code></pre> <p>\u26a0\ufe0f Dependency Conflicts Be careful not to include <code>langchain4j-core</code> or Spring framework JARs in your final artifact if they are already provided by the platform, unless you specifically need to override them (which is not recommended).</p>"},{"location":"extending/custom_tools/#step-4-deployment","title":"Step 4: Deployment","text":"<p>Uxopian-ai scans a specific directory for tools, typically configured as <code>tools/</code>.</p>"},{"location":"extending/custom_tools/#docker-deployment-recommended","title":"Docker Deployment (Recommended)","text":"<p>Add your compiled JAR to the <code>tools/</code> directory in your custom Docker image.</p> <p>Dockerfile example:</p> <pre><code># Start from the specific version of Uxopian-ai\nFROM artifactory.arondor.cloud:5001/uxopian-ai:v2026.0.0-ft1-rc2-full\n\n# Copy your custom Tool Fat JAR into the tools directory\nCOPY ./target/my-custom-tool-1.0-SNAPSHOT.jar /app/tools/\n</code></pre>"},{"location":"extending/custom_tools/#step-5-usage","title":"Step 5: Usage","text":"<p>Unlike Helpers, you do not call Tools explicitly in your prompt (e.g., no <code>[[${...}]]</code>).</p> <ol> <li>Enable the Tool: In the Uxopian-ai configuration (or Assistant setup), ensure your new Tool is selected/enabled for the conversation context.</li> <li>Prompting: Simply ask the LLM to perform the task.</li> </ol>"},{"location":"extending/custom_tools/#example-scenario","title":"Example Scenario","text":"<p>User says:</p> <p>\"Can you book the 'Red Room' for me for tomorrow?\"</p> <p>LLM process:</p> <ol> <li>The LLM reads the <code>@Tool</code> description: \"Books a meeting room if available\".</li> <li> <p>It extracts the parameters:</p> </li> <li> <p><code>roomName = \"Red Room\"</code></p> </li> <li><code>date = \"2025-10-20\"</code></li> <li> <p><code>organizer = \"User\"</code></p> </li> <li> <p>It executes the Java method <code>bookRoom(...)</code> server-side.</p> </li> <li>It receives the returned <code>String</code>.</li> </ol> <p>LLM response:</p> <p>\"I have successfully booked the Red Room for you for tomorrow.\"</p>"},{"location":"extending/custom_tools/#best-practices-for-descriptions","title":"Best Practices for Descriptions","text":"<p>The quality of your Tool depends directly on the quality of your <code>@Tool</code> annotation descriptions.</p> <p>Bad:</p> <pre><code>@Tool(\"Get data\")\n</code></pre> <p>Good:</p> <pre><code>@Tool(\"Retrieves the current stock price for a given ticker symbol (e.g., AAPL)\")\n</code></pre>"},{"location":"extending/new_provider/","title":"Adding a New LLM Provider (Custom Connector)","text":"<p>Uxopian-ai is built on an extensible architecture that allows you to integrate any Large Language Model (LLM).</p> <p>While the system uses LangChain4j as its internal abstraction layer, you are not limited to existing adapters. You can build a fully custom connector to integrate:</p> <ul> <li>Internal/Local models hosted on your infrastructure.</li> <li>Proprietary APIs not supported by standard libraries.</li> <li>Mock/Test services for development.</li> </ul> <p>This guide demonstrates how to implement a connector from scratch using a hypothetical \"FakeLLM\" that simulates responses.</p>"},{"location":"extending/new_provider/#prerequisites","title":"Prerequisites","text":"<ul> <li>Java 21+: The LLM Connector architecture requires Java 21 or higher.</li> <li>Maven Settings: Your <code>.m2/settings.xml</code> must be configured to access Uxopian-ai artifacts.</li> </ul>"},{"location":"extending/new_provider/#step-1-project-configuration","title":"Step 1: Project Configuration","text":"<p>Create a new Maven module. Inherit from the <code>shared</code> parent and include the <code>llm-connector</code> artifact.</p> <p><code>pom.xml</code> Setup:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"[http://maven.apache.org/POM/4.0.0](http://maven.apache.org/POM/4.0.0)\"\n         xmlns:xsi=\"[http://www.w3.org/2001/XMLSchema-instance](http://www.w3.org/2001/XMLSchema-instance)\"\n         xsi:schemaLocation=\"[http://maven.apache.org/POM/4.0.0](http://maven.apache.org/POM/4.0.0) [http://maven.apache.org/xsd/maven-4.0.0.xsd](http://maven.apache.org/xsd/maven-4.0.0.xsd)\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\n    &lt;artifactId&gt;my-custom-connector&lt;/artifactId&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;com.uxopian.ai&lt;/groupId&gt;\n            &lt;artifactId&gt;llm-connector&lt;/artifactId&gt;\n        &lt;/dependency&gt;\n        &lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre>"},{"location":"extending/new_provider/#step-2-implement-the-chat-logic-chatmodel","title":"Step 2: Implement the Chat Logic (<code>ChatModel</code>)","text":"<p>To create a custom connector, you must implement the <code>dev.langchain4j.model.chat.ChatModel</code> interface. This is where the core logic resides: receiving a request, processing it (or calling an external API), and returning a standardized response.</p> <p>Example: <code>FakeChatModel.java</code></p> <pre><code>package com.uxopian.ai.fakellm.client;\n\nimport java.util.List;\nimport dev.langchain4j.data.message.AiMessage;\nimport dev.langchain4j.data.message.ChatMessage;\nimport dev.langchain4j.data.message.UserMessage;\nimport dev.langchain4j.model.chat.ChatModel;\nimport dev.langchain4j.model.chat.request.ChatRequest;\nimport dev.langchain4j.model.chat.response.ChatResponse;\nimport dev.langchain4j.model.output.FinishReason;\nimport dev.langchain4j.model.output.TokenUsage;\n\npublic class FakeChatModel implements ChatModel {\n\n    private final String modelName;\n\n    public FakeChatModel(String modelName) {\n        this.modelName = modelName;\n    }\n\n    @Override\n    public ChatResponse chat(ChatRequest request) {\n        // 1. Extract the last message from the user\n        String userText = findLastUserMessage(request.messages());\n\n        // 2. Simulate Generating a Response (In a real scenario, call your API here)\n        String fakeResponse = \"FakeLLM (\" + modelName + \") says: I received your message: '\" + userText + \"'\";\n\n        // 3. Calculate simulated token usage\n        TokenUsage usage = new TokenUsage(10, 20, 30);\n\n        // 4. Return the standardized ChatResponse\n        return ChatResponse.builder()\n                .modelName(modelName)\n                .tokenUsage(usage)\n                .aiMessage(AiMessage.from(fakeResponse))\n                .finishReason(FinishReason.STOP)\n                .build();\n    }\n\n    // Helper to find the text input\n    private String findLastUserMessage(List&lt;ChatMessage&gt; messages) {\n        return messages.stream()\n                .filter(UserMessage.class::isInstance)\n                .map(m -&gt; ((UserMessage) m).singleText())\n                .reduce((first, second) -&gt; second)\n                .orElse(\"Hello\");\n    }\n}\n</code></pre>"},{"location":"extending/new_provider/#step-3-implement-streaming-streamingchatmodel","title":"Step 3: Implement Streaming (<code>StreamingChatModel</code>)","text":"<p>You must also implement <code>StreamingChatModel</code>. This interface allows the backend to stream tokens to the user interface in real-time.</p> <p>Example: <code>FakeStreamingChatModel.java</code></p> <pre><code>package com.uxopian.ai.fakellm.client;\n\nimport dev.langchain4j.data.message.AiMessage;\nimport dev.langchain4j.model.chat.StreamingChatModel;\nimport dev.langchain4j.model.chat.request.ChatRequest;\nimport dev.langchain4j.model.chat.response.ChatResponse;\nimport dev.langchain4j.model.chat.response.StreamingChatResponseHandler;\nimport dev.langchain4j.model.output.FinishReason;\nimport dev.langchain4j.model.output.TokenUsage;\n\npublic class FakeStreamingChatModel implements StreamingChatModel {\n\n    private final String modelName;\n\n    public FakeStreamingChatModel(String modelName) {\n        this.modelName = modelName;\n    }\n\n    @Override\n    public void chat(ChatRequest request, StreamingChatResponseHandler handler) {\n        try {\n            // Simulate a streaming response by sending chunks with a delay\n            String[] chunks = {\"Hello \", \"this \", \"is \", \"Fake \", \"Streaming...\"};\n            StringBuilder fullResponse = new StringBuilder();\n\n            for (String chunk : chunks) {\n                // Emit partial token\n                handler.onPartialResponse(chunk);\n                fullResponse.append(chunk);\n\n                // Simulate network latency\n                Thread.sleep(200);\n            }\n\n            // Finalize the stream\n            TokenUsage usage = new TokenUsage(10, 10, 20);\n            ChatResponse response = ChatResponse.builder()\n                    .aiMessage(AiMessage.from(fullResponse.toString()))\n                    .tokenUsage(usage)\n                    .finishReason(FinishReason.STOP)\n                    .build();\n\n            handler.onCompleteResponse(response);\n\n        } catch (Exception e) {\n            handler.onError(e);\n        }\n    }\n}\n</code></pre>"},{"location":"extending/new_provider/#step-4-create-the-provider-service","title":"Step 4: Create the Provider Service","text":"<p>Finally, create the Service that registers your provider with Uxopian-ai. This class implements <code>ModelProvider</code> and acts as a factory for your models.</p> <p>Crucial: You must annotate this class with <code>@Service(\"your-provider-name\")</code>. This name is what you will use in the API calls and provider configurations (e.g., <code>provider=fake-llm</code>).</p> <p>Recommended: Extend <code>AbstractLlmClient</code></p> <p>Instead of implementing <code>ModelProvider</code> directly, extend <code>AbstractLlmClient</code>. It provides the <code>applyIfNotNull(value, setter)</code> helper method for cleanly applying optional configuration parameters from <code>LlmModelConf</code>.</p> <p>Example: <code>FakeLLMClient.java</code></p> <pre><code>package com.uxopian.ai.fakellm.client;\n\nimport org.springframework.stereotype.Service;\nimport com.uxopian.ai.model.llm.connector.AbstractLlmClient;\nimport com.uxopian.ai.model.llm.connector.LlmModelConf;\nimport dev.langchain4j.model.chat.ChatModel;\nimport dev.langchain4j.model.chat.StreamingChatModel;\n\n@Service(\"fake-llm\") // &lt;--- This ID is used in API calls and provider configurations\npublic class FakeLLMClient extends AbstractLlmClient {\n\n    @Override\n    public ChatModel createChatModelInstance(LlmModelConf params) {\n        // Use params to access all configuration: model name, API key, temperature, etc.\n        return new FakeChatModel(params.getModelName());\n    }\n\n    @Override\n    public StreamingChatModel createStreamingChatModelInstance(LlmModelConf params) {\n        return new FakeStreamingChatModel(params.getModelName());\n    }\n}\n</code></pre> <p>The <code>LlmModelConf</code> parameter provides access to all configuration fields:</p> <ul> <li><code>params.getModelName()</code> \u2014 The actual model identifier (e.g., <code>fake-gpt-v1</code>).</li> <li><code>params.getApiSecret()</code> \u2014 The API key (decrypted automatically).</li> <li><code>params.getEndpointUrl()</code> \u2014 The provider API base URL.</li> <li><code>params.getTemperature()</code>, <code>params.getMaxTokens()</code>, <code>params.getTimeout()</code>, etc. \u2014 Generation parameters.</li> </ul> <p>These values are the result of merging the provider's global configuration with model-specific overrides. See Parameter Precedence.</p>"},{"location":"extending/new_provider/#step-5-register-provider-configuration","title":"Step 5: Register Provider Configuration","text":"<p>After deploying the provider bean, you must create a provider configuration (<code>LlmProviderConf</code>) to define which models are available and their parameters. This can be done in three ways:</p> <ol> <li> <p>YAML Bootstrapping \u2014 Add a <code>llm.provider.globals</code> entry in <code>llm-clients-config.yml</code>. It will be loaded into OpenSearch at startup. See Configuration Files.</p> </li> <li> <p>Admin API \u2014 Create a configuration via <code>POST /api/v1/admin/llm/provider-conf</code>. See REST API Reference.</p> </li> <li> <p>Admin UI \u2014 Use the LLM Provider Management page to visually configure the provider and test connectivity.</p> </li> </ol>"},{"location":"extending/new_provider/#step-6-packaging-deployment","title":"Step 6: Packaging &amp; Deployment","text":""},{"location":"extending/new_provider/#packaging-fat-jar","title":"Packaging (Fat JAR)","text":"<p>Even for simple connectors, it is best practice to package your provider as a Fat JAR (Uber JAR) to ensure all specific dependencies are included and do not conflict with the platform's classpath.</p> <p>Use the <code>maven-shade-plugin</code> or <code>spring-boot-maven-plugin</code> (repackage goal).</p>"},{"location":"extending/new_provider/#deployment-docker","title":"Deployment (Docker)","text":"<p>Add the JAR to the <code>/app/provider/</code> directory in your Docker image.</p> <pre><code># Start from the specific version of Uxopian-ai\nFROM artifactory.arondor.cloud:5001/uxopian-ai:2026.0.0-ft2-full\n\n# Copy your custom provider Fat JAR into the provider directory\nCOPY ./target/custom-fakellm-provider-1.0.jar /app/provider/\n</code></pre>"},{"location":"extending/new_provider/#step-7-verification-testing","title":"Step 7: Verification &amp; Testing","text":"<p>Once your provider is deployed and the application has restarted, you can verify it by sending a standard HTTP request using <code>cURL</code>.</p> <p>The key parameter here is <code>provider=fake-llm</code>, which matches the value defined in your <code>@Service(\"fake-llm\")</code> annotation.</p>"},{"location":"extending/new_provider/#curl-example","title":"cURL Example","text":"<pre><code>curl -X POST \"https://&lt;your-uxopian-endpoint&gt;/api/v1/requests?provider=fake-llm\" \\\n     -H \"Authorization: Bearer &lt;YOUR_ACCESS_TOKEN&gt;\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n           \"inputs\": [\n             {\n               \"role\": \"user\",\n               \"content\": [\n                 {\n                   \"type\": \"text\",\n                   \"value\": \"Hello, are you a real AI?\"\n                 }\n               ]\n             }\n           ]\n         }'\n</code></pre>"},{"location":"extending/new_provider/#expected-response","title":"Expected Response","text":"<p>Based on the logic implemented in <code>FakeChatModel.java</code>, the system should return a JSON response containing the simulated text:</p> <pre><code>{\n  \"answer\": \"FakeLLM (fake-gpt-v1) says: I received your message: 'Hello, are you a real AI?'\",\n  \"inputTokenCount\": 10,\n  \"outputTokenCount\": 20,\n  \"llmName\": \"fake-gpt-v1\",\n  \"id\": \"abc12345\",\n  \"createdAt\": \"2025-05-15T10:00:00\"\n}\n</code></pre>"},{"location":"getting_started/installation_guide/","title":"\ud83d\udcda Guide: Uxopian AI Service Deployment","text":"<p>This guide covers the installation process for the Uxopian AI backend service and its vector database.</p>"},{"location":"getting_started/installation_guide/#backend-deployment-ai-service","title":"\ud83d\udce6 Backend Deployment (AI Service)","text":"<p>This section covers the installation of the AI engine and its vector database. Two scenarios are possible: Docker (Recommended via Starter Kit) or Standalone (Native Java).</p>"},{"location":"getting_started/installation_guide/#scenario-a-docker-deployment-starter-kit","title":"\ud83d\ude80 Scenario A: Docker Deployment (Starter Kit)","text":"<p>The Starter Kit provides a ready-to-use stack containing the AI service and an OpenSearch node.</p>"},{"location":"getting_started/installation_guide/#step-1-download-and-structure","title":"\ud83d\udd39 Step 1: Download and Structure","text":"<p>Download</p> <p> uxopian-ai_docker_example.zip</p> <p>Once extracted, you should have the following directory structure:</p> <pre><code>.\n\u251c\u2500\u2500 config\n\u2502   \u251c\u2500\u2500 application.yml             # Main Spring configuration\n\u2502   \u251c\u2500\u2500 goals.yml                   # AI Goals definition\n\u2502   \u251c\u2500\u2500 llm-clients-config.yml      # API Keys and Model selection (OpenAI, Mistral, etc.)\n\u2502   \u251c\u2500\u2500 llm-clients-config.yml.example  # Example with all providers\n\u2502   \u251c\u2500\u2500 mcp-server.yml              # Model Context Protocol config\n\u2502   \u251c\u2500\u2500 metrics.yml                 # Micrometer &amp; Actuator config\n\u2502   \u251c\u2500\u2500 opensearch.yml              # Vector database connection\n\u2502   \u2514\u2500\u2500 prompts.yml                 # Pre-defined prompts\n\u251c\u2500\u2500 gateway-application.yaml        # Gateway config (if used)\n\u2514\u2500\u2500 uxopian-ai-stack.yml            # The docker-compose file\n</code></pre>"},{"location":"getting_started/installation_guide/#step-2-pull-images","title":"\ud83d\udd39 Step 2: Pull Images","text":"<p>Pull the required images from the registry configured in your <code>uxopian-ai-stack.yml</code>:</p> <pre><code>docker pull artifactory.arondor.cloud:5001/uxopian-ai:2026.0.0-ft1-rc3\n# Note: The OpenSearch image is public and will be pulled automatically by the compose file.\n</code></pre> <p>Registry</p> <p>The compose file uses <code>artifactory.arondor.cloud:5001/</code> by default. If your organization hosts images on a different registry (e.g., <code>docker.uxopian.com/preview/</code>), update the <code>image:</code> fields in <code>uxopian-ai-stack.yml</code> accordingly.</p>"},{"location":"getting_started/installation_guide/#step-3-environment-variable-configuration","title":"\ud83d\udd39 Step 3: Environment Variable Configuration","text":"<p>The <code>uxopian-ai-stack.yml</code> file orchestrates the containers. Do not modify the YAML structure, but you must adapt the environment variables of the <code>uxopian-ai-standalone</code> service to ensure network communication.</p> <p>There are two distinct communication flows to configure:</p>"},{"location":"getting_started/installation_guide/#1-integration-communication-ai-to-document-service","title":"1. Integration Communication (AI to Document Service)","text":"<p>The AI must contact the Document Service (e.g., ARender Service Broker) to read document text via the internal Docker network.</p> Variable Description Example (Internal Docker) <code>OPENSEARCH_HOST</code> OpenSearch container hostname. <code>uxopian-ai-opensearch-node1</code> <code>OPENSEARCH_PORT</code> OpenSearch port. <code>9200</code>"},{"location":"getting_started/installation_guide/#2-client-access-configuration-browser-to-ai","title":"2. Client Access Configuration (Browser to AI)","text":"<p>The user interface (running in the user's browser) must contact the AI service.</p> Variable Description Example (Public) <code>UXOPIAN_AI_PORT</code> Internal listening port of the service. <code>8080</code> <code>APP_BASE_URL</code> Public URL of the AI application (for callbacks). <code>http://localhost:8085</code> <code>SPRING_PROFILES_ACTIVE</code> Configuration profile (<code>dev</code> disables strict security). <code>dev</code> <p>About the <code>dev</code> Profile and the BFF Gateway</p> <p>The starter kit ships with <code>SPRING_PROFILES_ACTIVE=dev</code> and no Gateway service. This means you can call the AI service directly \u2014 missing <code>X-User-*</code> headers are filled in with defaults (<code>User-development</code> / <code>Tenant-development</code>).</p> <p>In a production deployment, you should remove the <code>dev</code> profile and deploy the Uxopian Gateway (BFF) in front of the AI service. The Gateway authenticates users, extracts their identity from JWT/OAuth2/LDAP tokens, and injects the <code>X-User-TenantId</code>, <code>X-User-Id</code>, <code>X-User-Roles</code>, and <code>X-User-Token</code> headers. The Gateway service is also included in this compose file (commented out) \u2014 uncomment the <code>uxopian-ai-gateway</code> block, configure its <code>provider</code>, and remove the <code>dev</code> profile to switch to a secured setup.</p>"},{"location":"getting_started/installation_guide/#step-4-start","title":"\ud83d\udd39 Step 4: Start","text":"<pre><code>docker-compose -f uxopian-ai-stack.yml up -d\n</code></pre>"},{"location":"getting_started/installation_guide/#scenario-b-manual-installation-zip-java","title":"\u2615 Scenario B: Manual Installation (ZIP / Java)","text":"<p>Use this method for deployment on a standard server (VM Linux/Windows) without Docker.</p> <p>Prerequisites:</p> <ul> <li>Java 21 Runtime Environment (JRE).</li> <li>OpenSearch 2.x installed and running on the network.</li> </ul>"},{"location":"getting_started/installation_guide/#step-1-installation","title":"\ud83d\udd39 Step 1: Installation","text":"<p>Download</p> <p> ai-standalone-2026.0.0-ft1-rc3-complete-package.zip</p> <p>Contact your Uxopian representative for access to this package.</p> <p>Unzip the archive:</p> <pre><code>unzip ai-standalone-2026.0.0-ft1-rc3-complete-package.zip\ncd ai-standalone\n</code></pre>"},{"location":"getting_started/installation_guide/#step-2-configuration","title":"\ud83d\udd39 Step 2: Configuration","text":"<p>All files are located in the <code>config/</code> folder. You must edit them:</p> <ul> <li><code>opensearch.yml</code>: Enter the IP and credentials of your external OpenSearch cluster.</li> <li><code>llm-clients-config.yml</code>: Configure your LLM providers (Azure OpenAI, Mistral, etc.) and API keys.</li> <li><code>application.yml</code>: General settings (ports, logs).</li> </ul> <p>Documentation is available at: Configuration</p>"},{"location":"getting_started/installation_guide/#step-3-execution","title":"\ud83d\udd39 Step 3: Execution","text":"<p>Run the Java service:</p> <pre><code>java -jar ai-standalone.jar\n</code></pre> <p>Production Recommendations</p> <ul> <li>Use <code>JAVA_OPTS</code> to allocate enough memory (e.g., <code>-Xmx4g</code>).</li> <li>Place the service behind a Reverse Proxy (NGINX/Apache) to handle SSL.</li> </ul>"},{"location":"getting_started/installation_guide/#step-4-create-your-first-prompt","title":"\ud83d\udd39 Step 4: Create Your First Prompt","text":"<p>Once the backend is running, you need to define what the AI should do by creating prompts via the API.</p> <p>See the Managing Prompts and Goals guide for detailed instructions and examples.</p>"},{"location":"getting_started/installation_guide_arender/","title":"\ud83d\udcda Complete Guide: Uxopian AI Deployment &amp; ARender Integration","text":"<p>This guide covers the entire process for deploying the Uxopian AI solution and integrating it into the ARender user interface.</p>"},{"location":"getting_started/installation_guide_arender/#part-1-backend-deployment-ai-service","title":"\ud83d\udce6 Part 1: Backend Deployment (AI Service)","text":"<p>This section covers the installation of the AI engine and its vector database. Two scenarios are possible: Docker (Recommended via Starter Kit) or Standalone (Native Java).</p>"},{"location":"getting_started/installation_guide_arender/#scenario-a-docker-deployment-starter-kit","title":"\ud83d\ude80 Scenario A: Docker Deployment (Starter Kit)","text":"<p>The Starter Kit provides a ready-to-use stack containing the AI service, an OpenSearch node, and a basic ARender stack for testing.</p>"},{"location":"getting_started/installation_guide_arender/#step-1-download-and-structure","title":"\ud83d\udd39 Step 1: Download and Structure","text":"<p>Download</p> <p> uxopian-ai_docker_example_arender.zip</p> <p>Once extracted, you should have the following directory structure:</p> <pre><code>.\n\u251c\u2500\u2500 arender/\n\u2502   \u2514\u2500\u2500 configurations/\n\u2502       \u251c\u2500\u2500 arender-custom-client.properties  # ARender AI host &amp; button config\n\u2502       \u251c\u2500\u2500 arender-plugins.xml               # Plugin loader\n\u2502       \u2514\u2500\u2500 toppanel-arender-ai-configuration.xml  # AI button definitions\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 application.yml                 # Main Spring configuration\n\u2502   \u251c\u2500\u2500 goals.yml                       # AI Goals definition\n\u2502   \u251c\u2500\u2500 llm-clients-config.yml          # API Keys and Model selection (OpenAI, Mistral, etc.)\n\u2502   \u251c\u2500\u2500 llm-clients-config.yml.example  # Example with all providers\n\u2502   \u251c\u2500\u2500 mcp-server.yml                  # Model Context Protocol config\n\u2502   \u251c\u2500\u2500 metrics.yml                     # Micrometer &amp; Actuator config\n\u2502   \u251c\u2500\u2500 opensearch.yml                  # Vector database connection\n\u2502   \u2514\u2500\u2500 prompts.yml                     # Pre-defined prompts\n\u251c\u2500\u2500 gateway-application.yaml            # Gateway config (if used)\n\u2514\u2500\u2500 uxopian-ai-stack.yml                # The docker-compose file\n</code></pre>"},{"location":"getting_started/installation_guide_arender/#step-2-pull-images","title":"\ud83d\udd39 Step 2: Pull Images","text":"<p>Pull the required images from the registry configured in your <code>uxopian-ai-stack.yml</code>:</p> <pre><code>docker pull artifactory.arondor.cloud:5001/uxopian-ai:2026.0.0-ft1-rc3-full\n# Note: The OpenSearch and ARender images will be pulled automatically by the compose file.\n</code></pre> <p>Registry</p> <p>The compose file uses <code>artifactory.arondor.cloud:5001/</code> by default. If your organization hosts images on a different registry (e.g., <code>docker.uxopian.com/preview/</code>), update the <code>image:</code> fields in <code>uxopian-ai-stack.yml</code> accordingly.</p>"},{"location":"getting_started/installation_guide_arender/#step-3-environment-variable-configuration","title":"\ud83d\udd39 Step 3: Environment Variable Configuration","text":"<p>The <code>uxopian-ai-stack.yml</code> file orchestrates the containers. Do not modify the YAML structure, but you must adapt the environment variables of the <code>uxopian-ai-standalone</code> service to ensure network communication.</p> <p>There are two distinct communication flows to configure:</p>"},{"location":"getting_started/installation_guide_arender/#1-server-to-server-communication-ai-backend-to-arender","title":"1. Server-to-Server Communication (AI Backend to ARender)","text":"<p>The AI must contact the ARender Service Broker to read document text. This happens via the internal Docker network.</p> Variable Description Example (Internal Docker) <code>RENDITION_BASE_URL</code> Internal URL of the ARender Service Broker. <code>http://dsb-service:8761</code> <code>OPENSEARCH_HOST</code> OpenSearch container hostname. <code>uxopian-ai-opensearch-node1</code> <code>OPENSEARCH_PORT</code> OpenSearch port. <code>9200</code>"},{"location":"getting_started/installation_guide_arender/#2-client-to-server-communication-browser-to-ai","title":"2. Client-to-Server Communication (Browser to AI)","text":"<p>The ARender Interface (running in the user's browser) must contact the AI.</p> Variable Description Example (Public) <code>UXOPIAN_AI_PORT</code> Internal listening port of the service. <code>8080</code> <code>APP_BASE_URL</code> Public URL of the AI application (for callbacks). <code>http://localhost:8085</code> <code>SPRING_PROFILES_ACTIVE</code> Configuration profile (<code>dev</code> disables strict security). <code>dev</code> <p>About the <code>dev</code> Profile and the BFF Gateway</p> <p>This starter kit ships with <code>SPRING_PROFILES_ACTIVE=dev</code> and no Gateway service. This lets you call the AI service directly \u2014 missing <code>X-User-*</code> headers are filled in with defaults. In a production deployment, deploy the Uxopian Gateway (BFF) in front of the AI service to handle authentication, role enforcement, and token propagation. The Gateway service is included in the compose file (commented out) \u2014 see the gateway block in <code>uxopian-ai-stack.yml</code>.</p> <p>Note on ARender UI</p> <p>In the ARender UI container configuration, do not forget to set <code>UXOPIAN_AI_HOST</code> to the public URL of the AI (e.g., <code>http://localhost:8085</code> or <code>https://ai.my-domain.com</code>).</p>"},{"location":"getting_started/installation_guide_arender/#step-4-start","title":"\ud83d\udd39 Step 4: Start","text":"<pre><code>docker-compose -f uxopian-ai-stack.yml up -d\n</code></pre>"},{"location":"getting_started/installation_guide_arender/#scenario-b-manual-installation-zip-java","title":"\u2615 Scenario B: Manual Installation (ZIP / Java)","text":"<p>Use this method for deployment on a standard server (VM Linux/Windows) without Docker.</p> <p>Prerequisites:</p> <ul> <li>Java 21 Runtime Environment (JRE).</li> <li>OpenSearch 2.x installed and running on the network.</li> </ul>"},{"location":"getting_started/installation_guide_arender/#step-1-installation","title":"\ud83d\udd39 Step 1: Installation","text":"<p>Download</p> <p> ai-standalone-2026.0.0-ft1-rc3-complete-package.zip</p> <p>Contact your Uxopian representative for access to this package.</p> <p>Unzip the archive:</p> <pre><code>unzip ai-standalone-2026.0.0-ft1-rc3-complete-package.zip\ncd ai-standalone\n</code></pre>"},{"location":"getting_started/installation_guide_arender/#step-2-configuration","title":"\ud83d\udd39 Step 2: Configuration","text":"<p>All files are located in the <code>config/</code> folder. You must edit them:</p> <ul> <li><code>opensearch.yml</code>: Enter the IP and credentials of your external OpenSearch cluster.</li> <li><code>llm-clients-config.yml</code>: Configure your LLM providers (Azure OpenAI, Mistral, etc.) and API keys.</li> <li><code>application.yml</code>: General settings (ports, logs).</li> </ul> <p>Documentation is available at: Configuration</p>"},{"location":"getting_started/installation_guide_arender/#step-3-execution","title":"\ud83d\udd39 Step 3: Execution","text":"<p>Run the Java service:</p> <pre><code>java -jar ai-standalone.jar\n</code></pre> <p>Production Recommendations</p> <ul> <li>Use <code>JAVA_OPTS</code> to allocate enough memory (e.g., <code>-Xmx4g</code>).</li> <li>Place the service behind a Reverse Proxy (NGINX/Apache) to handle SSL.</li> </ul>"},{"location":"getting_started/installation_guide_arender/#part-2-arender-configuration-frontend","title":"\ud83d\udda5\ufe0f Part 2: ARender Configuration (Frontend)","text":"<p>This section details how to modify the ARender configuration to display AI buttons and interact with the deployed backend.</p>"},{"location":"getting_started/installation_guide_arender/#configuration-file-structure","title":"\ud83d\udcc2 Configuration File Structure","text":"<p>Whether using Docker or manual installation, prepare the following files according to this structure:</p> <pre><code>.\n\u251c\u2500\u2500 configurations\n\u2502   \u251c\u2500\u2500 arender-custom-client.properties      # Activates scripts and configures AI host\n\u2502   \u251c\u2500\u2500 arender-plugins.xml                   # Imports Spring beans\n\u2502   \u2514\u2500\u2500 toppanel-arender-ai-configuration.xml # Defines the button and JS action\n\u2514\u2500\u2500 public\n    \u251c\u2500\u2500 web-components.css                    # Uxopian component styles\n    \u2514\u2500\u2500 web-components.js                     # Uxopian component JS logic\n</code></pre>"},{"location":"getting_started/installation_guide_arender/#step-1-prompt-creation-backend","title":"\ud83d\udd39 Step 1: Prompt Creation (Backend)","text":"<p>Before adding the button, the AI must know what to do. Create a prompt via the API.</p> <p>See the Managing Prompts and Goals guide for detailed instructions. For this ARender integration, create a prompt with ID <code>summarizeDocMd</code> that uses <code>[[${documentService.extractTextualContent(documentId)}]]</code> to extract document content.</p>"},{"location":"getting_started/installation_guide_arender/#step-2-property-configuration-arender-custom-clientproperties","title":"\ud83d\udd39 Step 2: Property Configuration (<code>arender-custom-client.properties</code>)","text":"<p>Edit <code>configurations/arender-custom-client.properties</code>. This file links the UI to the AI service.</p> <pre><code># 1. Load CSS (ARender Style + AI Style)\nstyle.sheet=css/arender-style.css,web-components.css\n\n# 2. Load Web Component script at startup\narenderjs.startupScript=web-components.js\n\n# 3. Add 'aiMenu' to the top toolbar (middle section)\ntopPanel.section.middle.buttons.beanNames=addStickyNoteAnnotationButton,annotationCreationOpenCreation,documentBuilderButton,aiMenu\n\n# 4. Configure Public AI URL\n# This is the address the user's browser will call\nuxopian.ai.host=http://localhost:8085\n# Production example: [https://ai.my-company.com](https://ai.my-company.com)\n\n# 5. (Optional) Disable visual logs (toasters)\ntoaster.log.info.enabled=false\n</code></pre>"},{"location":"getting_started/installation_guide_arender/#step-3-register-plugin-arender-pluginsxml","title":"\ud83d\udd39 Step 3: Register Plugin (<code>arender-plugins.xml</code>)","text":"<p>Edit <code>configurations/arender-plugins.xml</code> to import your button configuration file.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;beans default-lazy-init=\"true\" default-autowire=\"no\"\n    xmlns=\"[http://www.springframework.org/schema/beans](http://www.springframework.org/schema/beans)\"\n    xmlns:xsi=\"[http://www.w3.org/2001/XMLSchema-instance](http://www.w3.org/2001/XMLSchema-instance)\"\n    xsi:schemaLocation=\"[http://www.springframework.org/schema/beans](http://www.springframework.org/schema/beans)\n        [http://www.springframework.org/schema/beans/spring-beans.xsd](http://www.springframework.org/schema/beans/spring-beans.xsd)\"&gt;\n\n    &lt;import resource=\"plume.xml\"/&gt;\n    &lt;import resource=\"html-plugin.xml\"/&gt;\n\n    &lt;import resource=\"toppanel-arender-ai-configuration.xml\"/&gt;\n\n&lt;/beans&gt;\n</code></pre>"},{"location":"getting_started/installation_guide_arender/#step-4-button-definition-toppanel-arender-ai-configurationxml","title":"\ud83d\udd39 Step 4: Button Definition (<code>toppanel-arender-ai-configuration.xml</code>)","text":"<p>This XML file defines the dropdown menu and the button triggering the AI call. It contains injected JavaScript code (<code>$wnd.createChat</code>).</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;beans default-lazy-init=\"true\" default-autowire=\"no\"\n    xmlns=\"[http://www.springframework.org/schema/beans](http://www.springframework.org/schema/beans)\"\n    xmlns:xsi=\"[http://www.w3.org/2001/XMLSchema-instance](http://www.w3.org/2001/XMLSchema-instance)\"\n    xsi:schemaLocation=\"[http://www.springframework.org/schema/beans](http://www.springframework.org/schema/beans)\n        [http://www.springframework.org/schema/beans/spring-beans.xsd](http://www.springframework.org/schema/beans/spring-beans.xsd)\"&gt;\n\n    &lt;bean id=\"aiMenu\"\n       class=\"com.arondor.viewer.client.toppanel.presenter.SubMenuButtonPresenter\"&gt;\n       &lt;constructor-arg value=\"aiMenu\" /&gt;\n       &lt;constructor-arg value=\"AI\" /&gt;\n       &lt;constructor-arg value=\"standardButton fas fa-robot toppanelButton\"/&gt;\n       &lt;property name=\"enabled\" value=\"true\" /&gt;\n       &lt;property name=\"visibilityForTopPanel\"&gt;\n          &lt;ref bean=\"topPanelVisibilityMode\" /&gt;\n       &lt;/property&gt;\n       &lt;property name=\"orderedNamedList\" value=\"summarizeDocMdButton\" /&gt;\n    &lt;/bean&gt;\n\n    &lt;bean id=\"summarizeDocMdButton\"\n       class=\"com.arondor.viewer.client.toppanel.presenter.DropdownMenuItemPresenter\"&gt;\n       &lt;constructor-arg value=\"summarizeDocMdButton\"/&gt;\n       &lt;constructor-arg value=\"Summarize Document\"/&gt;\n       &lt;constructor-arg value=\"standardButton fas fa-list toppanelButton\"/&gt;\n       &lt;property name=\"enabled\" value=\"true\" /&gt;\n       &lt;property name=\"closingOnClick\" value=\"true\" /&gt;\n       &lt;property name=\"buttonHandler\"&gt;\n          &lt;bean class=\"com.arondor.viewer.client.jsapi.toppanel.JSCallButtonHandler\"&gt;\n             &lt;property name=\"jsCode\"&gt;\n                &lt;value&gt;\ntry {\n    // Call the global function exposed by web-components.js\n    $wnd.createChat({\n        endpoint: \"${uxopian.ai.host}\",   // Variable injected from .properties\n        wsEndpoint: \"${uxopian.ai.host}\", // Variable injected from .properties\n        request: {\n            inputs: [{\n                role: 'user',\n                content: [{\n                    type: 'PROMPT',\n                    value: 'summarizeDocMd', // Prompt ID defined in Step 1\n                    payload: {\n                        // Get current document ID via ARender JS API\n                        documentId: $wnd.getARenderJS().getCurrentDocumentId()\n                    }\n                }]\n            }]\n        }\n    });\n} catch(e) {\n  console.log('Error launching AI Chat: ' + e);\n}\n                &lt;/value&gt;\n             &lt;/property&gt;\n          &lt;/bean&gt;\n       &lt;/property&gt;\n    &lt;/bean&gt;\n&lt;/beans&gt;\n</code></pre>"},{"location":"getting_started/installation_guide_arender/#part-3-applying-changes-arender-deployment","title":"\ud83d\udee0\ufe0f Part 3: Applying Changes (ARender Deployment)","text":"<p>Once your configuration files are ready, apply them to your ARender instance.</p>"},{"location":"getting_started/installation_guide_arender/#option-a-docker-integration-custom-image-build","title":"\ud83c\udd70\ufe0f Option A: Docker Integration (Custom Image Build)","text":"<p>If using Docker for ARender, you must build a new image containing these configurations. Volume mounting alone can sometimes cause permission issues or file overwrites.</p> <p>1. Create Dockerfile At the root of your folder containing <code>configurations/</code> and <code>public/</code>:</p> <pre><code># Stage 1: File Preparation\nFROM alpine as builder\nWORKDIR /app\nCOPY configurations/ configurations/\nCOPY public/ public/\n\n# Stage 2: Final ARender Image\nFROM artifactory.arondor.cloud:5001/arender-ui-springboot:2023.16.0\n\n# Copy XML/Properties configurations\nCOPY --from=builder /app/configurations/* /home/arender/configurations/\n\n# Copy Web resources (JS/CSS)\nCOPY --from=builder /app/public/* /home/arender/public/\n</code></pre> <p>2. Build Image</p> <pre><code>docker build -t my-company/arender-ui-ai:custom .\n</code></pre> <p>3. Update docker-compose In your <code>docker-compose.yml</code>, replace the <code>ui</code> service image with <code>my-company/arender-ui-ai:custom</code>.</p>"},{"location":"getting_started/installation_guide_arender/#option-b-manual-installation-server","title":"\ud83c\udd71\ufe0f Option B: Manual Installation (Server)","text":"<p>For a standard installation (Tomcat or Executable Jar):</p> <ol> <li>Configurations: Copy the contents of your <code>configurations/</code> folder (the 3 files) to your ARender installation's config folder (<code>$ARENDER_HOME/configurations/</code>).</li> <li>Web Resources: Copy <code>web-components.js</code> and <code>web-components.css</code> to your installation's public folder (<code>$ARENDER_HOME/public/</code>).</li> <li>Restart: Restart the ARender service to load the new Spring Beans.</li> </ol>"},{"location":"getting_started/quickstart/","title":"Quick Start: Your First AI Exchange in 5 Minutes","text":"<p>This guide gets you from zero to a working AI response as fast as possible using the Docker Starter Kit.</p>"},{"location":"getting_started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed.</li> <li>An OpenAI API Key (or any supported LLM provider key).</li> </ul>"},{"location":"getting_started/quickstart/#step-1-download-the-starter-kit","title":"Step 1: Download the Starter Kit","text":"<p>Download</p> <p> uxopian-ai_docker_example.zip</p> <p>Extract the archive. You should have the following structure:</p> <pre><code>.\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 application.yml\n\u2502   \u251c\u2500\u2500 goals.yml\n\u2502   \u251c\u2500\u2500 llm-clients-config.yml\n\u2502   \u251c\u2500\u2500 llm-clients-config.yml.example\n\u2502   \u251c\u2500\u2500 mcp-server.yml\n\u2502   \u251c\u2500\u2500 metrics.yml\n\u2502   \u251c\u2500\u2500 opensearch.yml\n\u2502   \u2514\u2500\u2500 prompts.yml\n\u251c\u2500\u2500 gateway-application.yaml\n\u2514\u2500\u2500 uxopian-ai-stack.yml\n</code></pre>"},{"location":"getting_started/quickstart/#step-2-configure-your-api-key","title":"Step 2: Configure Your API Key","text":"<p>The LLM configuration uses environment variables. The simplest approach is to set your API key in the <code>uxopian-ai-stack.yml</code> file by adding it to the <code>environment</code> section of the <code>uxopian-ai-standalone</code> service:</p> <pre><code>environment:\n  - OPENAI_API_KEY=sk-YOUR_OPENAI_API_KEY_HERE\n</code></pre> <p>Alternatively, you can replace <code>${OPENAI_API_KEY:}</code> directly in <code>config/llm-clients-config.yml</code> with your key.</p> <p>Other Providers</p> <p>You can use any supported provider (Azure, Anthropic, Gemini, Mistral, Ollama...). See Configuration Files for the full reference.</p>"},{"location":"getting_started/quickstart/#step-3-start-the-stack","title":"Step 3: Start the Stack","text":"<p>Start all services:</p> <pre><code>docker-compose -f uxopian-ai-stack.yml up -d\n</code></pre> <p>Docker Images</p> <p>The compose file references images from <code>artifactory.arondor.cloud:5001/</code>. If your registry differs, update the <code>image:</code> fields in <code>uxopian-ai-stack.yml</code> accordingly.</p> <p>Wait a few seconds for OpenSearch to initialize. You can check the health:</p> <pre><code>curl http://localhost:8085/actuator/health\n</code></pre> <p>Port Mapping</p> <p>The starter kit maps the service to port 8085 on the host (<code>8085:8080</code>). All API calls from your machine go to <code>localhost:8085</code>.</p> <p>Why No Gateway?</p> <p>The starter kit runs in development mode (<code>SPRING_PROFILES_ACTIVE=dev</code>), which lets you call the AI service directly without a BFF Gateway. The service automatically fills in default values for missing authentication headers. In production, a BFF Gateway sits in front of the service and handles authentication, role injection, and tenant isolation.</p>"},{"location":"getting_started/quickstart/#step-4-create-a-prompt","title":"Step 4: Create a Prompt","text":"<p>Tell the AI what to do by creating your first prompt:</p> <pre><code>curl -X POST \"http://localhost:8085/api/v1/admin/prompts\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: quickstart\" \\\n  -H \"X-User-Id: admin\" \\\n  -H \"X-User-Roles: admin\" \\\n  -d '{\n    \"id\": \"helloAI\",\n    \"role\": \"system\",\n    \"content\": \"You are a helpful assistant. Answer concisely.\",\n    \"defaultLlmProvider\": \"openai\",\n    \"defaultLlmModel\": \"gpt-5.1\"\n  }'\n</code></pre> <p>Note</p> <p>The starter kit already includes pre-defined prompts in <code>config/prompts.yml</code> (e.g., <code>summarizeDocumentText</code>, <code>translate</code>). You can use them directly or create new ones via the API.</p>"},{"location":"getting_started/quickstart/#step-5-create-a-conversation-and-send-a-message","title":"Step 5: Create a Conversation and Send a Message","text":"<p>First, create a conversation:</p> <pre><code>curl -X POST \"http://localhost:8085/api/v1/conversations\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: quickstart\" \\\n  -H \"X-User-Id: demo-user\"\n</code></pre> <p>Copy the <code>id</code> from the response, then send your first message:</p> <pre><code>curl -X POST \"http://localhost:8085/api/v1/requests?conversation=YOUR_CONVERSATION_ID\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: quickstart\" \\\n  -H \"X-User-Id: demo-user\" \\\n  -d '{\n    \"inputs\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"value\": \"What is the capital of France?\"\n          }\n        ]\n      }\n    ]\n  }'\n</code></pre> <p>You should receive a JSON response with the AI's answer.</p>"},{"location":"getting_started/quickstart/#whats-next","title":"What's Next?","text":"<p>You've just completed your first AI exchange with uxopian-ai. Here's where to go from here:</p> <ul> <li>Understand the system: Read Core Concepts to learn about Prompts, Goals, and Conversations.</li> <li>Manage prompts: Learn how to manage prompts and goals with the API and the admin UI.</li> <li>Explore the Admin Panel: See your usage stats in the Dashboard.</li> <li>Go deeper: Discover the Templating Engine to build dynamic, context-aware prompts.</li> <li>Understand security: Learn how the BFF Gateway handles authentication in production and why this starter kit can skip it.</li> </ul>"},{"location":"how_to/api_usage/","title":"Using the REST API","text":"<p>This guide provides practical examples for the most common tasks in uxopian-ai: managing conversations, sending requests, orchestrating goals, and accessing admin endpoints.</p> <p>For details on the security model and authentication headers, see Security Model (BFF Pattern).</p>"},{"location":"how_to/api_usage/#prerequisites-authentication-headers","title":"Prerequisites: Authentication Headers","text":"<p>Uxopian-ai never handles authentication itself \u2014 it relies on a BFF Gateway to validate credentials and inject identity headers into every request. See Security Model (BFF Pattern) for the full architecture.</p> Header Description Required <code>X-User-TenantId</code> Isolates data per tenant (organization). Yes <code>X-User-Id</code> Unique identifier for the user. Yes <code>X-User-Roles</code> Comma-separated list of roles (e.g., <code>admin,user</code>). No <code>X-User-Token</code> Original user token, forwarded to integrations (FlowerDocs, ARender). No <p>The Gateway authenticates the user (OAuth2, JWT, LDAP...), extracts identity from the session, and enriches the request with these <code>X-User-*</code> headers before forwarding it to uxopian-ai. The AI service trusts these headers implicitly \u2014 which is why uxopian-ai must never be exposed directly to the network.</p> <p>Development Mode</p> <p>In development mode (<code>SPRING_PROFILES_ACTIVE=dev</code>), the AI service accepts requests without headers and fills in defaults (<code>User-development</code> / <code>Tenant-development</code>). The examples below include explicit headers so they work in both dev and production environments. See Development Mode for details.</p>"},{"location":"how_to/api_usage/#1-creating-a-conversation","title":"1. Creating a Conversation","text":"<p>Conversations are the container for all history and context. They are scoped to the specific <code>X-User-TenantId</code> provided in the headers.</p> <ul> <li>Endpoint: <code>POST /api/v1/conversations</code></li> </ul>"},{"location":"how_to/api_usage/#example-gateway-forwarding-a-creation-request","title":"Example: Gateway forwarding a creation request","text":"<p>cURL</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/conversations\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Id: user-james-bond\" \\\n  -H \"X-User-Roles: user\"\n</code></pre> <p>JavaScript (Node.js / Gateway Logic)</p> <pre><code>const createConversation = async (tenantId, userId) =&gt; {\n  const response = await fetch(\n    \"http://uxopian-ai-service:8080/api/v1/conversations\",\n    {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n        \"X-User-TenantId\": tenantId,\n        \"X-User-Id\": userId,\n      },\n    }\n  );\n\n  const data = await response.json();\n  console.log(\"New Conversation ID:\", data.id);\n  return data;\n};\n</code></pre>"},{"location":"how_to/api_usage/#2-sending-a-text-request-non-streaming","title":"2. Sending a Text Request (Non-Streaming)","text":"<p>Send a prompt to the LLM within a conversation. The <code>X-User-TenantId</code> ensures the user only accesses conversations they belong to.</p> <ul> <li>Endpoint: <code>POST /api/v1/requests</code></li> <li>Query Parameter: <code>conversation</code> (Required)</li> </ul>"},{"location":"how_to/api_usage/#example-standard-user-query","title":"Example: Standard User Query","text":"<p>cURL</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/requests?conversation=123e4567-e89b-12d3-a456-426614174000\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Id: user-james-bond\" \\\n  -d '{\n    \"inputs\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"value\": \"How do I reset my password?\"\n          }\n        ]\n      }\n    ]\n  }'\n</code></pre>"},{"location":"how_to/api_usage/#3-triggering-a-goal-orchestration","title":"3. Triggering a Goal (Orchestration)","text":"<p>Use the <code>goal</code> type to let uxopian-ai select the correct prompt based on the payload context.</p> <ul> <li>Content Type: <code>goal</code></li> <li>Payload: Context data for the filter engine.</li> </ul>"},{"location":"how_to/api_usage/#example-intelligent-goal-routing","title":"Example: Intelligent Goal Routing","text":"<p>cURL</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/requests?conversation=123e4567-e89b-12d3-a456-426614174000\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Id: user-james-bond\" \\\n  -d '{\n    \"inputs\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"goal\",\n            \"value\": \"analyze_document\",\n            \"payload\": {\n              \"docType\": \"financial_report\",\n              \"quarter\": \"Q3\"\n            }\n          }\n        ]\n      }\n    ]\n  }'\n</code></pre>"},{"location":"how_to/api_usage/#4-sending-a-streaming-request","title":"4. Sending a Streaming Request","text":"<p>For real-time responses (typing effect), use the streaming endpoint.</p> <ul> <li>Endpoint: <code>POST /api/v1/requests/stream</code></li> <li>Response Content-Type: <code>text/event-stream</code></li> </ul>"},{"location":"how_to/api_usage/#example-streaming-response","title":"Example: Streaming Response","text":"<p>cURL</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/requests/stream?conversation=123e4567-e89b-12d3-a456-426614174000\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Id: user-james-bond\" \\\n  --no-buffer \\\n  -d '{\n    \"inputs\": [\n      {\n        \"role\": \"user\",\n        \"content\": [{ \"type\": \"text\", \"value\": \"Explain quantum physics.\" }]\n      }\n    ]\n  }'\n</code></pre>"},{"location":"how_to/api_usage/#5-administrative-operations","title":"5. Administrative Operations","text":"<p>To access Admin endpoints, the Gateway must inject the <code>admin</code> role in the <code>X-User-Roles</code> header.</p>"},{"location":"how_to/api_usage/#example-fetching-global-statistics","title":"Example: Fetching Global Statistics","text":"<p>cURL</p> <pre><code>curl -X GET \"http://localhost:8080/api/v1/admin/stats/global\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Id: admin-user\" \\\n  -H \"X-User-Roles: admin\"\n</code></pre>"},{"location":"how_to/api_usage/#example-listing-llm-provider-configurations","title":"Example: Listing LLM Provider Configurations","text":"<p>cURL</p> <pre><code>curl -X GET \"http://localhost:8080/api/v1/admin/llm/provider-conf\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Id: admin-user\" \\\n  -H \"X-User-Roles: admin\"\n</code></pre>"},{"location":"how_to/api_usage/#example-creating-an-llm-provider-configuration","title":"Example: Creating an LLM Provider Configuration","text":"<p>cURL</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/admin/llm/provider-conf\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-User-TenantId: enterprise-corp-a\" \\\n  -H \"X-User-Id: admin-user\" \\\n  -H \"X-User-Roles: admin\" \\\n  -d '{\n    \"provider\": \"openai\",\n    \"defaultLlmModelConfName\": \"gpt5\",\n    \"globalConf\": {\n      \"apiSecret\": \"sk-your-api-key\",\n      \"temperature\": 0.7,\n      \"maxRetries\": 3,\n      \"timeout\": \"60s\"\n    },\n    \"llModelConfs\": [\n      {\n        \"llmModelConfName\": \"gpt5\",\n        \"modelName\": \"gpt-5.1\",\n        \"multiModalSupported\": true,\n        \"functionCallSupported\": true\n      },\n      {\n        \"llmModelConfName\": \"gpt5-mini\",\n        \"modelName\": \"gpt-5-mini\",\n        \"temperature\": 0.3,\n        \"multiModalSupported\": true,\n        \"functionCallSupported\": true\n      }\n    ]\n  }'\n</code></pre> <p>For the complete list of LLM provider management endpoints, see REST API Reference \u2014 LLM Providers.</p>"},{"location":"how_to/backup_recovery/","title":"\ud83d\udee1\ufe0f Backup and Recovery","text":"<p>This section provides guidance on how to protect the critical data managed by the <code>uxopian-ai</code> framework. A proper backup strategy is essential to prevent data loss and ensure business continuity.</p>"},{"location":"how_to/backup_recovery/#what-to-back-up","title":"\ud83d\udcbe What to Back Up","text":"<p>There are two primary sources of data you need to protect:</p> <ul> <li>\ud83d\udee0 Configuration Files: All the <code>.yml</code> files that define the service's behavior, including connections, provider settings, and default parameters.</li> <li>\ud83d\udce6 OpenSearch Data: The OpenSearch instance stores all the dynamic data generated by user interactions, which includes conversations, messages, and the centrally managed Prompts and Goals.</li> </ul>"},{"location":"how_to/backup_recovery/#built-in-yaml-backup-for-prompts-and-goals","title":"\ud83e\udde0 Built-in YAML Backup for Prompts and Goals","text":"<p>The framework includes an automatic backup mechanism for Prompts and Goals. Whenever you create, update, or delete a prompt or goal via the API, the service automatically writes the current state to YAML files in the configured backup directory.</p> <p>Note: This mechanism is multi-tenant aware, meaning it generates separate files for each tenant to ensure data isolation.</p>"},{"location":"how_to/backup_recovery/#configuration","title":"\u2699\ufe0f Configuration","text":"<p>You only need to configure the directory path where backups should be stored.</p> <p>For Prompts (<code>prompts.yml</code>):</p> <pre><code>prompts:\n  backup:\n    path: ${PROMPTS_BACKUP_PATH:./prompts/}\n</code></pre> <p>For Goals (<code>goals.yml</code>):</p> <pre><code>goals:\n  backup:\n    path: ${GOALS_BACKUP_PATH:./goals/}\n</code></pre>"},{"location":"how_to/backup_recovery/#file-naming-convention","title":"\ud83d\udcc2 File Naming Convention","text":"<p>The system automatically names backup files based on the <code>tenantId</code>.</p> <p>Format:</p> <ul> <li>Prompts: <code>prompts-{tenantId}.yaml</code></li> <li>Goals: <code>goals-{tenantId}.yaml</code></li> </ul> <p>Content Structure: A backup file contains the raw definition of a tenant's configuration.</p> <pre><code>tenantId: Tenant-development\nprompts:\n  - id: basePrompt\n    role: SYSTEM\n    content: \"...\"\n    # ...\n</code></pre>"},{"location":"how_to/backup_recovery/#restoring-from-yaml-backups","title":"\ud83d\udd04 Restoring from YAML Backups","text":"<p>Since backup files contain specific tenant data, restoration involves integrating this data back into your main configuration files or pushing it via the API.</p>"},{"location":"how_to/backup_recovery/#method-1-restoration-via-configuration-bootstrap","title":"\u2705 Method 1: Restoration via Configuration (Bootstrap)","text":"<p>This method allows you to restore data by injecting the backup content directly into the main configuration files (<code>prompts.yml</code> or <code>goals.yml</code>) before starting the service.</p> <p>Steps:</p> <ol> <li>Locate Backup: Open the specific backup file you wish to restore (e.g., <code>prompts-Tenant-development.yaml</code>).</li> <li>Copy Content: Copy the entire content of the file (including <code>tenantId</code> and the list of prompts/goals).</li> <li>Edit Main Config: Open your main configuration file (<code>config/prompts.yml</code> or <code>config/goals.yml</code>).</li> <li>Inject and Configure:<ul> <li>Locate the <code>tenants: []</code> list.</li> <li>Paste the backup content as a new item in this list.</li> <li>Crucial: Add the <code>mergeStrategy</code> field to define how this data should be applied.</li> </ul> </li> </ol> <p>Example for Prompts (<code>config/prompts.yml</code>):</p> <pre><code>prompts:\n  globals: [...]\n\n  # Paste your backup here under 'tenants'\n  tenants:\n    - tenantId: Tenant-development # &lt;--- From Backup\n      mergeStrategy: OVERRIDE # &lt;--- ADD THIS MANUALLY\n      prompts: # &lt;--- From Backup\n        - id: basePrompt\n          role: SYSTEM\n          content: \"...\"\n</code></pre> <ol> <li>Restart: Restart <code>uxopian-ai</code>. The service will process the tenants list and apply the configuration to OpenSearch according to the <code>mergeStrategy</code>.</li> </ol>"},{"location":"how_to/backup_recovery/#method-2-manual-restore-via-api","title":"\ud83e\uddea Method 2: Manual Restore via API","text":"<p>You can also manually restore a specific tenant's configuration using the REST API.</p> <ol> <li>Open the backup file (e.g., <code>goals-Tenant-development.yaml</code>).<ul> <li>Goals: Copy each entry from the <code>goalGroups</code> list.</li> <li>Prompts: Copy each entry from the <code>prompts</code> list.</li> </ul> </li> <li>Send a <code>POST</code> request for each item to the appropriate endpoint.<ul> <li>Prompts: <code>POST /api/v1/admin/prompts</code></li> <li>Goals: <code>POST /api/v1/admin/goals</code></li> </ul> </li> <li>Headers: You must provide the correct authentication headers to ensure the data is restored to the correct tenant.<ul> <li><code>X-User-TenantId: Tenant-development</code></li> </ul> </li> </ol>"},{"location":"how_to/backup_recovery/#merge-strategies-configuration-mode","title":"\ud83e\udde9 Merge Strategies (Configuration Mode)","text":"<p>When using Method 1, the <code>mergeStrategy</code> field controls how the restored data interacts with existing data in OpenSearch.</p>"},{"location":"how_to/backup_recovery/#override-disaster-recovery","title":"\ud83d\udd01 OVERRIDE (Disaster Recovery)","text":"<ul> <li>Behavior: Deletes ALL existing prompts/goals for this specific tenant in OpenSearch and replaces them with the content of the YAML.</li> <li>Use Case: Restoring a corrupted environment or reverting to a known clean state.</li> </ul>"},{"location":"how_to/backup_recovery/#merge-updatepatch","title":"\u2795 MERGE (Update/Patch)","text":"<ul> <li>Behavior:</li> <li>Updates items with matching IDs.</li> <li>Creates items that do not exist.</li> <li>Does not delete items that are in OpenSearch but not in the YAML.</li> <li>Use Case: Applying configuration updates without losing data created by users since the backup.</li> </ul>"},{"location":"how_to/backup_recovery/#create_if_missing-safe-init","title":"\ud83c\udd95 CREATE_IF_MISSING (Safe Init)","text":"<ul> <li>Behavior: Only creates items that do not currently exist. Existing items are ignored.</li> <li>Use Case: Deploying default prompts to a new environment without risking overwrite of existing data.</li> </ul>"},{"location":"how_to/backup_recovery/#backup-strategy-summary","title":"\ud83d\uddc2 Backup Strategy Summary","text":"Entity Storage Backup Mechanism Recommended Restore Method Configuration Filesystem Copy files manually Restore files to <code>./config/</code> Conversations OpenSearch OpenSearch Snapshots Elastic/OpenSearch Snapshot Restore Prompts/Goals OpenSearch Auto YAML Backup Method 1 (Config Merge) for full restore"},{"location":"how_to/backup_recovery/#key-recommendations","title":"\u2705 Key Recommendations","text":"<ul> <li>\ud83d\udcc6 Automate Commits: Since the system automatically updates the YAML files on disk, set up a cron job to commit and push these changes to a Git repository automatically.</li> <li>\ud83d\udcdd Verify Merge Strategy: When restoring via config, always double-check the <code>mergeStrategy</code>. Using <code>OVERRIDE</code> by mistake will wipe out any new prompts created since the backup.</li> </ul>"},{"location":"how_to/integrate_arender/","title":"How-To: Integrate AI Features in ARender","text":"<p>This guide details the process of adding context-aware AI buttons directly into the ARender High Content Interface (HCI). We will configure the necessary files to load the Uxopian-ai web components and add AI action buttons to the top panel.</p>"},{"location":"how_to/integrate_arender/#prerequisites","title":"Prerequisites","text":"<ul> <li>ARender HMI: Installed and accessible.</li> <li>Uxopian-ai: Deployed and running.</li> <li>Resources: The <code>web-components.js</code> and <code>web-components.css</code> files are available.</li> </ul>"},{"location":"how_to/integrate_arender/#file-structure-reference","title":"File Structure Reference","text":"<p>The ARender configuration files should follow this structure:</p> <pre><code>\u251c\u2500\u2500 configurations/\n\u2502   \u251c\u2500\u2500 arender-custom-client.properties  # Main configuration entry point\n\u2502   \u251c\u2500\u2500 arender-plugins.xml               # Plugin loader\n\u2502   \u2514\u2500\u2500 toppanel-arender-ai-configuration.xml # Button definitions (Beans)\n\u2514\u2500\u2500 public/\n    \u251c\u2500\u2500 web-components.css\n    \u2514\u2500\u2500 web-components.js\n</code></pre> <p>Starter Kit</p> <p>The Docker + ARender starter kit already includes these configuration files under <code>arender/configurations/</code>. You can use them as a starting point.</p>"},{"location":"how_to/integrate_arender/#step-1-create-the-prompts","title":"Step 1: Create the Prompts","text":"<p>The starter kit's <code>config/prompts.yml</code> already includes pre-defined prompts (e.g., <code>summarizeDocumentText</code>, <code>summarizeDocumentMarkdown</code>, <code>translate</code>, <code>detailedComparison</code>). If you need a custom prompt, create it via the API.</p> <p>Endpoint: <code>POST /api/v1/admin/prompts</code></p> <p>Request Body (example):</p> <pre><code>{\n  \"id\": \"summarizeDocumentText\",\n  \"role\": \"user\",\n  \"content\": \"Summarize the following document in a plain text format:\\n\\n[[${documentService.extractTextualContent(documentId)}]]\",\n  \"defaultLlmProvider\": \"openai\",\n  \"defaultLlmModel\": \"gpt-5.1\",\n  \"temperature\": \"0.7\",\n  \"timeSaved\": 300,\n  \"requiresMultiModalModel\": false,\n  \"requiresFunctionCallingModel\": false\n}\n</code></pre> <p>Context Injection</p> <p>The expression <code>[[${documentService.extractTextualContent(documentId)}]]</code> indicates that the text extraction happens on the server side (Uxopian backend) using the <code>documentId</code> passed in the payload.</p>"},{"location":"how_to/integrate_arender/#step-2-configure-arender-properties","title":"Step 2: Configure ARender Properties","text":"<p>Update <code>configurations/arender-custom-client.properties</code> to load the web components, define the menu button, and point to the AI host.</p> <pre><code># 1. Load Styles\nstyle.sheet=css/arender-style.css,web-components.css\n\n# 2. Load the Web Component Script\narenderjs.startupScript=web-components.js\n\n# 3. Add the 'aiMenu' to the top panel (middle section)\ntopPanel.section.middle.buttons.beanNames=addStickyNoteAnnotationButton,annotationCreationOpenCreation,documentBuilderButton,aiMenu\n\n# 4. Configure the connection to Uxopian-ai\n# Uses UXOPIAN_AI_HOST env var with a fallback default\nuxopian.ai.host=${UXOPIAN_AI_HOST:http://localhost:8085}\n\n# 5. Optional: Disable info toaster if preferred\ntoaster.log.info.enabled=false\n</code></pre> <p>UXOPIAN_AI_HOST</p> <p>This must be the public URL reachable from the user's browser, not the internal Docker network address. Set it via the <code>UXOPIAN_AI_HOST</code> environment variable in your ARender UI container (see the Docker + ARender installation guide).</p>"},{"location":"how_to/integrate_arender/#step-3-register-the-plugin","title":"Step 3: Register the Plugin","text":"<p>Ensure ARender loads your custom bean configuration by importing it in <code>configurations/arender-plugins.xml</code>.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;beans default-lazy-init=\"true\" default-autowire=\"no\"\n    xmlns=\"http://www.springframework.org/schema/beans\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://www.springframework.org/schema/beans\n        http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt;\n\n    &lt;import resource=\"plume.xml\"/&gt;\n    &lt;import resource=\"html-plugin.xml\"/&gt;\n    &lt;import resource=\"toppanel-arender-ai-configuration.xml\"/&gt;\n\n&lt;/beans&gt;\n</code></pre>"},{"location":"how_to/integrate_arender/#step-4-define-the-ai-buttons","title":"Step 4: Define the AI Buttons","text":"<p>Define the menu and buttons in <code>configurations/toppanel-arender-ai-configuration.xml</code>. The JavaScript handler invokes the <code>createChat</code> function exposed by the web component.</p> <p>Below is a simplified example with a single \"Summarize\" button. The starter kit includes a more complete version with summarize, compare, translate, and open chat buttons.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;beans default-lazy-init=\"true\" default-autowire=\"no\"\n    xmlns=\"http://www.springframework.org/schema/beans\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://www.springframework.org/schema/beans\n        http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt;\n\n    &lt;bean id=\"aiMenu\"\n       class=\"com.arondor.viewer.client.toppanel.presenter.SubMenuButtonPresenter\"&gt;\n       &lt;constructor-arg value=\"aiMenu\" /&gt;\n       &lt;constructor-arg value=\"AI\" /&gt;\n       &lt;constructor-arg value=\"standardButton fas fa-robot toppanelButton\"/&gt;\n       &lt;property name=\"enabled\" value=\"true\" /&gt;\n       &lt;property name=\"visibilityForTopPanel\"&gt;\n          &lt;ref bean=\"topPanelVisibilityMode\" /&gt;\n       &lt;/property&gt;\n       &lt;property name=\"orderedNamedList\" value=\"summarizeTxtButton\" /&gt;\n    &lt;/bean&gt;\n\n    &lt;bean id=\"summarizeTxtButton\"\n       class=\"com.arondor.viewer.client.toppanel.presenter.DropdownMenuItemPresenter\"&gt;\n       &lt;constructor-arg value=\"summarizeTxtButton\"/&gt;\n       &lt;constructor-arg value=\"Summarize in text format\"/&gt;\n       &lt;constructor-arg value=\"standardButton fas fa-list toppanelButton\"/&gt;\n       &lt;property name=\"enabled\" value=\"true\" /&gt;\n       &lt;property name=\"closingOnClick\" value=\"true\" /&gt;\n       &lt;property name=\"buttonHandler\"&gt;\n          &lt;bean class=\"com.arondor.viewer.client.jsapi.toppanel.JSCallButtonHandler\"&gt;\n             &lt;property name=\"jsCode\"&gt;\n                &lt;value&gt;\ntry {\n    $wnd.createChat({\n        endpoint: \"${uxopian.ai.host}\",\n        wsEndpoint: \"${uxopian.ai.host}\",\n        request: {\n            inputs: [{\n                role: 'user',\n                content: [{\n                    type: 'PROMPT',\n                    value: 'summarizeDocumentText',\n                    payload: {\n                        documentId: $wnd.getARenderJS().getCurrentDocumentId()\n                    }\n                }]\n            }]\n        }\n    });\n} catch(e) {\n  console.log('Error : ' + e);\n}\n                &lt;/value&gt;\n             &lt;/property&gt;\n          &lt;/bean&gt;\n       &lt;/property&gt;\n    &lt;/bean&gt;\n&lt;/beans&gt;\n</code></pre> <p>The <code>value</code> field in the JavaScript must match a prompt ID defined in the backend (here, <code>summarizeDocumentText</code> from <code>config/prompts.yml</code>).</p>"},{"location":"how_to/integrate_arender/#step-5-build-and-deploy","title":"Step 5: Build and Deploy","text":"<p>To deploy these changes, you must build a custom Docker image that extends the official ARender base image. This ensures your configuration files and web resources are correctly placed in the container's runtime environment.</p> <p>Starter Kit Alternative</p> <p>The starter kit's <code>uxopian-ai-stack.yml</code> already mounts <code>arender/configurations/</code> as a volume into the ARender UI container, so you can iterate without rebuilding the image during development.</p>"},{"location":"how_to/integrate_arender/#dockerfile-configuration","title":"Dockerfile Configuration","text":"<p>For production, create a <code>Dockerfile</code> at the root of your project:</p> <pre><code># Stage 1: Preparation\nFROM alpine as builder\nWORKDIR /app\nCOPY configurations/ configurations/\nCOPY public/ public/\n\n# Stage 2: Final Image\nFROM artifactory.arondor.cloud:5001/arender-ui-springboot:2023.16.0\n\n# Install configurations\nCOPY --from=builder /app/configurations/* /home/arender/configurations/\n\n# Install Web Resources\nCOPY --from=builder /app/public/* /home/arender/public/\n</code></pre>"},{"location":"how_to/integrate_arender/#build-command","title":"Build Command","text":"<pre><code>docker build -t my-custom-arender:1.0 .\n</code></pre>"},{"location":"how_to/integrate_flowerdocs/","title":"How-To: Integrate AI Features in FlowerDocs","text":"<p>This guide details the process of adding a context-aware AI feature\u2014specifically a Document Summarizer\u2014directly into the FlowerDocs user interface.</p>"},{"location":"how_to/integrate_flowerdocs/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure the following environment configurations are in place:</p> <ul> <li>Uxopian-ai Platform: The core platform is deployed and running.</li> <li>Web Component Resources: The <code>uxopian-ai</code> web component JavaScript and CSS files are deployed and accessible to FlowerDocs.</li> <li>BFF Configuration: The Backend-for-Frontend (BFF) is configured to proxy requests correctly.</li> <li>Plugin Setup: The necessary plugin configurations (security, routing) have been established by the environment administrator.</li> </ul>"},{"location":"how_to/integrate_flowerdocs/#step-1-create-the-prompt","title":"Step 1: Create the Prompt","text":"<p>First, we must define the behavior of the AI by creating a specific prompt in the Uxopian-ai backend. This prompt will be called by the frontend script.</p> <p>We will create a prompt with the ID <code>summarizeDocumentMarkdown</code>.</p> <p>Endpoint: <code>POST /api/v1/admin/prompts</code></p> <p>Request Body:</p> <pre><code>{\n  \"id\": \"summarizeDocMd\",\n  \"role\": \"user\",\n  \"content\": \"Summarize the following document in a plain text format: \\n [[${documentService.extractTextualContent(documentId)}]]\",\n  \"defaultLlmProvider\": \"openai\",\n  \"defaultLlmModel\": \"gpt-5.1\",\n  \"temperature\": \"0.7\",\n  \"timeSaved\": 300,\n  \"requiresMultiModalModel\": false,\n  \"requiresFunctionCallingModel\": false\n}\n</code></pre> <p>Variables</p> <p>Notice the usage of <code>[[${documentId}]]</code> in the content. This variable is resolved server-side by the Thymeleaf engine using the <code>documentId</code> value passed in the request payload (see Step 2).</p>"},{"location":"how_to/integrate_flowerdocs/#step-2-configure-the-flowerdocs-script","title":"Step 2: Configure the FlowerDocs Script","text":"<p>Next, navigate to the FlowerDocs Administration Console and access the Scripts section. Create a new JavaScript file (e.g., <code>Summarize Document</code>) and paste the following code.</p> <p>This script uses an IIFE (Immediately Invoked Function Expression) to encapsulate logic and prevent global variable conflicts.</p> <pre><code>(function () {\n  /**\n   * ============================================================\n   * 1. CONFIGURATION &amp; CONSTANTS\n   * Constants are scoped to this function to prevent global conflicts.\n   * ============================================================\n   */\n  const BASE_URL = window.location.origin;\n  const ENDPOINTS = {\n    CHAT: `${BASE_URL}/gui/gateway/uxopian-ai`,\n    WS: `${BASE_URL}/gui/gateway/uxopian-ai`,\n    // Dynamically retrieve the gateway URL based on the current user scope\n    getGATEWAY: () =&gt;\n      `${BASE_URL}/gui/plugins/${JSAPI.get()\n        .getUserAPI()\n        .getScope()}/gateway/uxopian-ai`,\n  };\n\n  /**\n   * ============================================================\n   * 2. HELPER FUNCTIONS\n   * ============================================================\n   */\n\n  /**\n   * Generates the technical context (JSON) required by the AI.\n   * This injects the current FlowerDocs Component ID and ARender Document ID.\n   */\n  function getComponentContext() {\n    const flowerId = JSAPI.get()\n      .getLastComponentFormAPI()\n      .getComponent()\n      .getId();\n    const arenderId = arenderJSAPI.getCurrentDocumentId();\n\n    return InputBuilder.textAsSystem(`\n      /* Contextual Data */\n      {\n        flowerdocId: { \n          value: \"${flowerId}\", \n          description: \"The ID of the Flowerdoc document\" \n        },\n        arenderDocId: { \n          value: \"${arenderId}\", \n          description: \"The ID of the Arender document (BASE64 encoded)\" \n        }\n      }\n    `);\n  }\n\n  /**\n   * Initiates the AI Chat interface using the Web Component's exposed function.\n   */\n  function openChatWindow(requestPayload) {\n    fetch(ENDPOINTS.getGATEWAY())\n      .then(() =&gt;\n        createChat({\n          endpoint: ENDPOINTS.CHAT,\n          wsEndpoint: ENDPOINTS.WS,\n          request: requestPayload,\n        })\n      )\n      .catch((error) =&gt; console.error(\"Failed to open chat:\", error));\n  }\n\n  /**\n   * Registers a custom button in the application header.\n   */\n  function registerHeaderAction({ label, icon, onExecute }) {\n    const jsapi = JSAPI.get();\n\n    // Register the label for translation\n    jsapi.getLabelsAPI().setLabels([label]);\n\n    jsapi.registerForComponentChange((api) =&gt; {\n      const resolvedLabel = jsapi.getLabelsAPI().getLabel(label.name);\n      const headerActions = api.getActions().getHeaderActions();\n\n      const actionItem = jsapi\n        .getActionFactoryAPI()\n        .buildMenu(\n          `ai-action-${label.name}`,\n          resolvedLabel,\n          icon || \"fa-solid fa-robot\",\n          onExecute\n        );\n\n      headerActions.add(actionItem);\n    });\n  }\n\n  /**\n   * ============================================================\n   * 3. MAIN EXECUTION\n   * ============================================================\n   */\n  registerHeaderAction({\n    icon: \"fa fa-file-alt\",\n    label: {\n      name: \"summarizeDocMd\",\n      FR: \"R\u00e9sumer le document en Markdown\",\n      EN: \"Summarize the document in Markdown\",\n    },\n    // The onExecute function builds the request at the moment of the click\n    onExecute: () =&gt; {\n      const request = {\n        inputs: [\n          // 1. Inject System Context (IDs)\n          getComponentContext(),\n          // 2. Inject User Prompt with dynamic Arender ID\n          InputBuilder.promptAsUser(\"summarizeDocMd\", {\n            documentId: arenderJSAPI.getCurrentDocumentId(),\n          }),\n        ],\n      };\n      openChatWindow(request);\n    },\n  });\n})();\n</code></pre>"},{"location":"how_to/integrate_flowerdocs/#code-breakdown","title":"Code Breakdown","text":""},{"location":"how_to/integrate_flowerdocs/#context-injection-getcomponentcontext","title":"Context Injection (<code>getComponentContext</code>)","text":"<p>The script automatically retrieves the current context from the FlowerDocs UI (<code>JSAPI</code>) and the viewer (<code>arenderJSAPI</code>). It formats this data as a System Message, ensuring the LLM understands which specific document it is analyzing.</p>"},{"location":"how_to/integrate_flowerdocs/#dynamic-execution-onexecute","title":"Dynamic Execution (<code>onExecute</code>)","text":"<p>The request body is constructed inside the <code>onExecute</code> callback. This ensures that the <code>documentId</code> and context are fetched at the moment the button is clicked, rather than when the page loads. This is crucial for Single Page Applications (SPAs) where the user might switch documents without reloading the page.</p>"},{"location":"how_to/integrate_flowerdocs/#the-chat-trigger-createchat","title":"The Chat Trigger (<code>createChat</code>)","text":"<p>The <code>createChat</code> function is exposed globally by the <code>uxopian-ai</code> web component. It accepts the connection endpoints and the initial <code>request</code> payload, seamlessly opening the chat modal over the FlowerDocs interface.</p>"},{"location":"how_to/integrate_web_page/","title":"How-To: Integrate AI into a Basic Web Page","text":"<p>This guide explains how to add Uxopian-ai features to a standard HTML/JavaScript application. We will create a simple button that, when clicked, opens the AI chat modal with specific context from the page.</p>"},{"location":"how_to/integrate_web_page/#prerequisites","title":"Prerequisites","text":"<ul> <li>Uxopian-ai Platform: Deployed and accessible.</li> <li>Web Components: The <code>web-components.js</code> and <code>web-components.css</code> files must be imported into your HTML file.</li> </ul>"},{"location":"how_to/integrate_web_page/#step-1-create-the-prompt","title":"Step 1: Create the Prompt","text":"<p>First, configure the prompt in the backend. This prompt will define how the AI behaves and what variables it expects from the frontend (e.g., the page title or user name).</p> <p>Endpoint: <code>POST /api/v1/admin/prompts</code></p> <p>Request Body:</p> <pre><code>{\n  \"id\": \"webAssistantPrompt\",\n  \"role\": \"system\",\n  \"content\": \"You are a helpful assistant for the website. The user is currently viewing the page: '[[${pageTitle}]]'. Answer their questions concisely.\",\n  \"defaultLlmProvider\": \"openai\",\n  \"defaultLlmModel\": \"gpt-5.1\",\n  \"temperature\": \"0.7\",\n  \"timeSaved\": 60,\n  \"requiresMultiModalModel\": false,\n  \"requiresFunctionCallingModel\": false\n}\n</code></pre>"},{"location":"how_to/integrate_web_page/#step-2-html-structure","title":"Step 2: HTML Structure","text":"<p>Create a basic HTML file. You must link the Uxopian CSS and JS files. Add a button that will trigger the interaction.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;title&gt;My Web App&lt;/title&gt;\n\n    &lt;link rel=\"stylesheet\" href=\"web-components.css\" /&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;header&gt;\n      &lt;h1&gt;Welcome to My App&lt;/h1&gt;\n      &lt;button id=\"ask-ai-btn\"&gt;Ask AI Helper&lt;/button&gt;\n    &lt;/header&gt;\n\n    &lt;main&gt;\n      &lt;p&gt;This is a sample page content.&lt;/p&gt;\n    &lt;/main&gt;\n\n    &lt;script src=\"web-components.js\"&gt;&lt;/script&gt;\n\n    &lt;script src=\"app.js\"&gt;&lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"how_to/integrate_web_page/#step-3-javascript-implementation","title":"Step 3: JavaScript Implementation","text":"<p>In your <code>app.js</code> file, add an event listener to the button. When clicked, it will call the <code>createChat</code> function exposed by the web component.</p> <p>We will verify that the prompt ID matches the one created in Step 1, and we will pass the document title dynamically via the <code>payload</code>.</p> <pre><code>// Configuration\nconst AI_CONFIG = {\n  ENDPOINT: \"https://your-uxopian-instance.com\", // HTTP Endpoint\n  WS_ENDPOINT: \"https://your-uxopian-instance.com\", // WebSocket Endpoint (usually same base)\n};\n\ndocument.addEventListener(\"DOMContentLoaded\", () =&gt; {\n  const aiButton = document.getElementById(\"ask-ai-btn\");\n\n  if (aiButton) {\n    aiButton.addEventListener(\"click\", () =&gt; {\n      // 1. Gather Context (e.g., page title, selected text, user ID)\n      const currentPageTitle = document.title;\n\n      // 2. Trigger the Chat Interface\n      try {\n        // 'createChat' is globally available via web-components.js\n        createChat({\n          endpoint: AI_CONFIG.ENDPOINT,\n          wsEndpoint: AI_CONFIG.WS_ENDPOINT,\n          request: {\n            inputs: [\n              {\n                role: \"user\",\n                content: [\n                  {\n                    type: \"PROMPT\",\n                    value: \"webAssistantPrompt\", // Must match Backend ID\n                    payload: {\n                      // Dynamic variables to inject into the prompt\n                      pageTitle: currentPageTitle,\n                    },\n                  },\n                ],\n              },\n            ],\n          },\n        });\n      } catch (error) {\n        console.error(\n          \"Uxopian-ai Web Component not loaded or error initializing:\",\n          error\n        );\n      }\n    });\n  }\n});\n</code></pre>"},{"location":"how_to/integrate_web_page/#technical-details","title":"Technical Details","text":""},{"location":"how_to/integrate_web_page/#the-createchat-function","title":"The <code>createChat</code> Function","text":"<p>This function allows you to open the chat modal programmatically.</p> <ul> <li>endpoint: The base URL for the REST API (e.g., for retrieving history).</li> <li>wsEndpoint: The base URL for WebSocket connections (used for streaming responses).</li> <li>request: The initial payload sent to the LLM to start the conversation.</li> <li>inputs: An array of message objects.</li> <li>value: The ID of the prompt configuration on the server.</li> <li>payload: A key-value object. Keys must match the variables defined in your prompt (e.g., <code>[[${pageTitle}]]</code>).</li> </ul> <p>CORS Configuration</p> <p>Ensure your Uxopian-ai backend is configured to allow Cross-Origin Resource Sharing (CORS) from your web application's domain.</p>"},{"location":"how_to/managing_prompts_goals/","title":"Managing Prompts and Goals","text":"<p>This guide explains how to create, update, and manage Prompts and Goals using the REST API and the admin interface.</p> <p>For a visual guide on managing these resources via the UI, refer to the Admin Interface Guide.</p> <p>For a deep dive into the templating syntax used in prompt content, see The Templating Engine.</p>"},{"location":"how_to/managing_prompts_goals/#managing-prompts-and-goals-via-the-api","title":"Managing Prompts and Goals via the API","text":"<p>The recommended way to manage Prompts and Goals is to store them in OpenSearch using the REST API. This allows for dynamic updates without restarting the service.</p> <p>Security Note</p> <p>These are Admin operations. Your Gateway must inject the <code>X-User-Roles: admin</code> header for these requests to succeed.</p>"},{"location":"how_to/managing_prompts_goals/#api-operations","title":"API Operations","text":"<p>Refer to the Swagger documentation for the complete schema details.</p> <p>Prompts</p> <ul> <li>Save a Prompt: <code>POST /api/v1/admin/prompts</code></li> <li>Update a Prompt: <code>PUT /api/v1/admin/prompts</code></li> <li>Get a Specific Prompt: <code>GET /api/v1/admin/prompts/{id}</code></li> <li>Delete a Prompt: <code>DELETE /api/v1/admin/prompts/{id}</code></li> </ul> <p>Goals</p> <ul> <li>Save a Goal: <code>POST /api/v1/admin/goals</code></li> <li>Update a Goal: <code>PUT /api/v1/admin/goals</code></li> <li>Get All Goals: <code>GET /api/v1/admin/goals</code></li> <li>Delete a Goal: <code>DELETE /api/v1/admin/goals/{id}</code></li> </ul>"},{"location":"how_to/managing_prompts_goals/#example-creating-a-new-prompt","title":"Example: Creating a New Prompt","text":"<p>Use this endpoint to store a prompt configuration, including its default LLM settings.</p> <p>cURL Request</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/admin/prompts\" \\\n-H \"Content-Type: application/json\" \\\n-H \"X-User-TenantId: enterprise-corp-a\" \\\n-H \"X-User-Id: admin-user\" \\\n-H \"X-User-Roles: admin\" \\\n-d '{\n  \"id\": \"summarizeDocumentText\",\n  \"role\": \"user\",\n  \"content\": \"Summarize the following document in a plain text format:\\n\\n[[${documentService.extractTextualContent(documentId)}]]\",\n  \"defaultLlmProvider\": \"openai\",\n  \"defaultLlmModel\": \"gpt-5.1\",\n  \"timeSaved\": 60\n}'\n</code></pre> <p>Note: The <code>timeSaved</code> field (in seconds) is used to calculate ROI stats in the admin panel.</p> <p>Choosing the Right Model</p> <p>The <code>defaultLlmModel</code> you assign to a prompt has a direct impact on response quality, speed, and cost. Use a flagship model (<code>gpt-5.1</code>, <code>gpt-5</code>) for complex analysis, a balanced model (<code>gpt-4.1</code>, <code>gpt-4o</code>) for general use, or a fast model (<code>gpt-5-mini</code>, <code>gpt-4.1-nano</code>) for high-volume, simple tasks. See Choosing the Right Model for a complete guide.</p>"},{"location":"how_to/managing_prompts_goals/#example-creating-a-new-goal","title":"Example: Creating a New Goal","text":"<p>A Goal maps a specific context to a prompt ID.</p> <p>cURL Request</p> <pre><code>curl -X POST \"http://localhost:8080/api/v1/admin/goals\" \\\n-H \"Content-Type: application/json\" \\\n-H \"X-User-TenantId: enterprise-corp-a\" \\\n-H \"X-User-Id: admin-user\" \\\n-H \"X-User-Roles: admin\" \\\n-d '{\n  \"goalName\": \"compare\",\n  \"promptId\": \"detailedComparison\",\n  \"filter\": \"[[${documentType == '\\''contract'\\''}]]\",\n  \"index\": 125\n}'\n</code></pre>"},{"location":"how_to/managing_prompts_goals/#examples-of-prompt-and-goal-definitions","title":"Examples of Prompt and Goal Definitions","text":"<p>Here are practical examples of how to structure your Goal and Prompt logic.</p>"},{"location":"how_to/managing_prompts_goals/#1-goal-logic-orchestration","title":"1. Goal Logic (Orchestration)","text":"<p>Goals use the <code>index</code> property to determine priority (lower numbers are checked first) and a <code>filter</code> to match the context.</p> <p>Logic:</p> <ol> <li>Check if <code>documentType</code> is 'contract'. If yes, use <code>detailedComparison</code>.</li> <li>Otherwise, fall back to <code>genericComparison</code>.</li> </ol> <pre><code>[\n  {\n    \"goalName\": \"compare\",\n    \"promptId\": \"detailedComparison\",\n    \"filter\": \"[[${documentType == 'contract'}]]\",\n    \"index\": 125\n  },\n  {\n    \"goalName\": \"compare\",\n    \"promptId\": \"genericComparison\",\n    \"filter\": \"true\",\n    \"index\": 1000\n  }\n]\n</code></pre>"},{"location":"how_to/managing_prompts_goals/#2-prompt-conditional-logic","title":"2. Prompt: Conditional Logic","text":"<p>This prompt uses a SpEL expression to dynamically set the target language.</p> <pre><code>Translate the following document in [[${language != null} ? ${language} : 'english']]:\n\n[[${documentService.extractTextualContent(documentId)}]]\n</code></pre>"},{"location":"how_to/managing_prompts_goals/#3-prompt-iteration","title":"3. Prompt: Iteration","text":"<p>This prompt iterates over a list of document IDs from the payload to compare multiple documents within a single request.</p> <pre><code>Please be exhaustive and provide a very detailed, point-by-point comparison.\nCompare the following documents:\n\n[# th:each=\"docId, iterStat : ${documentIds}\"]\nDocument content [[${iterStat.count}]] : [[${documentService.extractTextualContent(docId)}]]\n[/]\n</code></pre>"},{"location":"how_to/managing_prompts_goals/#4-prompt-composition-prompt-in-prompt","title":"4. Prompt: Composition (Prompt-in-Prompt)","text":"<p>A prompt can call another prompt. Here, <code>summarizeDocumentMarkdown</code> reuses the formatting rules defined in a separate prompt named <code>markdownResponse</code>.</p> <pre><code>Summarize the following document.\n[[${promptService.renderPrompt('markdownResponse')}]]\n\nDocument content:\n[[${documentService.extractTextualContent(documentId)}]]\n</code></pre>"},{"location":"how_to/managing_prompts_goals/#5-prompt-system-persona","title":"5. Prompt: System Persona","text":"<p>A generic <code>basePrompt</code> can be used to define the persona and core instructions for the AI.</p> <pre><code>You are Nono. You were born in 2025.\nYour primary mission is to assist users by:\nProviding clear and precise answers...\n</code></pre>"},{"location":"how_to/managing_prompts_goals/#web-interface-for-prompt-management","title":"Web Interface for Prompt Management","text":"<p>In addition to the REST API, uxopian-ai includes a built-in web interface that lets you visually manage prompts.</p> <p>Access: <code>https://&lt;your-uxopian-endpoint&gt;/ai</code></p> <p>Through this interface, you can:</p> <ul> <li>View and search existing Prompts.</li> <li>Edit their fields (Content, Filters, Model Settings).</li> <li>Add new Prompts.</li> <li>Delete or reorder items interactively.</li> </ul> <p>For a full walkthrough of the interface, see the Admin Interface Guide.</p>"},{"location":"how_to/monitoring/","title":"Monitoring Performance","text":"<p>This guide covers how to monitor the health and performance of your uxopian-ai deployment using the built-in observability stack.</p>"},{"location":"how_to/monitoring/#monitoring-architecture","title":"Monitoring Architecture","text":"<p>The monitoring system operates in two distinct stages:</p> <ol> <li>Data Collection (Actuator): The Spring Boot Actuator module exposes operational information about the running application \u2014 health, loggers, info.</li> <li>Instrumentation (Micrometer): Micrometer, an application metrics facade, instruments key methods with <code>Timer</code> metrics, recording the duration and count of every execution for a clear view of API performance.</li> </ol>"},{"location":"how_to/monitoring/#actuator-endpoints","title":"Actuator Endpoints","text":"<p>Spring Boot Actuator exposes the following endpoints (configured in <code>metrics.yml</code>):</p> Endpoint Description <code>/actuator/health</code> Application health status. <code>/actuator/info</code> General application information. <code>/actuator/loggers</code> View and modify log levels at runtime."},{"location":"how_to/monitoring/#checking-health","title":"Checking Health","text":"<pre><code>curl http://localhost:8080/actuator/health\n</code></pre> <p>A healthy response returns:</p> <pre><code>{\n  \"status\": \"UP\"\n}\n</code></pre>"},{"location":"how_to/monitoring/#metrics-export-to-opensearch","title":"Metrics Export to OpenSearch","text":"<p>Uxopian-ai uses Micrometer to export telemetry data directly to your OpenSearch instance. This data powers the Admin Dashboard statistics.</p>"},{"location":"how_to/monitoring/#configuration","title":"Configuration","text":"<p>The metrics export is configured in <code>metrics.yml</code>:</p> <pre><code>management:\n  elastic:\n    metrics:\n      export:\n        enabled: true\n        host: http://${opensearch.host}:${opensearch.port}\n        index: micrometer-metrics\n        auto-create-index: true\n  endpoints:\n    web:\n      exposure:\n        include: health,info,loggers\n  metrics:\n    uxopian-ai:\n      enable: true  # Enables custom business metrics (Token usage, ROI, etc.)\n</code></pre>"},{"location":"how_to/monitoring/#custom-business-metrics","title":"Custom Business Metrics","text":"<p>When <code>management.metrics.uxopian-ai.enable</code> is set to <code>true</code>, the following business metrics are collected:</p> <ul> <li>Token consumption \u2014 Input and output tokens per request.</li> <li>Request latency \u2014 Duration of LLM calls.</li> <li>ROI tracking \u2014 Time saved per prompt usage.</li> <li>Feature adoption \u2014 Multi-modal and function calling usage rates.</li> </ul> <p>These metrics feed directly into the Statistics dashboard.</p>"},{"location":"how_to/monitoring/#disabling-noisy-metrics","title":"Disabling Noisy Metrics","text":"<p>By default, standard JVM and framework metrics can generate a large volume of data. You can disable them selectively in <code>metrics.yml</code>:</p> <pre><code>management:\n  metrics:\n    enable:\n      application: false\n      tomcat: false\n      logback: false\n      jvm: false\n      system: false\n      http: false\n      process: false\n      disk: false\n      executor: false\n</code></pre>"},{"location":"how_to/monitoring/#recommended-monitoring-setup","title":"Recommended Monitoring Setup","text":"<p>For production deployments, we recommend:</p> <ol> <li>Health checks: Point your load balancer or orchestrator at <code>/actuator/health</code>.</li> <li>Log aggregation: Forward application logs to a centralized system (ELK, Datadog, etc.).</li> <li>Metrics visualization: Use the built-in Admin Dashboard or connect an external tool (Grafana, Kibana) to the <code>micrometer-metrics</code> index in OpenSearch.</li> <li>Alerting: Set up alerts on key metrics \u2014 LLM response latency, error rates, and token consumption spikes.</li> </ol>"},{"location":"reference/api/","title":"REST API Reference","text":"<p>All interactions with uxopian-ai go through its REST API. This page provides a complete overview of the endpoints and how to access the interactive documentation.</p> <p>For practical usage examples with cURL and JavaScript, see Using the REST API.</p>"},{"location":"reference/api/#interactive-documentation-swagger-ui","title":"Interactive Documentation (Swagger UI)","text":"<p>The service exposes a full OpenAPI 3.1.0 specification. We highly recommend using the built-in Swagger UI for exploring the API, inspecting schemas, and testing requests in real-time.</p>"},{"location":"reference/api/#accessing-swagger-ui","title":"Accessing Swagger UI","text":"<p>Depending on your environment, the Swagger UI is available at:</p> <ul> <li>Local / Docker: <code>http://localhost:8080/swagger-ui.html</code> (Default port is 8080)</li> </ul> <p>Note: If a custom <code>CONTEXT_PATH</code> is configured in <code>application.yml</code>, append it to the base URL (e.g., <code>http://localhost:8080/ai/swagger-ui.html</code>).</p> <p>Public Access</p> <p>The Swagger UI is publicly accessible (no authentication required). Endpoints are organized by tags prefixed with <code>Admin -</code> for admin operations, making it easy to explore the full API surface.</p>"},{"location":"reference/api/#authentication","title":"Authentication","text":"<p>The API expects authentication via headers injected by your Gateway or BFF. See Security Model for the full explanation.</p> Header Required Description <code>X-User-TenantId</code> Yes Tenant isolation key. <code>X-User-Id</code> Yes Unique User ID. <code>X-User-Roles</code> No Comma-separated roles (e.g., <code>admin</code>). <code>X-User-Token</code> No Original user token for downstream context."},{"location":"reference/api/#api-endpoints","title":"API Endpoints","text":""},{"location":"reference/api/#conversations","title":"Conversations","text":"<p>Manage the lifecycle of chat sessions.</p> Method Endpoint Description <code>POST</code> <code>/api/v1/conversations</code> Create a new conversation. <code>GET</code> <code>/api/v1/conversations</code> List user conversations (paginated). <code>GET</code> <code>/api/v1/conversations/{id}</code> Retrieve full details of a conversation. <code>DELETE</code> <code>/api/v1/conversations/{id}</code> Delete a conversation."},{"location":"reference/api/#requests-chat","title":"Requests (Chat)","text":"<p>Send messages and interact with the LLM.</p> Method Endpoint Description <code>POST</code> <code>/api/v1/requests</code> Send a message and get a synchronous response. <code>POST</code> <code>/api/v1/requests/stream</code> Send a message and receive the response as an SSE stream. <code>POST</code> <code>/api/v1/requests/retry</code> Regenerate the last answer. <p>Query Parameters:</p> <ul> <li><code>conversation</code> (Required) \u2014 The conversation ID.</li> <li><code>provider</code> (Optional) \u2014 Override the LLM provider for this request.</li> <li><code>model</code> (Optional) \u2014 Override the LLM model for this request.</li> </ul>"},{"location":"reference/api/#administration-prompts","title":"Administration \u2014 Prompts","text":"<p>Endpoints restricted to users with the <code>admin</code> role.</p> Method Endpoint Description <code>POST</code> <code>/api/v1/admin/prompts</code> Create a new prompt. <code>PUT</code> <code>/api/v1/admin/prompts</code> Update an existing prompt. <code>GET</code> <code>/api/v1/admin/prompts/{id}</code> Get a specific prompt by ID. <code>DELETE</code> <code>/api/v1/admin/prompts/{id}</code> Delete a prompt."},{"location":"reference/api/#administration-goals","title":"Administration \u2014 Goals","text":"Method Endpoint Description <code>POST</code> <code>/api/v1/admin/goals</code> Create a new goal. <code>PUT</code> <code>/api/v1/admin/goals</code> Update an existing goal. <code>GET</code> <code>/api/v1/admin/goals</code> Get all goals. <code>DELETE</code> <code>/api/v1/admin/goals/{id}</code> Delete a goal."},{"location":"reference/api/#administration--llm-providers","title":"Administration \u2014 LLM Providers","text":"<p>Manage LLM provider configurations at runtime. See LLM Provider Management for the full UI guide.</p> Method Endpoint Description <code>GET</code> <code>/api/v1/admin/llm/providers</code> List all registered provider types (bean names). <code>GET</code> <code>/api/v1/admin/llm/provider-conf</code> List all provider configurations for the tenant. <code>GET</code> <code>/api/v1/admin/llm/provider-conf/{id}</code> Get a specific provider configuration by ID. <code>POST</code> <code>/api/v1/admin/llm/provider-conf</code> Create a new provider configuration. <code>PUT</code> <code>/api/v1/admin/llm/provider-conf/{id}</code> Update an existing provider configuration. <code>DELETE</code> <code>/api/v1/admin/llm/provider-conf/{id}</code> Delete a provider configuration."},{"location":"reference/api/#administration--statistics","title":"Administration \u2014 Statistics","text":"<p>All statistics endpoints accept an optional <code>interval</code> query parameter to control time-series granularity. Supported values: <code>HOUR</code>, <code>DAY</code>, <code>WEEK</code>, <code>MONTH</code>, <code>YEAR</code>. Default: <code>DAY</code>.</p> Method Endpoint Description <code>GET</code> <code>/api/v1/admin/stats/global</code> Aggregate counters (total requests, tokens, time saved). <code>GET</code> <code>/api/v1/admin/stats/timeseries?interval=DAY</code> Time-series activity data (requests, tokens over time). <code>GET</code> <code>/api/v1/admin/stats/llm-distribution</code> Model usage distribution breakdown. <code>GET</code> <code>/api/v1/admin/stats/top-prompts-time-saved</code> Top prompts ranked by estimated time saved. <code>GET</code> <code>/api/v1/admin/stats/feature-adoption</code> Advanced feature adoption rates (multi-modal, function calling)."},{"location":"reference/config_files/","title":"\u2699\ufe0f Configuration","text":"<p>This section details the complete configuration of the uxopian-ai service using YAML files.</p> <p>The framework follows the Spring Boot externalized configuration model. Configurations are modular and split into specific files located in the ./config/ directory.</p>"},{"location":"reference/config_files/#1-general-application-configuration-applicationyml","title":"\ud83d\udcc2 1. General Application Configuration (application.yml)","text":"<p>The entry point for configuration. It defines server settings, security profiles, and imports other configuration modules.</p> <pre><code>app:\n  base-url: ${APP_BASE_URL:} # Public URL of the application\n\nserver:\n  servlet:\n    context-path: ${CONTEXT_PATH:} # Context path (e.g., /ai)\n  port: ${UXOPIAN_AI_PORT:8080} # Server port\n\nspring:\n  config:\n    import:\n      - \"optional:file:./config/llm-clients-config.yml\"\n      - \"optional:file:./config/prompts.yml\"\n      - \"optional:file:./config/goals.yml\"\n      - \"optional:file:./config/mcp-server.yml\"\n      - \"optional:file:./config/opensearch.yml\"\n      - \"optional:file:./config/metrics.yml\"\n      - \"optional:file:./config/application.yml\"\n  codec:\n    max-in-memory-size: 20MB # Adjust for large payloads (e.g., images)\n  main:\n    banner-mode: \"off\"\n  profiles:\n    # Use \"dev\" to disable authentication requirements for testing\n    active: ${SPRING_PROFILES_ACTIVE:}\n\ndotenv:\n  ignoreIfMissing: true\n  filename: .env\n</code></pre>"},{"location":"reference/config_files/#2-enterprise-connectors-flowerdocs-arender","title":"\ud83d\udd0c 2. Enterprise Connectors (FlowerDocs &amp; ARender)","text":"<p>To enable deep integration with your document management ecosystem, specific connectors must be configured in application.yml. These settings allow uxopian-ai to retrieve document content and generate previews.</p> <pre><code># FlowerDocs Core Integration\nws:\n  # URL to the FlowerDocs Core Web Services\n  # Examples: https://my-fd/core/ or http://localhost:8081/core\n  url: ${FD_WS_URL:#{null}}\n\n# ARender Integration\nrendition:\n  # Base URL for the ARender Rendition Server\n  # Examples: https://my-arender-rendition or http://localhost:8761\n  base-url: ${RENDITION_BASE_URL:#{null}}\n</code></pre> <p>Note: If these URLs are not set (null), the associated features (RAG on FlowerDocs documents, Document Previews) will be disabled.</p>"},{"location":"reference/config_files/#3-mcp-server-client-mcp-serveryml","title":"\ud83d\udee0\ufe0f 3. MCP Server Client (mcp-server.yml)","text":"<p>uxopian-ai can act as a client for the Model Context Protocol (MCP), connecting to external MCP servers to access tools or resources via SSE (Server-Sent Events).</p> <pre><code>mcp:\n  client:\n    name: uxopian-ai-mcp-server\n    log-requests: true # Useful for debugging tool calls\n  sse:\n    # The endpoint of the external MCP server\n    url: ${MCP_SSE_URL:http://localhost:8081/uxopian/ai/sse}\n\n  # Logging level for the MCP subsystem\n  level:\n    org:\n      springframework:\n        web: TRACE\n</code></pre>"},{"location":"reference/config_files/#4-persistence-vector-database-opensearchyml","title":"\ud83d\udd17 4. Persistence &amp; Vector Database (opensearch.yml)","text":"<p>Configures the connection to OpenSearch. This is critical for storing:</p> <ol> <li>Conversation history</li> <li>Prompts and Goals definitions</li> <li>LLM Provider configurations</li> <li>Vector embeddings (for RAG)</li> <li>Metrics</li> </ol> <pre><code>opensearch:\n  host: ${OPENSEARCH_HOST:localhost}\n  port: ${OPENSEARCH_PORT:9200}\n  scheme: ${OPENSEARCH_SCHEME:http}\n  username: ${OPENSEARCH_USERNAME:} # Leave empty if no auth\n  password: ${OPENSEARCH_PASSWORD:} # Leave empty if no auth\n\n  # CAUTION: Set to true only in development.\n  # Forces an index refresh after every write (impacts performance).\n  force-refresh-index: ${OPENSEARCH_FORCE_REFRESH_INDEX:false}\n</code></pre>"},{"location":"reference/config_files/#5-metrics-monitoring-metricsyml","title":"\ud83d\udcca 5. Metrics &amp; Monitoring (metrics.yml)","text":"<p>Configures Micrometer and Spring Actuator to export telemetry data directly to OpenSearch. This data powers the Admin Dashboard statistics.</p> <pre><code>management:\n  elastic:\n    metrics:\n      export:\n        enabled: true\n        # Target OpenSearch/Elasticsearch instance\n        host: http://${opensearch.host}:${opensearch.port}\n        # Index name for metrics\n        index: micrometer-metrics\n        auto-create-index: true\n  endpoints:\n    web:\n      exposure:\n        include: health,info,loggers\n  metrics:\n    uxopian-ai:\n      enable: true # Enables custom business metrics (Token usage, ROI, etc.)\n    # Standard JVM metrics can be noisy, disable them if not needed\n    enable:\n      application: false\n      tomcat: false\n      logback: false\n      jvm: false\n      system: false\n      http: false\n      process: false\n      disk: false\n      executor: false\n</code></pre>"},{"location":"reference/config_files/#6-llm-clients-configuration-llm-clients-configyml","title":"\ud83e\udd16 6. LLM Clients Configuration (llm-clients-config.yml)","text":"<p>This file manages global LLM defaults and dynamic provider configuration.</p>"},{"location":"reference/config_files/#61-global-defaults-context","title":"6.1 Global Defaults &amp; Context","text":"<pre><code>llm:\n  default:\n    provider: ${LLM_DEFAULT_PROVIDER:openai}\n    model: ${LLM_DEFAULT_MODEL:gpt-5.1}\n    base-prompt: ${LLM_DEFAULT_PROMPT:basePrompt}\n\n  context: ${LLM_CONTEXT_SIZE:10} # Sliding window size (number of messages)\n  debug:\n    enabled: ${LLM_DEBUG:false} # Logs full requests/responses (CAUTION: Sensitive data)\n</code></pre>"},{"location":"reference/config_files/#dynamic-provider-configuration","title":"6.2 Dynamic Provider Configuration","text":"<p>Since v2026.0.0-ft2, LLM provider configurations are dynamic entities stored in OpenSearch. They can be created, updated, and deleted at runtime via the Admin API or the Admin UI.</p> <p>YAML bootstrapping is still supported: configurations defined in this file are loaded into OpenSearch at startup, then managed dynamically. This section documents the YAML structure used for bootstrapping.</p>"},{"location":"reference/config_files/#structure-overview","title":"Structure Overview","text":"<p>A provider configuration (<code>LlmProviderConf</code>) contains:</p> <ul> <li>Provider type \u2014 The name of the registered provider bean (e.g., <code>openai</code>, <code>anthropic</code>).</li> <li>Default model alias \u2014 Which model to use when no model is specified.</li> <li>Global configuration (<code>globalConf</code>) \u2014 Connection and generation parameters shared across all models.</li> <li>Model configurations (<code>llModelConfs[]</code>) \u2014 Per-model overrides. Each model inherits from <code>globalConf</code> and can override any field.</li> </ul>"},{"location":"reference/config_files/#configuration-fields","title":"Configuration Fields","text":"<p><code>LlmBaseConf</code> \u2014 Shared by both global and per-model configurations:</p> Field Type Description <code>apiSecret</code> String API key or secret. Encrypted at rest via AES-GCM. <code>endpointUrl</code> String Provider API base URL. <code>temperature</code> Double Sampling temperature (0.0 \u2013 2.0). <code>topP</code> Double Nucleus sampling threshold. <code>topK</code> Integer Top-K sampling parameter. <code>seed</code> Integer Deterministic seed for reproducibility. <code>maxTokens</code> Integer Maximum tokens in the response. <code>presencePenalty</code> Double Penalizes repeated topics. <code>frequencyPenalty</code> Double Penalizes repeated tokens. <code>maxRetries</code> Integer Number of retry attempts on failure. <code>timeout</code> Duration Request timeout (e.g., <code>60s</code>). <code>multiModalSupported</code> Boolean Whether the model supports image inputs. <code>functionCallSupported</code> Boolean Whether the model supports tool/function calling. <code>extras</code> Map Provider-specific key-value pairs (e.g., <code>deploymentName</code> for Azure). <p><code>LlmModelConf</code> \u2014 Extends <code>LlmBaseConf</code> with two additional fields:</p> Field Type Description <code>llmModelConfName</code> String Alias used in prompts and API calls (e.g., <code>my-gpt5</code>). <code>modelName</code> String Actual model identifier sent to the provider's API (e.g., <code>gpt-5.1</code>). <p>Configuration Inheritance</p> <p>When processing a request, the service merges global and model-specific settings. Model-level values take precedence over global values. Fields not set at the model level fall back to the global configuration. See Parameter Precedence.</p>"},{"location":"reference/config_files/#full-yaml-example","title":"Full YAML Example","text":"<pre><code>llm:\n  provider:\n    # 1. Global Provider Configurations (apply to all tenants)\n    globals:\n      - provider: openai\n        defaultLlmModelConfName: gpt5\n        globalConf:\n          apiSecret: ${OPENAI_API_KEY:}\n          temperature: 0.7\n          maxRetries: 3\n          timeout: 60s\n        llModelConfs:\n          - llmModelConfName: gpt5\n            modelName: gpt-5.1\n            multiModalSupported: true\n            functionCallSupported: true\n          - llmModelConfName: gpt5-mini\n            modelName: gpt-5-mini\n            temperature: 0.3\n            multiModalSupported: true\n            functionCallSupported: true\n          - llmModelConfName: gpt4\n            modelName: gpt-4.1\n            multiModalSupported: true\n            functionCallSupported: true\n\n      - provider: anthropic\n        defaultLlmModelConfName: claude-sonnet\n        globalConf:\n          apiSecret: ${ANTHROPIC_API_KEY:}\n          endpointUrl: https://api.anthropic.com/v1/\n        llModelConfs:\n          - llmModelConfName: claude-sonnet\n            modelName: claude-sonnet-4-20250514\n            multiModalSupported: true\n            functionCallSupported: true\n\n    # 2. Tenant-Specific Overrides\n    tenants:\n      - tenantId: tenant-A\n        mergeStrategy: MERGE  # Options: MERGE, OVERWRITE, CREATE_IF_MISSING\n        providers:\n          - provider: openai\n            defaultLlmModelConfName: gpt5\n            globalConf:\n              apiSecret: sk-tenant-a-specific-key\n            llModelConfs:\n              - llmModelConfName: gpt5\n                modelName: gpt-5.1\n                multiModalSupported: true\n                functionCallSupported: true\n</code></pre>"},{"location":"reference/config_files/#merge-strategies","title":"Merge Strategies","text":"<p>When tenant-specific configurations are defined, the <code>mergeStrategy</code> field controls how they combine with global configurations:</p> Strategy Behavior <code>MERGE</code> Tenant providers are merged with globals. Matching providers are updated; non-matching ones are added. <code>OVERWRITE</code> Tenant configuration completely replaces the global configuration. <code>CREATE_IF_MISSING</code> Tenant providers are only added if no global configuration exists for that provider type."},{"location":"reference/config_files/#api-secret-encryption","title":"API Secret Encryption","text":"<p>API secrets stored in OpenSearch are encrypted using AES-GCM. Set the encryption key via the <code>APP_SECURITY_SECRET_KEY</code> environment variable (Base64-encoded AES key). See Environment Variables.</p>"},{"location":"reference/config_files/#7-bootstrapping-prompts-goals-llm-providers","title":"\ud83e\udde9 7. Bootstrapping Prompts, Goals &amp; LLM Providers","text":"<p>These files are used to initialize the system with default data. They support global definitions and tenant-specific overrides. Data defined in YAML is loaded into OpenSearch at startup and can then be managed dynamically via the Admin API or UI.</p>"},{"location":"reference/config_files/#prompts-promptsyml","title":"Prompts (prompts.yml)","text":"<pre><code>prompts:\n  backup:\n    path: ${PROMPTS_BACKUP_PATH:./prompts/}\n\n  # 1. Global Prompts (Apply to everyone)\n  globals:\n    - id: basePrompt\n      role: SYSTEM\n      content: |\n        You are Xopia. You were born in 2025...\n      reasoningDisabled: false\n      requiresMultiModalModel: false\n      requiresFunctionCallingModel: false\n\n  # 2. Tenant Specifics\n  tenants:\n    - tenantId: tenant-A\n      mergeStrategy: merge # Options: merge, override, createIfMissing\n      prompts:\n        - id: specificPrompt\n          role: USER\n          content: \"Use only the information explicitly present in the document...\"\n          requiresMultiModalModel: false\n          requiresFunctionCallingModel: false\n</code></pre>"},{"location":"reference/config_files/#goals-goalsyml","title":"Goals (goals.yml)","text":"<pre><code>goals:\n  backup:\n    path: ${GOALS_BACKUP_PATH:./goals/}\n\n  globals: []\n\n  tenants:\n    - tenantId: tenant-A\n      mergeStrategy: merge\n      goalGroups:\n        - id: compare\n          goals:\n            - promptId: detailedComparisonForTenantA\n              filter: \"[[${documentType == 'contract'}]]\"\n              index: 125\n            - promptId: genericComparison\n              filter: \"true\"\n              index: 1000\n</code></pre>"},{"location":"reference/config_files/#llm-providers-llm-clients-configyml","title":"LLM Providers (llm-clients-config.yml)","text":"<p>LLM Provider configurations follow the same bootstrapping pattern. They are defined under <code>llm.provider.globals</code> and <code>llm.provider.tenants</code> in <code>llm-clients-config.yml</code>. See Section 6.2 for the full YAML structure and field reference.</p> <p>For a quick reference of all environment variables available for Docker deployments, see Environment Variables.</p>"},{"location":"reference/env_variables/","title":"Environment Variables Reference","text":"<p>For Docker and production deployments, override these variables instead of editing YAML files directly. Set them in your <code>docker-compose.yml</code>, <code>.env</code> file, or container orchestrator.</p>"},{"location":"reference/env_variables/#general","title":"General","text":"Variable Description Default <code>SPRING_PROFILES_ACTIVE</code> Active profile (use <code>dev</code> to disable auth). empty <code>UXOPIAN_AI_PORT</code> Application server port. <code>8080</code> <code>APP_BASE_URL</code> Public base URL of the service. empty <code>CONTEXT_PATH</code> Servlet context path (e.g., <code>/ai</code>). empty"},{"location":"reference/env_variables/#security","title":"Security","text":"Variable Description Default <code>APP_SECURITY_SECRET_KEY</code> Base64-encoded AES key for encrypting LLM provider API secrets at rest. empty"},{"location":"reference/env_variables/#llm-providers","title":"LLM Providers","text":"Variable Description Default <code>LLM_DEFAULT_PROVIDER</code> Default LLM provider. <code>openai</code> <code>LLM_DEFAULT_MODEL</code> Default LLM model name. <code>gpt-5.1</code> <code>LLM_DEFAULT_PROMPT</code> Default base prompt ID. <code>basePrompt</code> <code>LLM_CONTEXT_SIZE</code> Sliding window size (messages). <code>10</code> <code>LLM_DEBUG</code> Log full LLM requests/responses (sensitive). <code>false</code> <code>OPENAI_API_KEY</code> API Key for OpenAI. empty <code>ANTHROPIC_API_KEY</code> API Key for Anthropic. <code>none</code> <code>AZURE_OPENAI_API_KEY</code> API Key for Azure OpenAI. <code>none</code> <code>GEMINI_API_KEY</code> API Key for Google Gemini. <code>none</code> <code>MISTRAL_API_KEY</code> API Key for Mistral AI. <code>none</code> <code>HUGGINGFACE_API_KEY</code> API Key for HuggingFace. <code>none</code> <code>BEDROCK_AWS_ACCESS_KEY</code> AWS access key for Bedrock. <code>none</code> <code>BEDROCK_AWS_SECRET_KEY</code> AWS secret key for Bedrock. <code>none</code> <p>Dynamic Provider Configuration</p> <p>Since v2026.0.0-ft2, LLM API keys are primarily managed via the dynamic provider configuration stored in OpenSearch. The individual environment variables above (<code>OPENAI_API_KEY</code>, etc.) remain functional for bean initialization and YAML bootstrapping.</p>"},{"location":"reference/env_variables/#opensearch","title":"OpenSearch","text":"Variable Description Default <code>OPENSEARCH_HOST</code> Hostname of the OpenSearch instance. <code>localhost</code> <code>OPENSEARCH_PORT</code> Port of the OpenSearch instance. <code>9200</code> <code>OPENSEARCH_SCHEME</code> Connection scheme (<code>http</code> or <code>https</code>). <code>http</code> <code>OPENSEARCH_USERNAME</code> OpenSearch username (if secure). empty <code>OPENSEARCH_PASSWORD</code> OpenSearch password (if secure). empty <code>OPENSEARCH_FORCE_REFRESH_INDEX</code> Force index refresh after writes (dev only, impacts perf). <code>false</code>"},{"location":"reference/env_variables/#integrations","title":"Integrations","text":"Variable Description Default <code>FD_WS_URL</code> FlowerDocs Core Web Services URL. null <code>RENDITION_BASE_URL</code> ARender Rendition Server base URL. null <code>MCP_SSE_URL</code> MCP server SSE endpoint URL. <code>http://localhost:8081/uxopian/ai/sse</code>"},{"location":"reference/env_variables/#backup","title":"Backup","text":"Variable Description Default <code>PROMPTS_BACKUP_PATH</code> Path to load/store prompt backups. <code>./prompts/</code> <code>GOALS_BACKUP_PATH</code> Path to load/store goal backups. <code>./goals/</code>"},{"location":"release_notes/v2026.0.0-ft1-rc2/","title":"Release Notes: uxopian-ai v2026.0.0-ft1-rc2","text":"<p>Release Date: February 2026 Version: 2026.0.0-ft1-rc2</p> <p>We are thrilled to announce the general availability of uxopian-ai 2025.0.0!</p> <p>uxopian-ai is a complete, standalone framework designed to accelerate the integration of GenAI features into enterprise applications. Built on Java 21 LTS and Spring 3.5, it moves beyond simple libraries to provide a deployable service with full conversation management, advanced orchestration, and enterprise-grade security.</p>"},{"location":"release_notes/v2026.0.0-ft1-rc2/#highlights","title":"\ud83c\udf1f Highlights","text":""},{"location":"release_notes/v2026.0.0-ft1-rc2/#standalone-service","title":"\ud83d\ude80 Standalone Service","text":"<p>Stop building the plumbing. uxopian-ai is a pre-packaged backend service that manages the complexities of LLM interactions. It is deployable immediately via Docker or as a Java 21 application, saving months of infrastructure setup.</p>"},{"location":"release_notes/v2026.0.0-ft1-rc2/#the-goal-system-context-aware-orchestration","title":"\ud83e\udde0 The Goal System (Context-Aware Orchestration)","text":"<p>This is the heart of the framework. Instead of hardcoding prompts in your application code, you send a generic Goal (e.g., \"compare\"). The engine dynamically selects the correct Prompt based on context filters (Tenant, Document Type, User Role) using SpEL (Spring Expression Language).</p>"},{"location":"release_notes/v2026.0.0-ft1-rc2/#native-multi-tenancy","title":"\ud83d\udee1\ufe0f Native Multi-Tenancy","text":"<p>Built for the enterprise, every conversation, prompt, and statistic is strictly scoped to a Tenant ID. The architecture supports a BFF (Backend for Frontend) pattern, relying on security headers (<code>X-User-TenantId</code>) to ensure logical data separation.</p>"},{"location":"release_notes/v2026.0.0-ft1-rc2/#key-features","title":"\u2728 Key Features","text":""},{"location":"release_notes/v2026.0.0-ft1-rc2/#llm-interaction-connectors","title":"\ud83e\udd16 LLM Interaction &amp; Connectors","text":"<ul> <li>Provider Abstraction: Out-of-the-box support for major providers (OpenAI, Azure, Anthropic).</li> <li>Parameter Precedence: A granular hierarchy for configuration. Parameters passed in the API call override Prompt defaults, which in turn override Global defaults.</li> <li>Advanced Capabilities: Native support for Function Calling, Multi-modal inputs (Text + Base64 Images), and streaming responses.</li> <li>MCP Client: Acts as a client for Multi-Content Platform (MCP) servers.</li> </ul>"},{"location":"release_notes/v2026.0.0-ft1-rc2/#conversation-management","title":"\ud83d\udcac Conversation Management","text":"<ul> <li>Persistent History: All conversations are stored in OpenSearch, managing the context window automatically.</li> <li>Rich Requests: Requests track inputs, answers, and token usage.</li> <li>User Feedback Loop: Built-in endpoints to tag responses as Good, Bad, or Neutral, enabling continuous improvement of prompt quality.</li> </ul>"},{"location":"release_notes/v2026.0.0-ft1-rc2/#frontend-components","title":"\ud83c\udfa8 Frontend Components","text":"<ul> <li>Web Components: A set of lightweight, embeddable JS/CSS components to add a Chat UI to any web application instantly.</li> <li>Thymeleaf Templating: Prompts and Image inputs support dynamic variable injection via Thymeleaf.</li> </ul>"},{"location":"release_notes/v2026.0.0-ft1-rc2/#admin-analytics-dashboard","title":"\ud83d\udcca Admin &amp; Analytics Dashboard","text":"<p>A comprehensive administration panel to monitor the health and ROI of your AI stack:</p> <ul> <li>ROI Tracking: Define \"Time Saved\" per prompt to calculate the estimated human hours saved.</li> <li>Token Monitoring: Visualize input/output token consumption globally or per user.</li> <li>Adoption Trends: Track daily requests, active users, and advanced feature adoption (e.g., Multi-modal usage).</li> </ul>"},{"location":"release_notes/v2026.0.0-ft1-rc2/#architecture-integrations","title":"\ud83c\udfd7\ufe0f Architecture &amp; Integrations","text":""},{"location":"release_notes/v2026.0.0-ft1-rc2/#technical-stack","title":"Technical Stack","text":"<ul> <li>Core: Java 21 LTS, Spring 3.5.</li> <li>LLM Engine: Powered by Langchain4j.</li> <li>Storage: OpenSearch (Vectors &amp; Metadata).</li> <li>API: Fully documented REST API with Swagger UI.</li> </ul>"},{"location":"release_notes/v2026.0.0-ft1-rc2/#ecosystem-integrations","title":"Ecosystem Integrations","text":"<p>v2026.0.0-ft1-rc2 demonstrates the framework's flexibility through complex integrations with ARender and FlowerDocs.</p> <p>These integrations were achieved seamlessly using standard JavaScript scripts and ScriptOperationHandlers, proving that uxopian-ai can be easily embedded into existing High Content Interfaces (HCI) and Content Services Platforms (CSP) without requiring heavy backend modifications.</p> <p>Ready to start? Check out the Quick Start or the full Installation Guide to deploy your first instance.</p>"},{"location":"release_notes/v2026.0.0-ft2/","title":"Release Notes: uxopian-ai v2026.0.0-ft2","text":"<p>Release Date: February 2026 Version: 2026.0.0-ft2</p> <p>This release introduces dynamic LLM provider management, a standalone Gateway service, and significant improvements to the admin panel, statistics, and developer experience.</p>"},{"location":"release_notes/v2026.0.0-ft2/#highlights","title":"\ud83c\udf1f Highlights","text":""},{"location":"release_notes/v2026.0.0-ft2/#dynamic-llm-provider-configuration","title":"\ud83d\udd27 Dynamic LLM Provider Configuration","text":"<p>LLM provider configurations are now dynamic entities stored in OpenSearch, replacing the previous static YAML-only approach. Providers, models, and their parameters can be created, updated, and deleted at runtime \u2014 per tenant \u2014 without restarting the service.</p> <p>Key changes:</p> <ul> <li>New provider configuration entity with global settings and per-model overrides.</li> <li>Full CRUD via the Admin API (<code>/api/v1/admin/llm/provider-conf</code>).</li> <li>Per-tenant configuration with merge strategies: <code>OVERWRITE</code>, <code>MERGE</code>, <code>CREATE_IF_MISSING</code>.</li> <li>AES-GCM encryption for API secrets at rest.</li> <li>YAML bootstrapping still supported \u2014 configurations defined in <code>llm-clients-config.yml</code> are loaded into OpenSearch at startup, then managed dynamically.</li> </ul> <p>See Configuration Files \u2014 Dynamic Provider Configuration and LLM Provider Management.</p>"},{"location":"release_notes/v2026.0.0-ft2/#standalone-gateway-service","title":"\ud83d\udee1\ufe0f Standalone Gateway Service","text":"<p>The BFF Gateway is now a standalone service, deployed independently from the AI service. The runtime architecture remains the same (Gateway authenticates, injects headers, proxies to AI service), but the Gateway can now be scaled and updated independently. This simplifies deployment and allows independent scaling of the security layer.</p> <p>See Security Model.</p>"},{"location":"release_notes/v2026.0.0-ft2/#statistics-improvements","title":"\ud83d\udcca Statistics Improvements","text":"<p>The statistics API has been expanded from a single endpoint to 5 dedicated endpoints, each with a configurable time interval parameter:</p> <ul> <li><code>GET /api/v1/admin/stats/global</code> \u2014 Aggregate counters</li> <li><code>GET /api/v1/admin/stats/timeseries?interval=DAY</code> \u2014 Time-series trends</li> <li><code>GET /api/v1/admin/stats/llm-distribution</code> \u2014 Model usage breakdown</li> <li><code>GET /api/v1/admin/stats/top-prompts-time-saved</code> \u2014 ROI ranking</li> <li><code>GET /api/v1/admin/stats/feature-adoption</code> \u2014 Advanced feature usage rates</li> </ul> <p>Supported intervals: <code>HOUR</code>, <code>DAY</code>, <code>WEEK</code>, <code>MONTH</code>, <code>YEAR</code>.</p> <p>See Statistics &amp; ROI and REST API Reference.</p>"},{"location":"release_notes/v2026.0.0-ft2/#new-features","title":"\u2728 New Features","text":""},{"location":"release_notes/v2026.0.0-ft2/#prompt-tester","title":"\ud83e\uddea Prompt Tester","text":"<p>The admin panel now includes a Prompt Tester that lets you execute prompts directly from the UI:</p> <ul> <li>Automatically detects Thymeleaf variables in the prompt template.</li> <li>Provides input fields for each variable (text or image).</li> <li>Executes the prompt against the configured LLM and displays the result.</li> <li>Generates the equivalent cURL command for easy reproduction.</li> </ul> <p>See Prompt Management \u2014 Prompt Tester.</p>"},{"location":"release_notes/v2026.0.0-ft2/#llm-provider-admin-ui","title":"\ud83d\udda5\ufe0f LLM Provider Admin UI","text":"<p>A complete management interface for LLM provider configurations:</p> <ul> <li>Provider List \u2014 Table with search, filter, and CRUD actions.</li> <li>Provider Editor \u2014 Form to configure provider identity, global settings, and per-model overrides.</li> <li>Connection Tester \u2014 Test connectivity per model with live status badges.</li> </ul> <p>See LLM Provider Management.</p>"},{"location":"release_notes/v2026.0.0-ft2/#fast2-authentication-provider","title":"\ud83d\udd11 Fast2 Authentication Provider","text":"<p>New built-in <code>Fast2Provider</code> for the Gateway. It validates JWT tokens issued by Fast2 by fetching the public key from a configurable remote endpoint. Configure it as <code>provider: Fast2Provider</code> in the Gateway route configuration.</p>"},{"location":"release_notes/v2026.0.0-ft2/#swagger-openapi-improvements","title":"\ud83d\udcd6 Swagger / OpenAPI Improvements","text":"<ul> <li>All admin controllers now use consistent <code>Admin -</code> tag prefixes for better organization.</li> <li>Complete request/response schema documentation on all endpoints.</li> <li>Swagger UI is publicly accessible (no authentication required) \u2014 ideal for API exploration during development.</li> </ul>"},{"location":"release_notes/v2026.0.0-ft2/#technical-changes","title":"\ud83c\udfd7\ufe0f Technical Changes","text":""},{"location":"release_notes/v2026.0.0-ft2/#modelprovider-interface-update","title":"\ud83e\udd16 ModelProvider Interface Update","text":"<p>The <code>ModelProvider</code> interface has been simplified:</p> <ul> <li>Before: <code>createChatModelInstance(String modelName)</code>, <code>getDefaultModelName()</code>, <code>getSupportedModels()</code></li> <li>After: <code>createChatModelInstance(LlmModelConf params)</code>, <code>createStreamingChatModelInstance(LlmModelConf params)</code></li> </ul> <p>The <code>getDefaultModelName()</code> and <code>getSupportedModels()</code> methods have been removed \u2014 model metadata is now managed via dynamic provider configurations. Custom providers should extend <code>AbstractLlmClient</code> and use <code>params.getModelName()</code>, <code>params.getApiSecret()</code>, etc.</p> <p>See Adding a New LLM Provider.</p>"},{"location":"release_notes/v2026.0.0-ft2/#parameter-precedence-5-levels","title":"\u2696\ufe0f Parameter Precedence (5 Levels)","text":"<p>The parameter resolution hierarchy has been extended from 3 to 5 levels:</p> <ol> <li>API Call Parameters \u2014 Values passed directly in the request.</li> <li>Prompt Defaults \u2014 <code>defaultLlmModel</code>, <code>defaultLlmProvider</code> on the Prompt entity.</li> <li>Provider Model Config \u2014 Per-model settings in <code>LlmModelConf</code>.</li> <li>Provider Global Config \u2014 Shared settings in <code>LlmProviderConf.globalConf</code>.</li> <li>YAML Global Defaults \u2014 <code>llm.default.*</code> in <code>llm-clients-config.yml</code>.</li> </ol>"},{"location":"release_notes/v2026.0.0-ft2/#dependency-upgrades","title":"\ud83d\udce6 Dependency Upgrades","text":"Dependency Previous Current Spring Boot 3.5.x 3.5.10 LangChain4J 1.x 1.11.0 OpenSearch Client 2.x 3.5.0 Docker Base Image 1.0.x 1.0.4"},{"location":"release_notes/v2026.0.0-ft2/#frontend-changes","title":"\ud83c\udfa8 Frontend Changes","text":"<ul> <li>Improved state management for better performance and responsiveness.</li> <li>Fixed edge cases in markdown rendering within chat responses.</li> <li>Improved auto-scroll behavior during streaming responses.</li> </ul>"},{"location":"release_notes/v2026.0.0-ft2/#migration-notes","title":"\ud83d\udd04 Migration Notes","text":""},{"location":"release_notes/v2026.0.0-ft2/#from-v202600-ft1-rc2","title":"From v2026.0.0-ft1-rc2","text":"<ol> <li> <p>LLM Configuration: The <code>llm-clients-config.yml</code> format has changed. The previous <code>supported-models</code> lists under each provider section are no longer supported. You must migrate your provider and model definitions to the new <code>llm.provider.globals</code> / <code>llm.provider.tenants</code> structure. These configurations are loaded into OpenSearch at startup and can then be managed dynamically via the Admin API or UI. See Configuration Files \u2014 Dynamic Provider Configuration for the new format and a full YAML example.</p> </li> <li> <p>Gateway Deployment: The Gateway is now deployed as a separate service. Update your Docker compose to use the dedicated Gateway image (<code>uxopian-ai/gateway-service</code>). The configuration format (<code>application.yml</code> with routes) remains the same.</p> </li> <li> <p>Custom LLM Providers: If you have custom <code>ModelProvider</code> implementations, update them to accept <code>LlmModelConf</code> instead of <code>String</code> in factory methods. Extend <code>AbstractLlmClient</code> for convenience. See the updated guide.</p> </li> <li> <p>API Secret Encryption: Set the <code>APP_SECURITY_SECRET_KEY</code> environment variable (Base64-encoded AES key) to enable encryption of provider API secrets stored in OpenSearch. If not set, secrets are stored in clear text.</p> </li> </ol> <p>Ready to start? Check out the Quick Start or the full Installation Guide.</p>"},{"location":"understanding/architecture/","title":"Architecture Overview","text":"<p>This section provides insight into the framework's design, covering both the high-level components and the software-level interactions.</p>"},{"location":"understanding/architecture/#component-architecture","title":"Component Architecture","text":"<p>This section provides insight into the framework's design, covering both the high-level components and the software-level interactions, specifically highlighting the security integration via the BFF pattern.</p> <p>The uxopian-ai framework is designed as a backend microservice that sits behind a security gateway. It is composed of several key components working in concert.</p> <p></p>"},{"location":"understanding/architecture/#component-descriptions","title":"Component Descriptions","text":"<p>Client Application User-facing application (e.g., ARender, FlowerDocs) that initiates requests. It never communicates directly with uxopian-ai.</p> <p>BFF / Gateway (Security Layer) Entry point for all traffic. The Gateway is a standalone, independently deployed service built on Spring Cloud Gateway (reactive) with a pluggable auth provider system. Responsible for:</p> <ul> <li>Authenticating the user via a pluggable provider (OAuth2, JWT, LDAP, or the <code>DevProvider</code> for development)</li> <li>Enriching requests with identity headers (<code>X-User-TenantId</code>, <code>X-User-Id</code>, <code>X-User-Roles</code>, <code>X-User-Token</code>)</li> <li>Enforcing role-based access control (e.g., admin-only paths)</li> <li>Proxying the request to the backend service</li> </ul> <p>The Gateway processes each request through a filter pipeline: <code>DefaultProviderHeaderFilter</code> (route matching) \u2192 <code>AuthFilter</code> (authentication + header injection) \u2192 Spring Cloud Gateway (routing). See Security Model for the full pipeline details and provider reference.</p> <p>uxopian-ai Service The core of the framework. This standalone Java application:</p> <ul> <li>Exposes the REST API (consumed by the BFF)</li> <li>Manages conversations and messages per Tenant ID</li> <li>Resolves Goals and Prompts via the templating engine</li> <li>Connects to external LLM providers using the <code>llm-clients</code> module</li> </ul> <p>OpenSearch Primary data store for:</p> <ul> <li>Conversations</li> <li>Messages</li> <li>Prompts</li> <li>Goals</li> </ul> <p>ARender Rendition Service External service used to fetch document content (e.g., extracting text).</p> <p>Qdrant Optional vector database enabling RAG (Retrieval-Augmented Generation).</p> <p>External LLM Providers Third-party services (OpenAI, Azure, etc.) handling natural language processing.</p>"},{"location":"understanding/architecture/#software-architecture-request-flow","title":"Software Architecture (Request Flow)","text":"<p>To understand how the components interact, here is the lifecycle of a typical API call: sending a message that triggers a Goal.</p> <p>The request flow emphasizes the role of the BFF in establishing the security context before the service logic executes.</p>"},{"location":"understanding/architecture/#sequence-diagram-executing-a-goal","title":"Sequence Diagram: Executing a Goal","text":""},{"location":"understanding/architecture/#_1","title":"Architecture Overview","text":""},{"location":"understanding/architecture/#workflow-steps","title":"Workflow Steps","text":"<ol> <li> <p>Client Request    Client sends a request to the BFF (e.g., <code>POST /api/v1/requests</code>) with a goal input such as <code>\"compare\"</code>.</p> </li> <li> <p>Authentication &amp; Injection    The BFF authenticates, then injects <code>X-User-TenantId</code>, <code>X-User-Id</code>, etc.</p> </li> <li> <p>Context Establishment    uxopian-ai reads the headers to derive the security context.</p> </li> <li> <p>Goal Resolution    The service queries OpenSearch for Goals matching <code>\"compare\"</code> in the tenant.</p> </li> <li> <p>Filter Evaluation    SpEL filters narrow the choice to a specific <code>promptId</code> (e.g., <code>\"detailedComparison\"</code>).</p> </li> <li> <p>Prompt &amp; Context Retrieval    The Prompt definition and conversation history are loaded.</p> </li> <li> <p>Template Rendering    Thymeleaf produces the final LLM prompt, optionally pulling external content.</p> </li> <li> <p>LLM Interaction    The prompt is sent to the configured LLM provider.</p> </li> <li> <p>Persistence    User message and LLM response are saved in OpenSearch.</p> </li> <li> <p>Response     The final response is returned through the BFF to the client.</p> </li> </ol>"},{"location":"understanding/concepts/","title":"\ud83e\udde0 Core Concepts","text":"<p>This section explains the primary entities and concepts that form the uxopian-ai framework. Understanding these concepts is essential for effective configuration and interaction.</p>"},{"location":"understanding/concepts/#multi-tenancy-users","title":"\ud83d\udd10 Multi-Tenancy &amp; Users","text":"<p>uxopian-ai is built with a multi-tenant architecture from the ground up, allowing for secure and logical separation of data within a single deployment.</p> <ul> <li>Tenants: Every interaction is scoped to a specific tenantId . This ensures that conversations, stats, and configurations are isolated per tenant.</li> <li>Users: Users are identified by an ID and assigned specific roles (including an admin flag) . This role-based access control governs access to the Admin API and sensitive operations.</li> </ul>"},{"location":"understanding/concepts/#providers-models","title":"\ud83d\udd0c Providers &amp; Models","text":"<p>A Provider is a connector to an external Large Language Model (LLM) service. The framework uses providers to abstract the specific implementation details of each LLM service.</p> <ul> <li>Providers: Providers are configured dynamically per tenant via <code>LlmProviderConf</code> entities stored in OpenSearch. You can manage them through the Admin API or the Admin UI. The list of registered provider types (e.g., <code>openai</code>, <code>azure</code>, <code>anthropic</code>) is available via <code>GET /api/v1/admin/llm/providers</code>.</li> <li>Models: Each provider configuration includes a list of model configurations (<code>LlmModelConf</code>), each with an alias (<code>llmModelConfName</code>) and the actual model identifier (<code>modelName</code>). The framework tracks model capabilities, specifically whether a model supports multi-modal inputs or function calling.</li> </ul>"},{"location":"understanding/concepts/#choosing-the-right-model","title":"Choosing the Right Model","text":"<p>Not all models are equal. Choosing the right one for each prompt is critical for balancing response quality, speed, and cost.</p> Model Tier Examples Strengths Best For Flagship <code>gpt-5.1</code>, <code>gpt-5</code>, <code>claude-3-opus</code>, <code>gemini-2.5-pro</code> Deepest reasoning, highest accuracy Complex analysis, multi-step logic, detailed comparisons Balanced <code>gpt-4.1</code>, <code>gpt-4o</code>, <code>claude-3-5-sonnet</code>, <code>gemini-2.5-flash</code> Good reasoning with faster response General-purpose use, summaries, document Q&amp;A Fast &amp; Cheap <code>gpt-5-mini</code>, <code>gpt-4.1-mini</code>, <code>gpt-4o-mini</code>, <code>gemini-2.5-flash-lite</code> Low latency, low cost per token High-volume tasks, simple extractions, intermediate map-reduce steps Ultra-light <code>gpt-5-nano</code>, <code>gpt-4.1-nano</code>, <code>gpt-3.5-turbo</code>, <code>gemini-2.0-flash-lite</code> Minimal cost, fastest response Classification, keyword extraction, routing decisions Reasoning <code>o3-mini</code>, <code>o4-mini</code> Extended chain-of-thought reasoning Math, logic puzzles, code generation (no multi-modal) <p>Practical Guidance</p> <ul> <li>Start balanced, optimize later. Use <code>gpt-5.1</code> or <code>gpt-4.1</code> to validate your prompt logic, then switch to a <code>mini</code> or <code>nano</code> variant once it works.</li> <li>Use cheap models for intermediate steps. In a Map-Reduce helper, the map phase processes many chunks \u2014 use a fast model like <code>gpt-5-mini</code> there, and reserve the flagship model for the final reduce step.</li> <li>Check capability flags. If your prompt uses images (<code>requiresMultiModalModel: true</code>) or tool calls (<code>requiresFunctionCallingModel: true</code>), verify that your chosen model supports them in its <code>LlmModelConf</code>. The <code>o3-mini</code>/<code>o4-mini</code> reasoning models, for example, do not support multi-modal inputs.</li> <li>Monitor with the admin panel. The Statistics dashboard shows token consumption and model distribution \u2014 use it to identify prompts that could benefit from a cheaper model.</li> </ul>"},{"location":"understanding/concepts/#model-capabilities","title":"Model Capabilities","text":"<p>Each model configuration (<code>LlmModelConf</code>) declares two capability flags:</p> <ul> <li><code>multiModalSupported</code>: Can the model process image inputs (Base64 or URL)? Required for prompts that inject images via <code>documentService.getPageImage(...)</code>.</li> <li><code>functionCallSupported</code>: Can the model invoke external tools (MCP, custom tools)? Required for prompts that trigger custom tools.</li> </ul> <p>When a prompt is configured with <code>requiresMultiModalModel: true</code> or <code>requiresFunctionCallingModel: true</code>, the framework verifies that the selected model actually supports the required capability. If it does not, the request will fail with a clear error message.</p>"},{"location":"understanding/concepts/#parameter-precedence","title":"\u2696\ufe0f Parameter Precedence","text":"<p>When making a call to an LLM, you can specify configuration parameters at multiple levels. The framework uses a clear 5-level order of precedence to determine which values to use:</p> <ol> <li>API Call Parameters:    Values passed directly in the request (e.g., <code>provider</code>, <code>model</code>, <code>temperature</code>) always take the highest priority.</li> <li>Prompt Defaults:    If a parameter is not specified in the API call, the framework looks for defaults defined in the Prompt configuration (<code>defaultLlmProvider</code>, <code>defaultLlmModel</code>).</li> <li>Provider Model Config (<code>LlmModelConf</code>):    Per-model settings defined in the provider configuration (e.g., a specific <code>temperature</code> or <code>maxTokens</code> for <code>gpt-5-mini</code>).</li> <li>Provider Global Config (<code>LlmProviderConf.globalConf</code>):    Shared settings defined at the provider level that apply to all models under that provider.</li> <li>YAML Global Defaults:    The fallback values defined in <code>llm.default.*</code> in <code>llm-clients-config.yml</code>.</li> </ol> <p>Example</p> <p>You define a global default of <code>gpt-5.1</code> in <code>llm-clients-config.yml</code> with <code>temperature: 0.7</code>. Your OpenAI provider configuration sets <code>temperature: 0.5</code> globally and <code>temperature: 0.2</code> specifically for the <code>gpt5-mini</code> model. Your \"quick classification\" prompt overrides the model with <code>defaultLlmModel: gpt5-mini</code>. A specific API call can still override everything by passing <code>\"model\": \"gpt-4.1\"</code> in the request body. This five-level system lets you set sensible defaults while retaining full flexibility per provider, per model, per prompt, and per call.</p>"},{"location":"understanding/concepts/#conversations","title":"\ud83d\udcac Conversations","text":"<p>A Conversation is a container that groups a sequence of exchanges between a user and the AI.</p> <ul> <li>Persistence: Conversations are persistent entities with a title, update timestamp, and a record of the last used LLM provider and model .</li> <li>Context: The framework automatically manages the context window, retrieving recent messages to ensure stateful interactions.</li> <li>Lifecycle: Conversations can be listed, retrieved by ID, or deleted via the API .</li> </ul>"},{"location":"understanding/concepts/#requests-inputs-multi-modal","title":"\u21c4 Requests &amp; Inputs (Multi-Modal)","text":"<p>A Request represents a single exchange (a \"turn\") within a conversation. Unlike simple text messages, uxopian-ai requests are rich and multi-modal.</p> <ul> <li>Structure: A request consists of a list of inputs (what was sent) and an answer (what the AI generated) .</li> <li>Multi-Modal Content: The Input block supports various content types. It is not limited to text; it can handle image content or references to prompt IDs and goal names .</li> <li>Dynamic Images: The image input type also supports Thymeleaf templating, allowing you to dynamically inject image data (e.g., <code>[[${documentService.getPageImage(documentId, 1, 800)}]]</code>).</li> <li>Constraint: If using templating for images, the expression must resolve to a raw Base64 string representing the image.</li> <li>Token Tracking: Every request logs inputTokenCount and outputTokenCount for precise cost monitoring .</li> <li>Feedback: Each request can be tagged with user feedback (Good, Bad, or Neutral) to help tune performance over time .</li> </ul>"},{"location":"understanding/concepts/#prompts","title":"\ud83d\udcdc Prompts","text":"<p>A Prompt is a reusable, templated instruction sent to a model. Prompts are central to standardizing AI behavior.</p> <ul> <li>Templating: Prompts use a templating engine (Thymeleaf) allowing for dynamic variable injection.</li> <li>Configuration: Beyond the text content, a prompt stores configuration flags like reasoningDisabled, requiresMultiModalModel, and requiresFunctionCallingModel .</li> <li>ROI Estimation: Prompts include a timeSaved attribute (in seconds). This is used to calculate the Return on Investment (ROI) by estimating how much human time is saved every time the prompt is used .</li> </ul>"},{"location":"understanding/concepts/#goals","title":"\ud83c\udfaf Goals","text":"<p>A Goal is a high-level orchestration unit. It decouples the user's intent from the specific prompt used, allowing for dynamic selection based on context.</p> <ul> <li>Logic: A goal maps a goalName to a specific promptId based on a filter expression .</li> <li>Filtering: The filter typically uses Spring Expression Language (SpEL) to evaluate the payload (e.g., <code>[[${documentType == 'contract'}]]</code>).</li> <li>Priority: Goals have an index property, allowing you to define priority order when multiple goals might match a scenario .</li> </ul>"},{"location":"understanding/concepts/#analytics-statistics","title":"\ud83d\udcca Analytics &amp; Statistics","text":"<p>uxopian-ai includes a comprehensive analytics engine accessible via the Admin API.</p> <ul> <li>Global Stats: Tracks total conversations, requests, tokens, and aggregated time saved .</li> <li>Time Series: Provides activity data (requests, tokens, time saved) aggregated by time intervals (e.g., DAY) to visualize trends .</li> <li>Adoption &amp; ROI:</li> <li>Feature Adoption: Tracks the usage rate of advanced features like multi-modal and function calling .</li> <li>Time Saved: Ranks prompts by the total estimated hours they have saved users .</li> </ul>"},{"location":"understanding/security/","title":"Security Model (BFF Pattern)","text":"<p>Uxopian-ai is a backend microservice designed to run behind a BFF (Backend for Frontend) or API Gateway. It should never be exposed directly to the public internet. This page explains why, how the Gateway works internally, and how to work without it during development.</p>"},{"location":"understanding/security/#why-a-bff-gateway","title":"Why a BFF Gateway?","text":"<p>The uxopian-ai service does not handle authentication itself \u2014 by design. This separation provides several advantages:</p> <ul> <li>Single responsibility: The AI service focuses on prompt resolution and LLM orchestration. The Gateway focuses on security.</li> <li>Pluggable authentication: Your organization can use OAuth2, JWT, SAML, LDAP, or any custom mechanism. The AI service does not need to change.</li> <li>Token propagation: The Gateway forwards the original user token (<code>X-User-Token</code>), which Helpers and integrations can use to call other APIs (FlowerDocs, ARender) on behalf of the authenticated user.</li> <li>Role-based access control: The Gateway maps roles from your identity provider into the <code>X-User-Roles</code> header. The AI service uses this to gate admin operations.</li> <li>Tenant isolation: The Gateway extracts the organization/scope from the authenticated session and injects it as <code>X-User-TenantId</code>, enabling full multi-tenancy.</li> </ul> <p>In a typical deployment, three layers are involved:</p> <ol> <li>Client Application \u2014 The user-facing app (ARender, FlowerDocs, a custom web app).</li> <li>BFF / Gateway \u2014 Authenticates the user and injects identity headers.</li> <li>Uxopian-ai Service \u2014 Receives pre-authenticated requests and processes them.</li> </ol> <p>The Gateway is the only component exposed to the network.</p>"},{"location":"understanding/security/#the-uxopian-gateway","title":"The Uxopian Gateway","text":"<p>The Uxopian Gateway is a standalone service, built and deployed independently from the AI service. It is built on Spring Cloud Gateway (reactive) and sits in front of the AI service, handling authentication via a pluggable provider system. The built-in auth providers (DevProvider, FlowerDocsProvider, Fast2Provider) are part of the Gateway service.</p>"},{"location":"understanding/security/#request-processing-pipeline","title":"Request Processing Pipeline","text":"<p>Every request passes through two filters before reaching the backend:</p> <pre><code>Client Request\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. DefaultProviderHeaderFilter \u2502  \u2190 Matches the request path to a route\n\u2502    (HIGHEST_PRECEDENCE)        \u2502    and injects X-Provider-ID header\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2. AuthFilter                  \u2502  \u2190 Uses the provider to authenticate,\n\u2502    (HIGHEST_PRECEDENCE + 1)    \u2502    then enriches the request with\n\u2502                                \u2502    X-User-Id, X-User-Roles,\n\u2502                                \u2502    X-User-TenantId, X-User-Token\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 3. Spring Cloud Gateway        \u2502  \u2190 Routes to the backend service URI\n\u2502    (Route + Path Rewrite)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502\n    \u25bc\n  uxopian-ai Service (port 8080)\n</code></pre> <ol> <li> <p><code>DefaultProviderHeaderFilter</code> \u2014 Uses <code>AntPathMatcher</code> to match the incoming request URL against the configured routes. When a match is found, it injects the <code>X-Provider-ID</code> internal header with the provider name from the route config (e.g., <code>FlowerDocsProvider</code>, <code>Fast2Provider</code>, <code>DevProvider</code>).</p> </li> <li> <p><code>AuthFilter</code> \u2014 Reads the <code>X-Provider-ID</code> header, loads the corresponding <code>AuthProvider</code> implementation, and calls its <code>authenticate(request)</code> method. The provider returns an <code>AuthenticatedUser</code> with <code>id</code>, <code>roles</code>, <code>tenantId</code>, and <code>token</code>. The filter then enriches the downstream request with these headers:</p> Enriched Header Source <code>X-User-Id</code> <code>AuthenticatedUser.id</code> <code>X-User-Roles</code> <code>AuthenticatedUser.roles</code> (comma-joined) <code>X-User-TenantId</code> <code>AuthenticatedUser.tenantId</code> <code>X-User-Token</code> <code>AuthenticatedUser.token</code> <p>The filter also creates a Spring Security <code>UsernamePasswordAuthenticationToken</code> and places it in the reactive <code>SecurityContext</code>, enabling role-based path security rules.</p> </li> <li> <p>Spring Cloud Gateway \u2014 Forwards the enriched request to the backend URI (<code>http://ai-standalone-service:8080</code>).</p> </li> </ol>"},{"location":"understanding/security/#pluggable-auth-providers","title":"Pluggable Auth Providers","text":"<p>The Gateway uses a plugin architecture. Auth providers are loaded dynamically from JAR files at startup:</p> <ul> <li>At boot, the Gateway scans the <code>provider/</code> directory (configurable via <code>auth.provider.path</code>) for provider implementations.</li> <li>Each provider is a Spring <code>@Service</code> with a name (e.g., <code>@Service(\"FlowerDocsProvider\")</code>).</li> <li>The route configuration references that name in the <code>provider:</code> field.</li> </ul> <p>Built-in providers:</p> Provider Authentication Method <code>DevProvider</code> Reads identity from raw request headers (for development) <code>FlowerDocsProvider</code> Validates FlowerDocs JWT tokens (from <code>token</code> header or <code>SESSION</code> cookie) <code>Fast2Provider</code> Decrypts Fast2 JWT tokens via a remote public key <p>To integrate with your organization's identity system (Keycloak, Azure AD, custom SSO), you implement the <code>AuthProvider</code> interface and drop the JAR into the <code>provider/</code> directory. See Adding a Custom Auth Provider.</p>"},{"location":"understanding/security/#gateway-route-configuration","title":"Gateway Route Configuration","text":"<p>Routes are defined in the Gateway's <code>application.yml</code>:</p> <pre><code>app:\n  gateway:\n    provider-header: X-Provider-ID\n  routes:\n    - id: uxopian-ai\n      uri: http://ai-standalone-service:8080\n      prefix: /gui/gateway/uxopian-ai/\n      path: /gui/gateway/uxopian-ai/**\n      provider: FlowerDocsProvider        # \u2190 Which auth provider to use\n      security:\n        - path: /.well-known/**\n          public: true                     # \u2190 No auth required\n        - path: /swagger-ui/**\n          public: true\n        - path: /prompt/**\n          roles: [ \"ADMIN\" ]              # \u2190 Requires ADMIN role\n        - path: /goal/**\n          roles: [ \"ADMIN\" ]\n</code></pre> <p>Each route maps a URL pattern to a backend service and specifies which auth provider to use and what security rules apply per path.</p>"},{"location":"understanding/security/#authentication-headers","title":"Authentication Headers","text":"<p>When deploying in production, your Gateway (Uxopian's or your own) must inject the following headers into every request before it reaches uxopian-ai:</p> Header Description Required <code>X-User-TenantId</code> Isolates data per tenant (organization). Yes <code>X-User-Id</code> Unique identifier for the user. Yes <code>X-User-Roles</code> Comma-separated list of roles (e.g., <code>admin,user</code>). No <code>X-User-Token</code> Original user token (if needed for downstream context). No <p>The <code>X-User-Roles</code> header controls access to admin endpoints. Operations under <code>/api/v1/admin/*</code> require the <code>admin</code> role. The <code>X-User-Token</code> is propagated to Helpers and integrations that need to call external APIs (e.g., FlowerDocs, ARender) on behalf of the user.</p>"},{"location":"understanding/security/#production-mode","title":"Production Mode","text":"<p>In production, the Gateway intercepts every incoming request, validates the user's credentials, and injects the <code>X-User-*</code> headers. The uxopian-ai service trusts these headers implicitly.</p> <p>Flow:</p> <ol> <li>User sends a request to the Gateway.</li> <li>Gateway selects the appropriate <code>AuthProvider</code> based on the route.</li> <li>Provider authenticates (OAuth2, JWT, SAML...) and returns an <code>AuthenticatedUser</code>.</li> <li>Gateway enriches the request with <code>X-User-TenantId</code>, <code>X-User-Id</code>, <code>X-User-Roles</code>, <code>X-User-Token</code>.</li> <li>Gateway forwards the request to uxopian-ai.</li> <li>Uxopian-ai reads the headers and establishes the security context (<code>AiContext</code>).</li> <li>All data access (conversations, prompts, stats) is scoped to the tenant.</li> </ol> <p>Never expose uxopian-ai directly</p> <p>Without a Gateway, anyone can forge <code>X-User-*</code> headers and impersonate any user or tenant. The Gateway is what makes these headers trustworthy.</p>"},{"location":"understanding/security/#development-mode","title":"Development Mode","text":"<p>For local development and testing, there are two levels of dev-mode that work together:</p>"},{"location":"understanding/security/#1-the-devprovider-gateway-side","title":"1. The <code>DevProvider</code> (Gateway Side)","text":"<p>The <code>DevProvider</code> is a built-in auth provider that trusts raw request headers without any validation. Instead of decrypting a JWT or calling an OAuth endpoint, it simply reads <code>X-User-Id</code>, <code>X-User-Roles</code>, and <code>X-User-Tenant</code> directly from the incoming request.</p> <pre><code>@Service(\"DevProvider\")\npublic class DevProvider implements AuthProvider {\n    @Override\n    public Mono&lt;AuthenticatedUser&gt; authenticate(ServerHttpRequest request) {\n        String userId = request.getHeaders().getFirst(\"X-User-Id\");\n        String rolesRaw = request.getHeaders().getFirst(\"X-User-Roles\");\n        String tenantId = request.getHeaders().getFirst(\"X-User-Tenant\");\n        // ... builds AuthenticatedUser from these headers\n    }\n}\n</code></pre> <p>To activate it, configure a route with <code>provider: DevProvider</code> in the Gateway's <code>application.yml</code>. This lets you call the Gateway with plain headers (e.g., from <code>curl</code> or Postman) without needing real credentials.</p>"},{"location":"understanding/security/#2-the-dev-profile-ai-service-side","title":"2. The <code>dev</code> Profile (AI Service Side)","text":"<p>When the uxopian-ai service is started with <code>SPRING_PROFILES_ACTIVE=dev</code>, its internal <code>AuthFilter</code> injects default credentials if the <code>X-User-*</code> headers are missing:</p> <ul> <li>Default Tenant: <code>Tenant-development</code></li> <li>Default User: <code>User-development</code></li> </ul> <p>This means you can call the AI service directly (bypassing the Gateway entirely) and it will still work:</p> <pre><code># No X-User-* headers needed \u2014 dev profile fills in defaults\ncurl http://localhost:8085/api/v1/conversations\n</code></pre> <p>You can also override specific headers while letting others fall back to defaults:</p> <pre><code># Custom tenant, default user\ncurl -H \"X-User-TenantId: my-test-tenant\" http://localhost:8085/api/v1/conversations\n</code></pre> <p>Starter Kit Uses Dev Mode</p> <p>The Docker Starter Kit ships with <code>SPRING_PROFILES_ACTIVE=dev</code> and no Gateway service. This is why the Quick Start curl commands work directly against <code>localhost:8085</code> \u2014 the dev profile handles the missing headers automatically.</p> <p>Never use <code>dev</code> in production</p> <p>The <code>dev</code> profile disables all authentication enforcement. Any request without headers will be accepted as <code>User-development</code> in <code>Tenant-development</code>. Combined with no Gateway, this means zero access control.</p>"},{"location":"understanding/security/#when-to-use-what","title":"When to Use What","text":"Scenario Gateway AI Service Profile How Auth Works Production Uxopian Gateway + real provider (e.g., <code>FlowerDocsProvider</code>) Default (no <code>dev</code>) Gateway validates credentials, injects headers. AI service rejects requests without headers. Staging / Integration Uxopian Gateway + <code>DevProvider</code> Default (no <code>dev</code>) You pass raw headers through the Gateway. AI service still requires headers. Local development None <code>dev</code> Call the AI service directly. Missing headers get default values."},{"location":"understanding/security/#multi-tenancy","title":"Multi-Tenancy","text":"<p>Every piece of data in uxopian-ai is scoped to a Tenant ID:</p> <ul> <li>Conversations belong to a specific tenant.</li> <li>Prompts and Goals can be global or tenant-specific.</li> <li>Statistics are aggregated per tenant.</li> </ul> <p>The <code>X-User-TenantId</code> header determines which tenant's data a request can access. A user from <code>tenant-A</code> cannot see conversations from <code>tenant-B</code>, even if they share the same uxopian-ai instance.</p> <p>This logical separation is enforced at the data layer (OpenSearch queries always include the tenant filter).</p>"},{"location":"understanding/security/#going-further","title":"Going Further","text":"<ul> <li>Adding a Custom Auth Provider \u2014 Implement your own authentication logic for the Gateway.</li> <li>Architecture Overview \u2014 See the full component and sequence diagrams.</li> <li>Configuration Files \u2014 Gateway and AI service configuration reference.</li> </ul>"},{"location":"understanding/templating/","title":"The Templating Engine","text":"<p>Uxopian-ai uses Thymeleaf combined with Spring Expression Language (SpEL) to make prompts dynamic. Instead of sending static text to the LLM, you can inject runtime data, call Java services, and build conditional logic directly inside your prompt definitions.</p>"},{"location":"understanding/templating/#basic-syntax","title":"Basic Syntax","text":"<p>All dynamic expressions use the Thymeleaf inline text output syntax:</p> <pre><code>[[${expression}]]\n</code></pre> <p>Everything inside <code>[[${...}]]</code> is evaluated at render time before the prompt is sent to the LLM.</p>"},{"location":"understanding/templating/#variable-resolution","title":"Variable Resolution","text":"<p>When a request is sent to uxopian-ai with a payload, each key in the payload JSON becomes a top-level template variable. There is no <code>payload.</code> prefix.</p> <p>Example: Given this API request payload:</p> <pre><code>{\n  \"documentId\": \"doc-abc-123\",\n  \"language\": \"french\"\n}\n</code></pre> <p>The prompt template can reference these variables directly:</p> <pre><code>Translate the following document into [[${language}]]:\n\n[[${documentService.extractTextualContent(documentId)}]]\n</code></pre> <p>At render time, <code>language</code> resolves to <code>\"french\"</code> and <code>documentId</code> resolves to <code>\"doc-abc-123\"</code>.</p>"},{"location":"understanding/templating/#java-services-helpers","title":"Java Services (Helpers)","text":"<p>Registered Spring beans are also available as top-level variables in templates. The built-in services include:</p> <ul> <li><code>documentService</code> \u2014 Extract text or images from documents (e.g., via ARender or FlowerDocs).</li> <li><code>promptService</code> \u2014 Render other prompts or access prompt definitions.</li> </ul> <p>You call their public methods directly:</p> <pre><code>[[${documentService.extractTextualContent(documentId)}]]\n</code></pre> <p>You can also create your own custom helpers. See Creating Custom Helpers for details.</p>"},{"location":"understanding/templating/#conditional-logic","title":"Conditional Logic","text":"<p>Use SpEL ternary expressions for conditional rendering:</p> <pre><code>Translate the following document in [[${language != null ? language : 'english'}]]:\n\n[[${documentService.extractTextualContent(documentId)}]]\n</code></pre> <p>If <code>language</code> is present in the payload, it is used; otherwise, it defaults to <code>\"english\"</code>.</p>"},{"location":"understanding/templating/#iteration","title":"Iteration","text":"<p>Use Thymeleaf's <code>th:each</code> to iterate over lists from the payload. This is useful when processing multiple documents in a single request.</p> <p>Payload:</p> <pre><code>{\n  \"documentIds\": [\"doc-001\", \"doc-002\", \"doc-003\"]\n}\n</code></pre> <p>Prompt template:</p> <pre><code>Please provide a detailed, point-by-point comparison of the following documents:\n\n[# th:each=\"docId, iterStat : ${documentIds}\"]\nDocument [[${iterStat.count}]]:\n[[${documentService.extractTextualContent(docId)}]]\n[/]\n</code></pre> <p>The <code>[# th:each=\"...\"]...[/]</code> block repeats for each item in the list.</p>"},{"location":"understanding/templating/#composition-prompt-in-prompt","title":"Composition (Prompt-in-Prompt)","text":"<p>A prompt can include the rendered output of another prompt using <code>promptService.renderPrompt()</code>. This lets you reuse common formatting instructions or personas across multiple prompts.</p> <pre><code>Summarize the following document.\n[[${promptService.renderPrompt('markdownResponse')}]]\n\nDocument content:\n[[${documentService.extractTextualContent(documentId)}]]\n</code></pre> <p>Here, <code>markdownResponse</code> is a separate prompt that contains formatting rules (e.g., \"Use Markdown headers and bullet points\"). It is rendered inline before the final prompt is sent to the LLM.</p>"},{"location":"understanding/templating/#conversation-history","title":"Conversation History","text":"<p>The variable <code>messages</code> gives access to the current conversation history. You can inject it into a prompt to provide the LLM with prior context:</p> <pre><code>Here is the conversation so far:\n[[${messages}]]\n\nNow answer the following question:\n[[${userQuestion}]]\n</code></pre>"},{"location":"understanding/templating/#going-further","title":"Going Further","text":"<ul> <li>Creating Custom Helpers \u2014 Expose your own Java services in templates.</li> <li>Creating Advanced Helpers \u2014 Implement Map-Reduce patterns for large documents.</li> <li>Managing Prompts and Goals \u2014 CRUD operations on prompts via the API.</li> </ul>"}]}