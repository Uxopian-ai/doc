{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 Welcome to uxopian-ai","text":"<p>uxopian-ai is a complete, standalone framework designed to accelerate and simplify the integration of powerful AI features into any enterprise application.</p> <p>Built on a solid foundation of Java 21 LTS and Spring 3.5, it goes far beyond a simple library by providing a full suite of tools \u2014 from backend services to frontend components \u2014 to create sophisticated, reliable, and scalable AI solutions.</p>"},{"location":"#the-uxopian-ai-advantage-more-than-just-a-library","title":"\u2728 The uxopian-ai Advantage: More Than Just a Library","text":"<p>While uxopian-ai uses the excellent Langchain4j library as its core for LLM interactions, it builds a complete enterprise-ready ecosystem around it. Here\u2019s the added value:</p> <p>\u2705 Standalone Service, Not Just Code A pre-packaged, deployable service that saves you months of development and infrastructure setup.</p> <p>\u2705 Ready-to-Use UI Components Instantly integrate AI with web-components (IIFE compiled, scoped CSS), plus plug-and-play integration scripts.</p> <p>\u2705 Advanced Orchestration Engine The unique Goal system enables dynamic prompt selection based on context \u2014 no need to build this from scratch.</p> <p>\u2705 Complete Conversation Management Persistent conversations with cost tracking, response regeneration, and user feedback support.</p> <p>\u2705 Simplified Extensibility Easily add custom LLM connectors or template helpers through a clean, plugin-style architecture.</p>"},{"location":"#key-features-at-a-glance","title":"\ud83d\udd0d Key Features at a Glance","text":""},{"location":"#effortless-integration","title":"\u2699\ufe0f Effortless Integration","text":"<ul> <li>Standalone Service: Deployable via Docker or as a Java 21 application.</li> <li>Web-Component UI: Lightweight, embeddable components for any web app.</li> <li>Rich REST API: Fully documented (Swagger) for seamless integration.</li> </ul>"},{"location":"#intelligent-orchestration","title":"\ud83e\udde0 Intelligent Orchestration","text":"<ul> <li>Goal System: Define context-aware workflows using filters and priorities.   Example: A \"comparison\" goal automatically picks a legal prompt for contracts, and a generic one for others.</li> <li>Templating Engine: Dynamic data injection, custom Java services, and conditional logic with Thymeleaf.</li> <li>Template Helpers: Add your own Java functions to enrich prompts.</li> </ul>"},{"location":"#robust-llm-interaction","title":"\ud83e\udd16 Robust LLM Interaction","text":"<ul> <li>Broad Support: Compatible with many LLM providers out-of-the-box.</li> <li>Custom Connectors: Add private or fine-tuned models easily.</li> <li>Advanced Features: Native support for function calling, multi-modal requests (text + image), and streaming/non-streaming responses.</li> <li>MCP Server Client: Acts as a client for Multi-Content Platform (MCP) servers.</li> </ul>"},{"location":"#complete-conversation-management","title":"\ud83d\udcac Complete Conversation Management","text":"<ul> <li>Persistent History: Conversations and messages are stored with full context.</li> <li>Cost &amp; Feedback Tracking: Monitor token usage and gather user feedback.</li> <li>Rich UX: Regenerate, copy, and manage conversation content easily.</li> </ul>"},{"location":"#who-is-this-for","title":"\ud83d\udc65 Who Is This For?","text":"<p>This documentation is tailored for integrators and developers looking to deploy, configure, and extend the uxopian-ai framework to deliver cutting-edge AI features faster.</p>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>Ready to dive in? Check out the Installation Guide to set up your first instance of uxopian-ai.</p>"},{"location":"administration/backup_recovery/","title":"Backup and Recovery","text":"<p>This section provides guidance on how to protect the critical data managed by the <code>uxopian-ai</code> framework. A proper backup strategy is essential to prevent data loss and ensure business continuity.</p>"},{"location":"administration/backup_recovery/#what-to-back-up","title":"\ud83d\udcbe What to Back Up","text":"<p>There are two primary sources of data you need to protect:</p> <ul> <li>\ud83d\udee0 Configuration Files: All the <code>.yml</code> files that define the service's behavior, including connections, provider settings, and default parameters.</li> <li>\ud83d\udce6 OpenSearch Data: The OpenSearch instance stores all the dynamic data generated by user interactions, which includes conversations, messages, and the centrally managed Prompts and Goals.</li> </ul>"},{"location":"administration/backup_recovery/#built-in-yaml-backup-for-prompts-and-goals","title":"\ud83e\udde0 Built-in YAML Backup for Prompts and Goals","text":"<p>The framework includes an automatic backup mechanism for Prompts and Goals. Whenever you create, update, or delete a prompt or goal via the API, the service automatically writes the current state of all prompts/goals to a YAML file.</p> <p>This provides a persistent, file-based backup that can be easily version-controlled (e.g., with Git) and used for disaster recovery.</p>"},{"location":"administration/backup_recovery/#configuration","title":"\u2699\ufe0f Configuration","text":"<p>You can configure the path and filename for these backup files in your configuration:</p> <p>For Prompts:</p> <pre><code>prompts:\n  backup:\n    path: ${PROMPTS_BACKUP_PATH:./prompts/}\n    filename: ${PROMPTS_BACKUP_FILENAME:prompts-backup.yml}\n</code></pre> <p>For Goals:</p> <pre><code>goals:\n  backup:\n    path: ${GOALS_BACKUP_PATH:./goals/}\n    filename: ${GOALS_BACKUP_FILENAME:goals-backup.yml}\n</code></pre>"},{"location":"administration/backup_recovery/#restoring-from-yaml-backups","title":"\ud83d\udd04 Restoring from YAML Backups","text":"<p>If you need to restore your Prompts and Goals from their backup files (e.g., after data loss in OpenSearch or when setting up a new environment), you can use the following methods:</p>"},{"location":"administration/backup_recovery/#method-1-loading-from-files-on-startup-recommended","title":"\u2705 Method 1: Loading from Files on Startup (Recommended)","text":"<p>This method uses the framework's ability to load Prompts and Goals from local YAML files on the classpath when the application starts.</p> <ol> <li>Prepare the Files: Take your <code>prompts-backup.yml</code> and <code>goals-backup.yml</code> files. You can either use them as is or rename them (e.g., to <code>prompts.yml</code> and <code>goals.yml</code>).</li> <li>Place Files in Classpath: Place these files in a directory that is part of the application's classpath (e.g., the <code>config</code> directory).</li> <li>Configure <code>application.yml</code>:</li> </ol> <pre><code>spring:\n  config:\n    import:\n      - \"optional:classpath:llm-clients-config.yml\"\n      - \"optional:classpath:prompts.yml\" # Your prompts file\n      - \"optional:classpath:goals.yml\" # Your goals file\n      - \"classpath:opensearch.yml\"\n</code></pre> <ol> <li>Restart the Service: When <code>uxopian-ai</code> starts, it will read the contents of the imported files and automatically load them into OpenSearch.</li> </ol>"},{"location":"administration/backup_recovery/#method-2-manual-restore-via-api","title":"\ud83e\uddea Method 2: Manual Restore via API","text":"<p>You can also manually restore prompts and goals by copying their definitions from the YAML backup file and using the API.</p> <ol> <li>Open your <code>prompts-backup.yml</code> or <code>goals-backup.yml</code> file.</li> <li>For each entry, construct a JSON payload corresponding to the prompt or goal.</li> <li>Use the <code>POST /prompt</code> or <code>POST /goal</code> API endpoint to re-create each entry one by one.</li> </ol>"},{"location":"administration/backup_recovery/#merge-strategies-for-yaml-restoration","title":"\ud83e\udde9 Merge Strategies for YAML Restoration","text":"<p>When restoring Prompts and Goals from YAML files, the framework supports multiple merge strategies to control how data is integrated into OpenSearch.</p>"},{"location":"administration/backup_recovery/#strategy-override","title":"\ud83d\udd01 Strategy: <code>Override</code>","text":"<ul> <li> <p>Prompts:</p> </li> <li> <p>All existing prompts in OpenSearch are deleted.</p> </li> <li> <p>Prompts from the YAML file are fully re-created based on their <code>id</code>.</p> </li> <li> <p>Goals:</p> </li> <li> <p>All existing goals are deleted.</p> </li> <li>Goals from the YAML file are fully re-created based on their <code>name</code>.</li> </ul> <p>Use this mode when you want a clean slate and full replacement of current definitions.</p>"},{"location":"administration/backup_recovery/#strategy-merge","title":"\u2795 Strategy: <code>Merge</code>","text":"<ul> <li> <p>Prompts:</p> </li> <li> <p>For each prompt in the YAML file:</p> <ul> <li>If a prompt with the same <code>id</code> exists: missing values are merged into the existing prompt.</li> <li>If it doesn't exist: it is created.</li> </ul> </li> <li> <p>Goals:</p> </li> <li> <p>For each goal in the YAML file:</p> <ul> <li> <p>If a goal with the same <code>name</code> exists:</p> </li> <li> <p>New values are added to the existing goal.</p> </li> <li> <p>Missing fields are merged, but no data is removed.</p> </li> <li> <p>If it doesn\u2019t exist: it is created.</p> </li> </ul> </li> </ul> <p>Use this mode to augment existing definitions without removing anything.</p>"},{"location":"administration/backup_recovery/#strategy-createifmissing","title":"\ud83c\udd95 Strategy: <code>CreateIfMissing</code>","text":"<ul> <li> <p>Prompts:</p> </li> <li> <p>Only prompts with new <code>id</code>s are added.</p> </li> <li> <p>Existing prompts are left untouched.</p> </li> <li> <p>Goals:</p> </li> <li> <p>Only goals with new <code>name</code>s are added.</p> </li> <li>Existing goals are left untouched.</li> </ul> <p>Use this mode to safely append new definitions while preserving the current state.</p>"},{"location":"administration/backup_recovery/#backup-strategy-table","title":"\ud83d\uddc2 Backup Strategy Table","text":"<p>The following table summarizes what to back up and the recommended method for each type of data:</p> Entity Primary Storage Backup Mechanism Recommended Method Configuration Filesystem (.yml) N/A Backup the configuration directory/files. Conversations OpenSearch Index OpenSearch Snapshots Schedule periodic OpenSearch snapshots. Messages OpenSearch Index OpenSearch Snapshots Schedule periodic OpenSearch snapshots. Goals OpenSearch Index Built-in YAML Backup Schedule OpenSearch snapshots and commit the <code>goals-backup.yml</code> to version control. Prompts OpenSearch Index Built-in YAML Backup Schedule OpenSearch snapshots and commit the <code>prompts-backup.yml</code> to version control."},{"location":"administration/backup_recovery/#key-recommendations","title":"\u2705 Key Recommendations","text":"<ul> <li>\ud83d\udcc6 Automate your backups: Schedule regular, automated backups for both your configuration files and your OpenSearch cluster.</li> <li>\ud83d\uddc3 Version control your backups: Use a version control system like Git for your main configuration files and the YAML backups of your prompts and goals.</li> <li>\ud83d\udd01 Test your recovery process: Periodically test restoring from a backup to ensure your strategy works as expected.</li> <li>\ud83d\udd10 Store backups securely: Keep your backups in a separate, secure location.</li> </ul>"},{"location":"architecture/overview/","title":"Architecture","text":""},{"location":"architecture/overview/#architecture","title":"Architecture","text":"<p>This section provides insight into the framework's design, covering both the high-level components and the software-level interactions.</p>"},{"location":"architecture/overview/#component-architecture","title":"Component Architecture","text":"<p>The <code>uxopian-ai</code> framework is composed of several key components that work together to deliver its functionality. </p> <ul> <li>Client Application: The user-facing application (like ARender or FlowerDocs) that initiates requests to the <code>uxopian-ai</code> service via its REST API.</li> <li> <p>uxopian-ai Service: The core of the framework. This standalone Java application is responsible for:</p> </li> <li> <p>Exposing the REST API for all interactions.</p> </li> <li>Managing conversations and messages.</li> <li>Resolving Goals and Prompts using the templating engine.</li> <li>Connecting to various external LLM providers via the <code>llm-clients</code> module.</li> <li>Persisting conversation history.</li> <li>OpenSearch: The primary data store for the framework. It is used to save and retrieve all Conversations, Messages, Prompts, and Goals, providing the necessary context for ongoing interactions.</li> <li>ARender Rendition Service: An external service used by the templating engine to fetch data, such as extracting the full text of a document using its ID (<code>documentService.extractTextualContent(documentId)</code>).</li> <li>Qdrant: An optional vector database used for advanced Retrieval-Augmented Generation (RAG) use cases.</li> <li>External LLM Providers: Third-party services (like OpenAI, Anthropic, Azure OpenAI, etc.) that perform the actual language model processing. The <code>uxopian-ai</code> service acts as a unified gateway to these providers.</li> </ul>"},{"location":"architecture/overview/#software-architecture-request-flow","title":"Software Architecture (Request Flow)","text":"<p>To understand how the components interact, let's trace the lifecycle of a typical API call: sending a message that uses a Goal.</p> <p>The chat module handles incoming messages with a clear priority system: a <code>goalName</code> is processed first, followed by a <code>promptId</code>, and finally a simple content message.</p>"},{"location":"architecture/overview/#sequence-diagram-executing-a-goal","title":"Sequence Diagram: Executing a Goal","text":""},{"location":"architecture/overview/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Request: The client sends a message to a conversation, specifying a <code>goalName</code> and a payload with context variables.</li> <li>Goal Resolution: The service queries OpenSearch to find the Goal(s) matching the specified name.</li> <li>Filter Evaluation: It evaluates the filter conditions defined in the retrieved Goal(s) against the payload to select the appropriate <code>promptId</code>.</li> <li>Prompt Retrieval: The service retrieves the corresponding Prompt definition from OpenSearch using its ID.</li> <li>Context Retrieval: It queries OpenSearch to fetch the recent message history for the conversation, providing context.</li> <li>Template Rendering: The templating engine (Thymeleaf) assembles the final prompt, injecting the message history and the variables from the client's payload. This may involve calling external services like the ARender Rendition Service to fetch data.</li> <li>LLM Interaction: The service sends the complete prompt to the configured external LLM provider.</li> <li>Persistence: Upon receiving the response, the service saves both the user's request and the LLM's answer as new messages in OpenSearch.</li> <li>Response: The final answer from the LLM is sent back to the client.</li> </ol> <p>Next, let's dive into the [Core Concepts](../concept/overview.md) of the framework.</p>"},{"location":"concepts/overview/","title":"\ud83e\udde0 Core Concepts","text":"<p>This section explains the primary entities and concepts that form the uxopian-ai framework. Understanding these concepts is essential for effective configuration and interaction.</p>"},{"location":"concepts/overview/#providers","title":"\ud83d\udd0c Providers","text":"<p>A Provider is a connector to an external Large Language Model (LLM) service. The framework uses providers to abstract the specific implementation details of each LLM service, offering a unified interface.</p> <p>Examples: <code>openai</code>, <code>azure</code>, <code>anthropic</code>, <code>mistral</code></p> <p>Configuration: The active providers and their API keys are configured in your <code>.yml</code> files. This sets the global default.</p> <p>Extensibility: You can add custom providers by implementing the <code>ModelProvider</code> Java interface.</p>"},{"location":"concepts/overview/#models","title":"\ud83e\udde0 Models","text":"<p>A Model refers to a specific language model available through a provider. Each provider supports one or more models with different capabilities and costs.</p> <p>Examples: <code>gpt-4o</code> (from OpenAI), <code>claude-3-5-sonnet-20240620</code> (from Anthropic)</p> <p>Configuration: You can set a default model for the entire framework. This default can be overridden at the prompt level or during an API call.</p>"},{"location":"concepts/overview/#parameter-precedence-provider-model-temperature-reasoning","title":"\u2696\ufe0f Parameter Precedence (Provider, Model, Temperature &amp; Reasoning)","text":"<p>When making a call to an LLM, you can specify the provider, model, temperature, and reasoning at multiple levels. The framework uses a clear order of precedence to determine which values to use:</p> <ol> <li> <p>API Call Parameters:    Values for provider, model, temperature, or reasoning passed directly in the request to the LLM endpoint will always be used first. This offers maximum flexibility for a single call.</p> </li> <li> <p>Prompt-Specific Defaults:    If a parameter is not specified in the API call, the framework will look for default values (<code>defaultLlmProvider</code>, <code>defaultLlmModel</code>) defined within the prompt itself.</p> </li> <li> <p>Global Defaults:    If no specific parameters are found in either the API call or the prompt, the framework will fall back to the global default values defined in your <code>.yml</code> configuration files.</p> </li> </ol>"},{"location":"concepts/overview/#conversations","title":"\ud83d\udcac Conversations","text":"<p>A Conversation is a container that groups a sequence of exchanges between a user and the AI. It maintains the context and history of the interaction.</p> <p>Persistence: Conversations and their associated messages are stored in OpenSearch.</p> <p>Context: The framework automatically retrieves recent messages from the current conversation to provide context for new requests, enabling stateful interactions.</p> <p>Key Attributes:</p> <ul> <li><code>title</code>: The title of the conversation.</li> <li><code>updatedAt</code>: Timestamp of the last interaction.</li> <li><code>llmProvider</code>: The last LLM provider used in the conversation.</li> <li><code>llmModel</code>: The last LLM model used.</li> </ul>"},{"location":"concepts/overview/#messages","title":"\u2709\ufe0f Messages","text":"<p>A Message represents a single turn in a conversation. It can be a user's query or the AI's response. When sending a message, you can provide one of the following (processed in order of priority):</p> <ul> <li><code>goalName</code>: To execute a high-level task.</li> <li><code>promptId</code>: To use a specific, pre-defined prompt.</li> <li><code>content</code>: A simple, direct text query.</li> </ul> <p>Persistence: All messages are stored in OpenSearch, linked to their parent conversation.</p> <p>Key Attributes:</p> <ul> <li><code>content</code>: The initial content/question of the message.</li> <li><code>answer</code>: The response generated by the LLM.</li> <li><code>promptRole</code>: The role of the message sender (user, assistant, or system).</li> <li><code>createdAt</code>: Timestamp of when the message was created.</li> <li><code>inputTokenCount</code> &amp; <code>outputTokenCount</code>: The number of tokens used, for cost tracking.</li> <li><code>llmName</code>: The name of the language model that processed the message.</li> <li><code>feedback</code>: An optional ID linking to user feedback on the response.</li> </ul>"},{"location":"concepts/overview/#prompts","title":"\ud83d\udcdc Prompts","text":"<p>A Prompt is a reusable, templated instruction sent to a model to guide its response. Prompts are the core building blocks for interacting with LLMs.</p> <p>Templating: They use the Thymeleaf engine, allowing for dynamic content using variables (e.g., <code>${payload.documentId}</code>) and advanced logic.</p> <p>Storage: Prompts are stored and managed in OpenSearch. They can be created and updated via the REST API.</p> <p>Key Attributes:</p> <ul> <li><code>role</code>: Role of the prompt sender (e.g., USER, ASSISTANT, SYSTEM).</li> <li><code>content</code>: Content of the prompt.</li> <li><code>defaultLlmProvider</code>: The default LLM provider for the prompt (Used if no provider is specified in the API call).</li> <li><code>defaultLlmModel</code>: The default LLM model for the prompt (Used if no model is specified in the API call).</li> <li><code>temperature</code>: The default temperature for the prompt (Used if no temperature is specified in the API call).</li> <li><code>reasoningEnabled</code>: Whether reasoning is enabled for the prompt (Used if no reasoning setting is specified in the API call).</li> </ul>"},{"location":"concepts/overview/#goals","title":"\ud83c\udfaf Goals","text":"<p>A Goal is a high-level, reusable task that orchestrates which Prompt to use based on a given context. A goal is essentially a mapping between a specific situation (defined by a filter) and a specific prompt.</p> <p>Example Use Case: A goal named <code>compare</code> could use the <code>detailedComparison</code> prompt if the document type is a <code>contract</code>, but use the <code>genericComparison</code> prompt otherwise.</p> <p>Filtering: The filter logic uses Spring Expression Language (SpEL) to evaluate the context sent in the API call's payload.</p> <p>Storage: Like prompts, goals are stored and managed in OpenSearch.</p>"},{"location":"configuration/api_reference/","title":"Configuration &amp; API","text":"<p>This section details how to configure the <code>uxopian-ai</code> service using <code>.yml</code> files and how to interact with it through its REST API.</p>"},{"location":"configuration/api_reference/#configuration-management","title":"\u2699\ufe0f Configuration Management","text":"<p>The framework uses a standard Spring Boot configuration model, making it flexible and easy to manage across different environments.</p>"},{"location":"configuration/api_reference/#configuration-files-and-profiles","title":"\ud83d\udcc1 Configuration Files and Profiles","text":"<p>Configuration is primarily defined in:</p> <pre><code>src/main/resources/application.yml\n</code></pre> <p>You can override any property using environment variables, which is the recommended approach for Docker deployments.</p> <p>To manage multiple environments (e.g., development, production), use Spring Profiles:</p> <pre><code># Default properties\nllm:\n  default:\n    model: ${LLM_DEFAULT_MODEL:gpt-3.5-turbo}\n    provider: ${LLM_DEFAULT_PROVIDER:openai}\n    base-prompt: ${LLM_DEFAULT_PROMPT:basePrompt}\n\n---\nspring:\n  config:\n    activate:\n      on-profile: dev\nopensearch:\n  host: http://dev-opensearch:9200\n</code></pre>"},{"location":"configuration/api_reference/#llm-provider-configuration","title":"\ud83e\udd16 LLM Provider Configuration","text":"<p>You can select the default LLM provider and configure its credentials in your configuration files or via environment variables:</p> <pre><code>llm:\nopenai:\n  api-key: ${OPENAI_API_KEY:your-api-key}\n  model-name: gpt-3.5-turbo\n  temperature: 0.7\n  timeout: 60s\n  supported-models:\n    - gpt-4o\n    - gpt-4-turbo\n    - gpt-3.5-turbo\n    - dall-e-3\n\ngemini:\n  api-key: ${GEMINI_API_KEY:none}\n  model-name: gemini-1.5-pro-latest\n  temperature: 0.7\n  timeout: 60s\n  supported-models:\n    - gemini-1.5-pro-latest\n    - gemini-1.5-flash-latest\n    - gemini-pro\n</code></pre> <p>\u2705 Supported providers:</p> <ul> <li><code>openai</code></li> <li><code>azure</code></li> <li><code>anthropic</code></li> <li><code>bedrock</code></li> <li><code>gemini</code></li> <li><code>huggingface</code></li> <li><code>mistral</code></li> <li><code>ollama</code></li> </ul> <p>Each provider may have specific required fields.</p>"},{"location":"configuration/api_reference/#opensearch-and-qdrant-settings","title":"\ud83d\udd17 OpenSearch and Qdrant Settings","text":"<p>These settings define your persistence and vector storage layers:</p> <pre><code>opensearch:\n  host: ${OPENSEARCH_HOST:localhost}\n  port: ${OPENSEARCH_PORT:9200}\n  force-refresh-index: ${OPENSEARCH_FORCE_REFRESH_INDEX:false}\nqdrant:\n  url: ${QDRANT_URL:http://localhost:6333}\n  api-key: ${QDRANT_API_KEY:}\n</code></pre> <p>Disabling <code>qdrant</code> (<code>enabled: false</code>) disables vector search and RAG-based features.</p>"},{"location":"configuration/api_reference/#entities-bootstrapped-from-yaml","title":"\ud83e\udde9 Entities Bootstrapped from YAML","text":"<p>At startup, <code>uxopian-ai</code> can automatically load Prompts and Goals from YAML files via <code>spring.config.import</code>. These imports are merged into OpenSearch using a strategy defined by the application:</p> <pre><code>spring:\n  config:\n    import:\n      - \"optional:classpath:prompts.yml\"\n      - \"optional:classpath:goals.yml\"\n</code></pre>"},{"location":"configuration/api_reference/#merge-strategy-configuration","title":"Merge Strategy Configuration","text":"<p>Each YAML import uses one of the following strategies:</p> Strategy Prompts (by <code>id</code>) Goals (by <code>name</code>) <code>Override</code> Deletes all prompts, then recreates them Deletes all goals, then recreates them <code>Merge</code> Merges missing values into existing prompts Merges missing values into existing goals <code>CreateIfMissing</code> Adds only new prompts Adds only new goals"},{"location":"configuration/api_reference/#rest-api-reference","title":"\ud83e\uddea REST API Reference","text":"<p>All interactions with <code>uxopian-ai</code> go through its REST API, including:</p> <ul> <li>Managing conversations and messages</li> <li>CRUD operations on Prompts and Goals</li> <li>Sending dynamic LLM requests</li> </ul> <p>For full documentation and testing:</p> <p>\ud83d\udc49 Visit the Swagger UI</p> <p>The Swagger UI includes all routes, schemas, and live testing tools.</p>"},{"location":"configuration/api_reference/#best-practices","title":"\u2705 Best Practices","text":"<ul> <li>\ud83d\udccc Use environment variables for secrets and deployment-specific overrides</li> <li>\ud83d\udcc2 Version control your YAML configuration and backups</li> <li>\ud83d\udd01 Use the Merge/CreateIfMissing strategy during CI/CD deployments to preserve custom data</li> <li>\ud83e\uddea Use Swagger UI to validate and explore all available endpoints interactively</li> </ul>"},{"location":"getting_started/installation_guide/","title":"\ud83d\udce6 Installation Guide","text":"<p>This guide provides instructions for deploying the uxopian-ai service. We will cover the recommended Docker-based deployment and the manual Java application setup.</p>"},{"location":"getting_started/installation_guide/#docker-deployment-recommended","title":"\ud83d\udc33 Docker Deployment (Recommended)","text":"<p>Deploying with Docker is the recommended method as it provides a consistent and isolated environment.</p>"},{"location":"getting_started/installation_guide/#step-1-pull-the-docker-image","title":"\ud83d\udd39 Step 1: Pull the Docker Image","text":"<p>Pull the official uxopian-ai image from the Arondor Artifactory:</p> <pre><code>docker pull artifactory.arondor.cloud:5001/uxopian-ai/ai-standalone\n</code></pre>"},{"location":"getting_started/installation_guide/#step-2-understand-configuration","title":"\ud83d\udd39 Step 2: Understand Configuration","text":"<p>The service is configured through a set of <code>.yml</code> files (<code>application.yml</code>, <code>opensearch.yml</code>, <code>llm-clients-config.yml</code>, etc.). You can override any configuration parameter by setting an environment variable when running the container.</p> <p>\ud83d\udccc For example, the OpenSearch host is defined in <code>opensearch.yml</code> as:</p> <pre><code>host: ${OPENSEARCH_HOST:localhost}\n</code></pre> <p>You can set the <code>OPENSEARCH_HOST</code> environment variable to specify your server's address.</p>"},{"location":"getting_started/installation_guide/#key-environment-variables-to-configure","title":"\ud83d\udd27 Key Environment Variables to Configure","text":"<p>OpenSearch Connection:</p> <ul> <li><code>OPENSEARCH_HOST</code>: The hostname of your OpenSearch server.</li> <li><code>OPENSEARCH_PORT</code>: The port for your OpenSearch instance (default: <code>9200</code>).</li> </ul> <p>LLM Provider API Keys:</p> <ul> <li><code>OPENAI_API_KEY</code>: Your API key for OpenAI.</li> <li><code>ANTHROPIC_API_KEY</code>: Your API key for Anthropic.   (See <code>llm-clients-config.yml</code> for all provider variables)</li> </ul> <p>Default LLM:</p> <ul> <li><code>LLM_DEFAULT_PROVIDER</code>: The default provider to use (e.g., <code>openai</code>).</li> <li><code>LLM_DEFAULT_MODEL</code>: The default model to use (e.g., <code>gpt-4o</code>).</li> </ul> <p>Server Port:</p> <ul> <li><code>UXOPIAN_AI_PORT</code>: The port on which the service will run inside the container (default: <code>8080</code>).</li> </ul>"},{"location":"getting_started/installation_guide/#step-3-run-the-container","title":"\ud83d\udd39 Step 3: Run the Container","text":"<p>Run the Docker container, mapping the port and passing the necessary environment variables.</p>"},{"location":"getting_started/installation_guide/#example-command","title":"\ud83e\uddea Example Command:","text":"<p>This example runs the service on port 8080 and connects it to an OpenSearch instance.</p> <pre><code>docker run --rm \\\n  -p 8080:8080 \\\n  -e OPENSEARCH_HOST=\"your_opensearch_host\" \\\n  -e OPENSEARCH_PORT=\"9200\" \\\n  -e OPENAI_API_KEY=\"your_openai_api_key\" \\\n  --name uxopian-ai \\\n  artifactory.arondor.cloud:5001/uxopian-ai/ai-standalone\n</code></pre>"},{"location":"getting_started/installation_guide/#java-application-deployment","title":"\u2615 Java Application Deployment","text":"<p>You can also run the service directly as a Java application.</p>"},{"location":"getting_started/installation_guide/#step-1-download-the-application-package","title":"\ud83d\udd39 Step 1: Download the Application Package","text":"<p>Download the installation ZIP file from the Arondor Artifactory:</p> <p>\ud83d\udd17 ai-standalone-2025.0.0.zip</p>"},{"location":"getting_started/installation_guide/#step-2-configure-the-service","title":"\ud83d\udd39 Step 2: Configure the Service","text":"<ul> <li>Unzip the package.</li> <li>Navigate to the <code>config</code> directory.</li> <li>Edit the <code>.yml</code> files (<code>opensearch.yml</code>, <code>llm-clients-config.yml</code>, etc.) to match your environment.</li> </ul> <p>\u26a0\ufe0f You must at least configure your OpenSearch connection and provide the necessary API keys for the LLM providers you intend to use.</p>"},{"location":"getting_started/installation_guide/#step-3-run-the-application","title":"\ud83d\udd39 Step 3: Run the Application","text":"<p>From the root of the unzipped directory, start the service using the Java 21 runtime:</p> <pre><code>java -jar ai-standalone-2025.0.0.jar\n</code></pre>"},{"location":"getting_started/installation_guide/#client-side-integration","title":"\ud83d\udcbb Client-Side Integration","text":"<p>Once the uxopian-ai service is running, you must configure your client application to communicate with it.</p>"},{"location":"getting_started/installation_guide/#arender-integration","title":"\ud83e\udde9 ARender Integration","text":"<p>Download the Integration Package: \ud83d\udce6 <code>arender-iris-2025.0.0-SNAPSHOT.zip</code> from Arondor Artifactory.</p> <p>Deploy Files: Place the files next to <code>arondor-arender-hmi-spring-boot-[current_version].jar</code>. Front-end configs load automatically.</p> <p>Configure the Endpoint: Open <code>arender-custom-client.properties</code> and locate:</p> <pre><code>uxopian.ai.host=http://localhost:8080/ai\n</code></pre> <p>Update the URL to match your running uxopian-ai service.</p>"},{"location":"getting_started/installation_guide/#flowerdocs-integration","title":"\ud83e\udde9 FlowerDocs Integration","text":"<p>Download the Integration Package: \ud83d\udce6 <code>flowerdocs-iris-2025.0.0-SNAPSHOT.zip</code> from Arondor Artifactory.</p> <p>Deploy Files: Unzip the package and integrate configuration files into your FlowerDocs instance.</p> <p>Configure the Endpoint: Open the following file:</p> <pre><code>./conf/Script/consts/consts\n</code></pre> <p>Find and update this line:</p> <pre><code>const UXO_AI_ENDPOINT = \"https://iris.demos.uxopian.com/ai\";\n</code></pre> <p>Replace the URL with your own service endpoint.</p>"},{"location":"getting_started/installation_guide/#integration-in-other-applications","title":"\ud83c\udf10 Integration in Other Applications","text":"<p>To integrate the uxopian-ai front-end components into any other web application:</p>"},{"location":"getting_started/installation_guide/#step-1-download-the-web-components-package","title":"\ud83d\udd39 Step 1: Download the Web Components Package","text":"<p>\ud83d\udce6 <code>web-components-[current-version].zip</code> from the Artifactory.</p>"},{"location":"getting_started/installation_guide/#step-2-import-the-assets","title":"\ud83d\udd39 Step 2: Import the Assets","text":"<p>Include the CSS and JavaScript in your HTML:</p> <pre><code>&lt;!-- Add the stylesheet to your &lt;head&gt; --&gt;\n&lt;link rel=\"stylesheet\" href=\"/path/to/uxopian-ai-styles.css\" /&gt;\n\n&lt;!-- Add the script at the end of your &lt;body&gt; --&gt;\n&lt;script src=\"/path/to/uxopian-ai-components.js\"&gt;&lt;/script&gt;\n</code></pre> <p>Once imported, you'll have access to all uxopian-ai web components.</p>"},{"location":"how_to_guides/contributing/","title":"How to Contribute Prompts and Goals","text":"<p>This guide explains how to manage Prompts and Goals and how to use advanced templating features to make them powerful and dynamic.</p>"},{"location":"how_to_guides/contributing/#managing-prompts-and-goals-via-the-api","title":"Managing Prompts and Goals via the API","text":"<p>The recommended way to manage Prompts and Goals is to store them in OpenSearch using the REST API. This allows for dynamic updates without restarting the service.</p>"},{"location":"how_to_guides/contributing/#api-operations","title":"API Operations","text":"<ul> <li>Create/Update a Prompt: <code>POST /prompt</code>, <code>PUT /prompt</code></li> <li>Create/Update a Goal: <code>POST /goal</code>, <code>PUT /goal</code></li> <li>List all Prompts/Goals: <code>GET /prompt/all</code>, <code>GET /goal/all</code></li> <li>Get a specific item: <code>GET /prompt/{id}</code>, <code>GET /goal/{id}</code></li> <li>Delete an item: <code>DELETE /prompt/{id}</code>, <code>DELETE /goal/{id}</code></li> </ul> <p>Refer to the Swagger UI documentation for a complete list of endpoints and detailed models.</p>"},{"location":"how_to_guides/contributing/#example-creating-a-new-prompt","title":"Example: Creating a New Prompt","text":"<pre><code>curl -X POST http://localhost:8080/ai/prompt \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"id\": \"summarizeDocumentText\",\n  \"role\": \"user\",\n  \"content\": \"Summarize the following document in a plain text format:\\n\\n[[${documentService.extractTextualContent(documentId)}]]\",\n  \"defaultLlmProvider\": \"openai\",\n  \"defaultLlmModel\": \"gpt-3.5-turbo\"\n}'\n</code></pre>"},{"location":"how_to_guides/contributing/#example-creating-a-new-goal","title":"Example: Creating a New Goal","text":"<pre><code>curl -X POST http://localhost:8080/ai/goal \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"goalName\": \"compare\",\n  \"promptId\": \"detailedComparison\",\n  \"filter\": \"[[${documentType == ''contract''}]]\",\n  \"index\": 125\n}'\n</code></pre>"},{"location":"how_to_guides/contributing/#using-advanced-template-functions","title":"Using Advanced Template Functions","text":"<p>The templating engine is based on Thymeleaf and supports Spring Expression Language (SpEL), giving you powerful capabilities within your prompts.</p> <ul> <li>Accessing the Request Payload: Use <code>${payload.fieldName}</code> to access any field from the JSON payload sent with your message.</li> <li>Accessing Conversation History: Use <code>${messages}</code> to provide the LLM with the context of the current conversation.</li> <li>Using Conditional Logic: Use SpEL for conditional logic, e.g., <code>[[${payload.language != null} ? ${payload.language} : 'english']]</code>.</li> <li>Calling Java Services: Call public methods from registered Spring beans, e.g., <code>[[${documentService.extractTextualContent(payload.documentId)}]]</code>.</li> </ul>"},{"location":"how_to_guides/contributing/#examples-of-prompt-and-goal-definitions","title":"Examples of Prompt and Goal Definitions","text":"<p>Here are some practical examples of Goals and Prompts to illustrate these features.</p>"},{"location":"how_to_guides/contributing/#example-of-a-goal-definition","title":"Example of a Goal Definition","text":"<p>A Goal maps a situation to a specific prompt. The <code>index</code> property determines priority (lower numbers are checked first).</p> <pre><code>goals:\n  map:\n    compare:\n      - promptId: detailedComparison\n        filter: \"[[${documentType == 'contract'}]]\"\n        index: 125\n      - promptId: genericComparison\n        filter: \"true\" # Fallback prompt\n        index: 1000\n</code></pre> <p>In this example, when the <code>compare</code> goal is triggered:</p> <ul> <li>If the payload contains <code>documentType: 'contract'</code>, the <code>detailedComparison</code> prompt is used.</li> <li>Otherwise, the <code>genericComparison</code> prompt is used as a fallback.</li> </ul>"},{"location":"how_to_guides/contributing/#examples-of-prompt-definitions","title":"Examples of Prompt Definitions","text":""},{"location":"how_to_guides/contributing/#1-basic-prompt-with-a-service-call","title":"1. Basic Prompt with a Service Call","text":"<pre><code>- id: summarizeDocumentText\n  role: user\n  content: |\n    Summarize the following document in a plain text format:\n\n    [[${documentService.extractTextualContent(documentId)}]]\n</code></pre>"},{"location":"how_to_guides/contributing/#2-prompt-with-conditional-logic","title":"2. Prompt with Conditional Logic","text":"<p>This prompt uses a SpEL expression to dynamically set the translation language.</p> <pre><code>- id: translate\n  role: user\n  content: |\n    Translate the following document in [[${language != null} ? ${language} : 'english']]:\n\n    [[${documentService.extractTextualContent(documentId)}]]\n</code></pre>"},{"location":"how_to_guides/contributing/#3-prompt-with-iteration","title":"3. Prompt with Iteration","text":"<p>This prompt iterates over a list of document IDs from the payload to compare multiple documents.</p> <pre><code>- id: detailedComparison\n  role: user\n  content: |\n    Please be exhaustive and provide a very detailed, point-by-point comparison.\n    Compare the following documents:\n\n    [# th:each=\"docId, iterStat : ${documentIds}\"]\n    Document content [[${iterStat.count}]] : [[${documentService.extractTextualContent(docId)}]]\n    [/]\n</code></pre>"},{"location":"how_to_guides/contributing/#4-prompt-composition","title":"4. Prompt Composition","text":"<p>A prompt can call another prompt. Here, <code>summarizeDocumentMarkdown</code> reuses the formatting rules defined in <code>markdownResponse</code>.</p> <pre><code>- id: summarizeDocumentMarkdown\n  role: user\n  content: |\n    Summarize the following document.\n    [[${promptService.renderPrompt('markdownResponse')}]]\n\n    Document content:\n      [[${documentService.extractTextualContent(documentId)}]]\n</code></pre>"},{"location":"how_to_guides/contributing/#5-system-level-prompt","title":"5. System-level Prompt","text":"<p>A <code>basePrompt</code> can be used to define the persona and core instructions for the AI across the application.</p> <pre><code>- id: basePrompt\n  role: SYSTEM\n  content: |\n    You are Nono. You were born in 2025, you are non-binary...\n    Your primary mission is to assist users by:\n    Providing clear and precise answers...\n</code></pre>"},{"location":"how_to_guides/contributing/#web-interface-for-prompt-and-goal-management","title":"Web Interface for Prompt and Goal Management","text":"<p>In addition to the REST API, <code>uxopian-ai</code> includes a built-in web interface that lets you visually manage prompts.</p> <p>\ud83d\udda5\ufe0f URL: <code>https://&lt;your-uxopian-endpoint&gt;/ai</code> Replace with your actual deployment host.</p> <p>Through this interface, you can:</p> <ul> <li>View and search existing Prompts</li> <li>Edit their fields (ID, content, filters, etc.)</li> <li>Add new Prompts</li> <li>Delete or reorder items interactively</li> </ul> <p></p> <p>This interface is especially useful during development or testing phases where rapid iteration is required.</p>"},{"location":"how_to_guides/new_provider/","title":"Adding a New LLM Provider","text":""},{"location":"how_to_guides/new_provider/#how-to-add-a-new-llm-provider-other-extensions","title":"How to Add a New LLM Provider &amp; Other Extensions","text":"<p>This guide covers advanced extensibility, including adding new LLM providers, implementing custom Java logic, and securing the service.</p>"},{"location":"how_to_guides/new_provider/#adding-a-new-llm-provider-plugin-system","title":"Adding a New LLM Provider (Plugin System)","text":"<p>You can extend <code>uxopian-ai</code> with new LLM providers without modifying the core codebase by using the plugin system.</p>"},{"location":"how_to_guides/new_provider/#step-1-enable-the-plugin-directory","title":"Step 1: Enable the Plugin Directory","text":"<p>In your <code>application.yml</code>, specify a path where the framework will look for custom provider JARs:</p> <pre><code>llm:\n  clients:\n    path: /opt/uxopian-ai/plugins\n</code></pre> <p>The application will automatically scan and load all valid provider JARs from this directory at startup.</p>"},{"location":"how_to_guides/new_provider/#step-2-implement-the-modelprovider-interface","title":"Step 2: Implement the <code>ModelProvider</code> Interface","text":"<p>Create a new Java project and implement the <code>com.uxopian.ai.model.ModelProvider</code> interface:</p> <pre><code>package com.uxopian.ai.model.llm;\n\nimport dev.langchain4j.model.chat.ChatModel;\nimport dev.langchain4j.model.chat.StreamingChatModel;\n\npublic interface ModelProvider {\n    String[] getSupportedModels();\n    ChatModel createChatModelInstance(String modelName);\n    StreamingChatModel createStreamingChatModelInstance(String modelName);\n    String getDefaultModelName();\n}\n</code></pre>"},{"location":"how_to_guides/new_provider/#step-3-create-the-service-bean","title":"Step 3: Create the Service Bean","text":"<p>Your implementation must be a Spring <code>@Service</code>. The service name will be the identifier used in the configuration (e.g., <code>uxopian.llm.provider: 'my-custom-provider'</code>).</p> <pre><code>import org.springframework.stereotype.Service;\n\n@Service(\"my-custom-provider\")\npublic class MyCustomProviderClient implements ModelProvider {\n\n    // Inject configuration properties for your provider\n\n    @Override\n    public ChatModel createChatModelInstance(String modelName) {\n        // Example using Langchain4j builders\n        return MyCustomChatModel.builder()\n            .apiKey(this.apiKey)\n            .modelName(modelName)\n            .build();\n    }\n\n    // ... implement other methods\n}\n</code></pre>"},{"location":"how_to_guides/new_provider/#step-4-package-and-deploy","title":"Step 4: Package and Deploy","text":"<p>Compile your project into a JAR file, ensuring it includes all necessary dependencies. Drop the final JAR into the configured plugin directory. The framework will load it on the next restart.</p>"},{"location":"introduction/introduction/","title":"Welcome to uxopian-ai","text":"<p>uxopian-ai is a standalone framework designed to streamline the integration of Large Language Models (LLMs) into third-party software. It provides a robust set of tools and a unified interface to interact with various AI providers, enabling developers and integrators to build powerful AI-driven features with ease.</p> <p>This documentation will guide you through the installation, architecture, concepts, and configuration of the uxopian-ai service.</p>"},{"location":"introduction/introduction/#getting-started","title":"Getting Started","text":"<p>Before you begin, ensure your environment meets the following requirements.</p>"},{"location":"introduction/introduction/#deployment-options","title":"Deployment Options","text":"<p>uxopian-ai is designed as a standalone service and can be deployed in two primary ways:</p> <ul> <li>As a Java Application: The service can be run directly, requiring a Java 21 runtime environment.  </li> <li>As a Docker Container: This is the recommended method for a standardized and isolated deployment. You will need to use the official Docker image provided by Uxopian.</li> </ul>"},{"location":"introduction/introduction/#requirements","title":"Requirements","text":"<p>To run the uxopian-ai service, the following dependency is mandatory:</p> <ul> <li>OpenSearch: The framework requires a running instance of OpenSearch to store and manage entities like conversations and messages.  </li> <li>Required Version: 2.12.0 or higher</li> </ul> <p>All other libraries are managed internally by the framework and do not require any separate installation.</p> <p>To continue, please proceed to the Installation Guide.</p>"}]}